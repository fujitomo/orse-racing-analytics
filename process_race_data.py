"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
è¨ˆç”»æ›¸Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«å¯¾å¿œç‰ˆï¼‰

å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´ï¼š
1. æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ï¼ˆCSVä½œæˆæ™‚ï¼‰
2. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒ¬ãƒãƒ¼ãƒˆ
3. æ®µéšçš„å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾©æ—§æ©Ÿèƒ½
5. å‡¦ç†æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
"""
from horse_racing.data.processors.bac_processor import process_all_bac_files
from horse_racing.data.processors.sed_processor import process_all_sed_files
from horse_racing.data.processors.srb_processor import process_all_srb_files, merge_srb_with_sed
import argparse
import logging
import time
import pandas as pd
from pathlib import Path
from datetime import datetime

from typing import Dict, Any, Tuple, List
import numpy as np
import re
from collections import defaultdict

# å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°è¨­å®š
def setup_logging(log_level='INFO', log_file=None):
    """å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°è¨­å®š"""
    import logging
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®š
    if log_file:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(log_file, encoding='utf-8')
            ]
        )
    else:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

# ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¬ãƒ¼
logger = logging.getLogger(__name__)

class DataQualityChecker:
    """
    ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
    å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã«å¿…è¦ãªå“è³ªç®¡ç†æ©Ÿèƒ½
    """
    
    def __init__(self):
        self.quality_report = {}  # å„å‡¦ç†æ®µéšã®ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
        
    def check_data_quality(self, df: pd.DataFrame, stage_name: str) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        
        Args:
            df: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®DataFrame
            stage_name: å‡¦ç†æ®µéšå
            
        Returns:
            å“è³ªãƒ¬ãƒãƒ¼ãƒˆè¾æ›¸
        """
        logger.info(f"ğŸ“Š {stage_name} - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        start_time = time.time()
        
        report = {
            'stage': stage_name,  # å‡¦ç†æ®µéšåï¼ˆä¾‹ï¼š'BACå‡¦ç†å¾Œ', 'çµ±åˆå¾Œ'ï¼‰
            'timestamp': datetime.now().isoformat(),  # å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œæ™‚åˆ»ï¼ˆISOå½¢å¼ï¼‰
            'total_rows': len(df),  # ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼‰
            'total_columns': len(df.columns),  # ãƒ‡ãƒ¼ã‚¿åˆ—æ•°ï¼ˆã‚«ãƒ©ãƒ æ•°ï¼‰
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            'missing_values': {},  # æ¬ æå€¤åˆ†æçµæœï¼ˆåˆ—åˆ¥ã®æ¬ ææ•°ãƒ»å‰²åˆï¼‰
            'data_types': {},  # ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±ï¼ˆåˆ—åã¨ãƒ‡ãƒ¼ã‚¿å‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
            'duplicates': 0,  # é‡è¤‡è¡Œæ•°
            'outliers': {},  # å¤–ã‚Œå€¤æ¤œå‡ºçµæœï¼ˆåˆ—åˆ¥ã®å¤–ã‚Œå€¤æ•°ï¼‰
            'warnings': [],  # å“è³ªè­¦å‘Šãƒªã‚¹ãƒˆï¼ˆç•°å¸¸å€¤ã€ä¸æ­£ãƒ‡ãƒ¼ã‚¿ãªã©ï¼‰
            'recommendations': []  # æ”¹å–„æ¨å¥¨äº‹é …ãƒªã‚¹ãƒˆ
        }
        
        try:
            # 1. æ¬ æå€¤åˆ†æ
            logger.info("   ğŸ” æ¬ æå€¤åˆ†æä¸­...")
            missing_analysis = self._analyze_missing_values(df)
            report['missing_values'] = missing_analysis
            
            # 2. ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            logger.info("   ğŸ·ï¸ ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯ä¸­...")
            report['data_types'] = self._check_data_types(df)
            
            # 3. é‡è¤‡ãƒã‚§ãƒƒã‚¯
            logger.info("   ğŸ”„ é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
            report['duplicates'] = int(df.duplicated().sum())
            
            # 4. å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
            logger.info("   ğŸ“ˆ å¤–ã‚Œå€¤æ¤œå‡ºä¸­...")
            report['outliers'] = self._detect_outliers(df)
            
            # 5. ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼
            logger.info("   ğŸ“‹ ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼ä¸­...")
            warnings, recommendations = self._validate_business_rules(df)
            report['warnings'] = warnings
            report['recommendations'] = recommendations
            
            execution_time = time.time() - start_time
            report['execution_time_seconds'] = execution_time
            
            logger.info(f"âœ… {stage_name} - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº† ({execution_time:.2f}ç§’)")
            
            # ãƒ¬ãƒãƒ¼ãƒˆè¦ç´„ã‚’ãƒ­ã‚°å‡ºåŠ›
            self._log_quality_summary(report)
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            report['error'] = str(e)
        
        self.quality_report[stage_name] = report
        return report
    
    def _analyze_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ¬ æå€¤ã®è©³ç´°åˆ†æ"""
        missing_counts = df.isnull().sum()
        # æ¬ æå€¤ã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
        missing_percentages = (missing_counts / len(df)) * 100
        
        analysis = {
            'total_missing_cells': int(missing_counts.sum()),
            'columns_with_missing': {k: int(v) for k, v in missing_counts[missing_counts > 0].to_dict().items()},
            'missing_percentages': missing_percentages[missing_percentages > 0].to_dict(),
            'critical_columns': []  # 50%ä»¥ä¸Šæ¬ æã®åˆ—
        }
        
        # é‡è¦ãªæ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š
        for col, pct in missing_percentages.items():
            if pct >= 50:
                analysis['critical_columns'].append(col)
        
        return analysis
    
    def _check_data_types(self, df: pd.DataFrame) -> Dict[str, str]:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        return {col: str(dtype) for col, dtype in df.dtypes.items()}
    
    def _detect_outliers(self, df: pd.DataFrame) -> Dict[str, int]:
        """IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º"""
        outlier_counts = {}
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if df[col].notna().sum() > 0:  # æ¬ æå€¤ã§ãªã„å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                outlier_counts[col] = int(len(outliers))
        
        return outlier_counts
    
    def _validate_business_rules(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼"""
        warnings = []
        recommendations = []
        
        # ç€é †ã®ãƒã‚§ãƒƒã‚¯
        if 'ç€é †' in df.columns:
            invalid_positions = df[df['ç€é †'] < 0]
            if len(invalid_positions) > 0:
                warnings.append(f"ä¸æ­£ãªç€é †ãƒ‡ãƒ¼ã‚¿: {len(invalid_positions)}ä»¶")
        
        # ã‚¿ã‚¤ãƒ ã®ãƒã‚§ãƒƒã‚¯
        if 'ã‚¿ã‚¤ãƒ ' in df.columns:
            # ç•°å¸¸ã«é€Ÿã„/é…ã„ã‚¿ã‚¤ãƒ ã®æ¤œå‡º
            if df['ã‚¿ã‚¤ãƒ '].notna().sum() > 0:
                median_time = df['ã‚¿ã‚¤ãƒ '].median()
                if median_time and (median_time < 60 or median_time > 300):
                    warnings.append(f"ç•°å¸¸ãªã‚¿ã‚¤ãƒ ä¸­å¤®å€¤: {median_time}ç§’")
        
        # è·é›¢ã®ãƒã‚§ãƒƒã‚¯
        if 'è·é›¢' in df.columns:
            if df['è·é›¢'].notna().sum() > 0:
                min_distance = df['è·é›¢'].min()
                max_distance = df['è·é›¢'].max()
                if min_distance < 1000 or max_distance > 4000:
                    warnings.append(f"ç•°å¸¸ãªè·é›¢ç¯„å›²: {min_distance}m - {max_distance}m")
        
        # æ¨å¥¨äº‹é …
        if len(warnings) == 0:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿å“è³ªã¯è‰¯å¥½ã§ã™")
        else:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return warnings, recommendations
    
    def _log_quality_summary(self, report: Dict[str, Any]):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚µãƒãƒªãƒ¼ã®ãƒ­ã‚°å‡ºåŠ›"""
        logger.info(f"ğŸ“Š ã€{report['stage']}ã€‘å“è³ªã‚µãƒãƒªãƒ¼:")
        logger.info(f"   ğŸ“ ãƒ‡ãƒ¼ã‚¿è¦æ¨¡: {report['total_rows']:,}è¡Œ x {report['total_columns']}åˆ—")
        logger.info(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {report['memory_usage_mb']:.1f}MB")
        logger.info(f"   â“ æ¬ æã‚»ãƒ«æ•°: {report['missing_values']['total_missing_cells']:,}")
        logger.info(f"   ğŸ”„ é‡è¤‡è¡Œæ•°: {report['duplicates']:,}")
        
        if report['warnings']:
            logger.warning(f"   âš ï¸ è­¦å‘Š: {len(report['warnings'])}ä»¶")
            for warning in report['warnings']:
                logger.warning(f"      â€¢ {warning}")

class MissingValueHandler:
    """
    æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚¯ãƒ©ã‚¹
    è¨ˆç”»æ›¸Phase 0ã®è¦ä»¶ã«åŸºã¥ãå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®æ¬ æå€¤å‡¦ç†
    """
    
    def __init__(self):
        self.processing_log = []
        
    def handle_missing_values(self, df: pd.DataFrame, strategy_config: Dict[str, Any] = None) -> pd.DataFrame:
        """
        æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã®å®Ÿè¡Œ
        
        Args:
            df: å‡¦ç†å¯¾è±¡DataFrame
            strategy_config: å‡¦ç†æˆ¦ç•¥è¨­å®š
            
        Returns:
            æ¬ æå€¤å‡¦ç†æ¸ˆã¿DataFrame
        """
        logger.info("ğŸ”§ æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†é–‹å§‹")
        start_time = time.time()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆ¦ç•¥è¨­å®š
        if strategy_config is None:
            strategy_config = self._get_default_strategy()
        
        df_processed = df.copy()
        original_rows = len(df_processed)
        
        try:
            # 1. é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_critical_columns(df_processed, strategy_config)
            
            # 2. æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_numeric_columns(df_processed, strategy_config)
            
            # 3. ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_categorical_columns(df_processed, strategy_config)
            
            # 4. æ®‹å­˜æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†
            df_processed = self._handle_remaining_missing(df_processed, strategy_config)
            
            # 5. é¦¬é½¢è¨ˆç®—ï¼ˆè¡€çµ±ç™»éŒ²ç•ªå·ã¨å¹´æœˆæ—¥ã‹ã‚‰ï¼‰
            df_processed = self._calculate_horse_age_from_registration(df_processed)
            
            execution_time = time.time() - start_time
            final_rows = len(df_processed)
            
            logger.info(f"âœ… æ¬ æå€¤å‡¦ç†å®Œäº† ({execution_time:.2f}ç§’)")
            logger.info(f"   ğŸ“Š å‡¦ç†å‰: {original_rows:,}è¡Œ")
            logger.info(f"   ğŸ“Š å‡¦ç†å¾Œ: {final_rows:,}è¡Œ")
            logger.info(f"   ğŸ“‰ é™¤å»è¡Œæ•°: {original_rows - final_rows:,}è¡Œ ({((original_rows - final_rows) / original_rows) * 100:.1f}%)")
            
            # å‡¦ç†ãƒ­ã‚°ã®ä¿å­˜
            self._save_processing_log(df_processed)
            
        except Exception as e:
            logger.error(f"âŒ æ¬ æå€¤å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
        
        return df_processed
    
    def _get_default_strategy(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¬ æå€¤å‡¦ç†æˆ¦ç•¥"""
        return {
            'critical_columns': {
                'ç€é †': 'drop',  # ç€é †ãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                'è·é›¢': 'drop',   # è·é›¢ãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                'é¦¬å': 'drop',   # é¦¬åãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                'IDM': 'drop'     # IDMãŒæ¬ æã®è¡Œã¯å‰Šé™¤
            },
            'numeric_columns': {
                'method': 'median',  # ä¸­å¤®å€¤ã§è£œå®Œ
                'max_missing_rate': 0.5  # 50%ä»¥ä¸Šæ¬ æã®åˆ—ã¯å‰Šé™¤
            },
            'categorical_columns': {
                'method': 'mode',    # æœ€é »å€¤ã§è£œå®Œ
                'unknown_label': 'ä¸æ˜',
                'max_missing_rate': 0.8  # 80%ä»¥ä¸Šæ¬ æã®åˆ—ã¯å‰Šé™¤
            },
            # æ®‹å­˜æ¬ æå€¤ã¯é‡è¦åˆ—ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã®ã¿è¡Œå‰Šé™¤ï¼ˆå®Ÿå‹™ãƒ¬ãƒãƒ¼ãƒˆæ–¹é‡ï¼‰
            'remaining_strategy': 'drop_subset',
            'remaining_subset': ['ç€é †', 'è·é›¢', 'é¦¬å', 'IDM', 'ã‚°ãƒ¬ãƒ¼ãƒ‰']
        }
    
    def _handle_critical_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†"""
        logger.info("   ğŸ¯ é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        critical_config = config.get('critical_columns', {})
        
        for column, strategy in critical_config.items():
            if column in df.columns:
                missing_count = df[column].isnull().sum()
                if missing_count > 0:
                    logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã®æ¬ æå€¤ã‚’{strategy}å‡¦ç†")
                    
                    if strategy == 'drop':
                        df = df.dropna(subset=[column])
                        self.processing_log.append(f"{column}: {missing_count}è¡Œã‚’å‰Šé™¤ï¼ˆé‡è¦åˆ—ï¼‰")
        
        return df
    
    def _handle_numeric_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†"""
        logger.info("   ğŸ”¢ æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        numeric_config = config.get('numeric_columns', {})
        method = numeric_config.get('method', 'median')
        max_missing_rate = numeric_config.get('max_missing_rate', 0.5)
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ãŒæ–‡å­—åˆ—ã§ã‚‚æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ãŒå‹•ãã‚ˆã†ã«æ•°å€¤åŒ–ã‚’è©¦ã¿ã‚‹
        for grade_col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade', 'ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰']:
            if grade_col in df.columns:
                df[grade_col] = pd.to_numeric(df[grade_col], errors='coerce')

        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        # è³é‡‘é–¢é€£ã®åˆ—ã‚’æ¬ æå€¤å‡¦ç†ã®å¯¾è±¡ã‹ã‚‰é™¤å¤–ï¼ˆæ¬ æãŒå¤šãã¦å‰Šé™¤ã•ã‚Œã‚‹ã®ã‚’é˜²ãï¼‰
        prize_columns = [
            '2ç€è³é‡‘', '3ç€è³é‡‘', '4ç€è³é‡‘', '5ç€è³é‡‘',
            '1ç€ç®—å…¥è³é‡‘', '2ç€ç®—å…¥è³é‡‘',
            '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', '2ç€è³é‡‘(2ç€ç®—å…¥è³é‡‘è¾¼ã¿)', 'å¹³å‡è³é‡‘'
        ]
        columns_to_process = [
            col for col in numeric_columns 
            if col not in prize_columns
        ]

        for column in columns_to_process:
            missing_count = df[column].isnull().sum()
            missing_rate = missing_count / len(df) if len(df) > 0 else 0
            
            if missing_count > 0:
                # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰
                if column in ['ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade', 'ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰']:
                    logger.info(f"      â€¢ {column}: å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†ã‚’å®Ÿè¡Œ")
                    df = self._estimate_grade_from_features(df, column)
                    
                    # æ¨å®šå¾Œã®æ¬ ææ•°ã‚’ãƒã‚§ãƒƒã‚¯
                    remaining_missing = df[column].isnull().sum()
                    estimated_count = missing_count - remaining_missing
                    
                    if estimated_count > 0:
                        logger.info(f"      â€¢ {column}: {estimated_count:,}ä»¶ã‚’è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰æ¨å®šè£œå®Œ")
                        self.processing_log.append(f"{column}: è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰{estimated_count}ä»¶æ¨å®šâ†’ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ")
                    
                    # æ¨å®šã§ããªã‹ã£ãŸåˆ†ã¯NaNã®ã¾ã¾æ®‹ã™ï¼ˆæ®‹å­˜æ¬ æå€¤å‡¦ç†ã§è¡Œå‰Šé™¤ã•ã‚Œã‚‹ï¼‰
                    if remaining_missing > 0:
                        logger.info(f"      â€¢ {column}: æ¨å®šä¸å¯èƒ½ãª{remaining_missing:,}ä»¶ã¯NaNã®ã¾ã¾ä¿æŒï¼ˆå¾Œç¶šå‡¦ç†ã§è¡Œå‰Šé™¤ï¼‰")
                        self.processing_log.append(f"{column}: æ¨å®šä¸å¯èƒ½{remaining_missing}ä»¶â†’NaNä¿æŒâ†’è¡Œå‰Šé™¤å¯¾è±¡")
                
                elif missing_rate > max_missing_rate:
                    logger.warning(f"      â€¢ {column}: æ¬ æç‡{missing_rate:.1%} > {max_missing_rate:.1%} â†’ åˆ—å‰Šé™¤")
                    df = df.drop(columns=[column])
                    self.processing_log.append(f"{column}: é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤")
                else:
                    if method == 'median':
                        fill_value = df[column].median()
                    elif method == 'mean':
                        fill_value = df[column].mean()
                    else:
                        fill_value = 0
                    
                    df[column] = df[column].fillna(fill_value)
                    logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’{method}({fill_value})ã§è£œå®Œ")
                    self.processing_log.append(f"{column}: {method}ã§{missing_count}ä»¶è£œå®Œ")
        
        return df
    
    def _handle_categorical_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†"""
        logger.info("   ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        categorical_config = config.get('categorical_columns', {})
        method = categorical_config.get('method', 'mode')
        unknown_label = categorical_config.get('unknown_label', 'ä¸æ˜')
        max_missing_rate = categorical_config.get('max_missing_rate', 0.8)
        
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns
        
        for column in categorical_columns:
            # ã‚°ãƒ¬ãƒ¼ãƒ‰ã¯ãƒ¢ãƒ¼ãƒ‰è£œå®Œã®å¯¾è±¡ã‹ã‚‰é™¤å¤–ï¼ˆæ¨å®šãƒ­ã‚¸ãƒƒã‚¯ã«å§”ã­ã‚‹ï¼‰
            if column in ['ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade', 'ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰', 'ã‚°ãƒ¬ãƒ¼ãƒ‰å']:
                continue
            
            # ã‚°ãƒ¬ãƒ¼ãƒ‰_yã®ç‰¹åˆ¥å‡¦ç†ï¼ˆäºˆæ¸¬ãƒãƒ¼ã‚¯ä»˜ãï¼‰
            if column == 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y':
                missing_count = df[column].isnull().sum()
                if missing_count > 0:
                    logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’mode(ç‰¹åˆ¥)ã§è£œå®Œï¼ˆäºˆæ¸¬ãƒãƒ¼ã‚¯ä»˜ãï¼‰")
                    df[column] = df[column].fillna('ç‰¹åˆ¥ï¼ˆäºˆæ¸¬ï¼‰')
                    self.processing_log.append(f"{column}: {missing_count}ä»¶ã‚’mode(ç‰¹åˆ¥)ã§è£œå®Œï¼ˆäºˆæ¸¬ãƒãƒ¼ã‚¯ä»˜ãï¼‰")
                continue
            
            missing_count = df[column].isnull().sum()
            missing_rate = missing_count / len(df) if len(df) > 0 else 0
            
            if missing_count > 0:
                if missing_rate > max_missing_rate:
                    logger.warning(f"      â€¢ {column}: æ¬ æç‡{missing_rate:.1%} > {max_missing_rate:.1%} â†’ åˆ—å‰Šé™¤")
                    df = df.drop(columns=[column])
                    self.processing_log.append(f"{column}: é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤")
                else:
                    if method == 'mode':
                        mode_values = df[column].mode()
                        fill_value = mode_values.iloc[0] if not mode_values.empty else unknown_label
                    else:
                        fill_value = unknown_label
                    
                    df[column] = df[column].fillna(fill_value)
                    logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’{method}({fill_value})ã§è£œå®Œ")
                    self.processing_log.append(f"{column}: {method}ã§{missing_count}ä»¶è£œå®Œ")
        
        return df
    
    def _handle_remaining_missing(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """æ®‹å­˜æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†"""
        remaining_missing = df.isnull().sum().sum()
        
        if remaining_missing > 0:
            logger.info(f"   ğŸ”§ æ®‹å­˜æ¬ æå€¤å‡¦ç†ä¸­: {remaining_missing:,}ä»¶")
            
            strategy = config.get('remaining_strategy', 'drop')
            
            if strategy == 'drop':
                initial_rows = len(df)
                df = df.dropna()
                dropped_rows = initial_rows - len(df)
                
                if dropped_rows > 0:
                    logger.info(f"      â€¢ æ®‹å­˜æ¬ æå€¤ã®ã‚ã‚‹{dropped_rows:,}è¡Œã‚’å‰Šé™¤")
                    self.processing_log.append(f"æ®‹å­˜æ¬ æå€¤: {dropped_rows}è¡Œå‰Šé™¤")
            elif strategy == 'drop_subset':
                subset = config.get('remaining_subset', [])
                subset = [col for col in subset if col in df.columns]
                if subset:
                    initial_rows = len(df)
                    df = df.dropna(subset=subset)
                    dropped_rows = initial_rows - len(df)
                    if dropped_rows > 0:
                        logger.info(f"      â€¢ é‡è¦åˆ—({', '.join(subset)})ã®æ®‹å­˜æ¬ æ{dropped_rows:,}è¡Œã‚’å‰Šé™¤")
                        self.processing_log.append(f"æ®‹å­˜æ¬ æ(é‡è¦åˆ—): {dropped_rows}è¡Œå‰Šé™¤")
        
        return df
    
    def _estimate_grade_from_features(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """
        å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†
        è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åãƒ»å‡ºèµ°é ­æ•°ç­‰ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å®š
        æ¨å®šã§ããªã„å ´åˆã¯è©²å½“ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
        
        Args:
            df: å‡¦ç†å¯¾è±¡DataFrame
            grade_column: ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—å
            
        Returns:
            ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šæ¸ˆã¿DataFrameï¼ˆæ¨å®šå¤±æ•—ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯å‰Šé™¤æ¸ˆã¿ï¼‰
        """
        initial_rows = len(df)
        grade_missing_mask = df[grade_column].isnull()
        initial_missing_count = grade_missing_mask.sum()
        
        if not grade_missing_mask.any():
            # æ—¢å­˜ã®æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ã‚’ä½œæˆ
            df = self._add_grade_name_column(df, grade_column)
            return df
        
        logger.info(f"ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æå€¤: {initial_missing_count:,}ä»¶ ({initial_missing_count/initial_rows*100:.1f}%)")
        
        # æ¨å®šå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        estimation_df = df[grade_missing_mask].copy()
        
        # 1. 1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š
        if '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)' in df.columns:
            estimation_df = self._estimate_grade_from_prize(estimation_df, grade_column)
        
        # 2. æœ¬è³é‡‘ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'æœ¬è³é‡‘' in df.columns:
            estimation_df = self._estimate_grade_from_base_prize(estimation_df, grade_column)
        
        # 3. ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'ãƒ¬ãƒ¼ã‚¹å' in df.columns:
            estimation_df = self._estimate_grade_from_race_name_fallback(estimation_df, grade_column)
        
        # 4. å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹è£œæ­£ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ - æ¬ æå€¤å¯¾å¿œã‚’å³å¯†åŒ–ï¼‰
        # if 'é ­æ•°' in df.columns:
        #     estimation_df = self._adjust_grade_by_field_size(estimation_df, grade_column)
        
        # 5. è·é›¢ã«ã‚ˆã‚‹è£œæ­£ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ - æ¬ æå€¤å¯¾å¿œã‚’å³å¯†åŒ–ï¼‰
        # if 'è·é›¢' in df.columns:
        #     estimation_df = self._adjust_grade_by_distance(estimation_df, grade_column)
        
        # æ¨å®šçµæœã‚’å…ƒã®DataFrameã«åæ˜ 
        df.loc[grade_missing_mask, grade_column] = estimation_df[grade_column]
        
        # æ¨å®šå¾Œã®æ®‹å­˜æ¬ æå€¤ã‚’ãƒã‚§ãƒƒã‚¯
        remaining_missing_mask = df[grade_column].isnull()
        remaining_missing_count = remaining_missing_mask.sum()
        estimated_count = initial_missing_count - remaining_missing_count
        
        if estimated_count > 0:
            logger.info(f"      âœ… ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šæˆåŠŸ: {estimated_count:,}ä»¶")
            self.processing_log.append(f"{grade_column}: è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰{estimated_count}ä»¶æ¨å®š")
        
        # æ®‹å­˜æ¬ æå€¤ï¼ˆæ¨å®šå¤±æ•—ï¼‰ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
        if remaining_missing_count > 0:
            logger.info(f"      âŒ ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå¤±æ•—â†’å‰Šé™¤: {remaining_missing_count:,}ä»¶ ({remaining_missing_count/initial_rows*100:.1f}%)")
            df = df[~remaining_missing_mask]
            self.processing_log.append(f"{grade_column}: æ¨å®šå¤±æ•—ã«ã‚ˆã‚Š{remaining_missing_count}è¡Œå‰Šé™¤")
        
        # æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ä¿æŒã—ã¤ã¤ã€Œã‚°ãƒ¬ãƒ¼ãƒ‰åã€åˆ—ã‚’ä½œæˆ
        df = self._add_grade_name_column(df, grade_column)
        
        final_rows = len(df)
        deleted_rows = initial_rows - final_rows
        
        if deleted_rows > 0:
            logger.info(f"      ğŸ“‰ å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰çµ±è¨ˆ: {deleted_rows:,}è¡Œå‰Šé™¤ (å‰Šé™¤ç‡: {deleted_rows/initial_rows*100:.1f}%)")
            logger.info(f"      ğŸ“Š æ®‹å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰: {final_rows:,}è¡Œ (æ®‹å­˜ç‡: {final_rows/initial_rows*100:.1f}%)")
        
        return df
    
    def _estimate_grade_from_prize(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """è³é‡‘ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆå®Ÿå‹™ãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ãåŸºæº–ï¼‰
        1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)ã®ã¿ã‚’ä½¿ç”¨
        ã—ãã„å€¤ã¯ä¸‡å††ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒ³å®šï¼ˆãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒ«å·®ç•°ã¯ãã®ã¾ã¾æ¯”è¼ƒï¼‰
        """
        # 1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)ã®ã¿ã‚’ä½¿ç”¨
        prize_col = '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)'
        if prize_col not in df.columns:
            return df

        # æ•°å€¤åŒ–
        df[prize_col] = pd.to_numeric(df[prize_col], errors='coerce')

        # ã—ãã„å€¤ï¼ˆä¸‡å††ï¼‰: formattedãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã«åŸºã¥ãå®Ÿè¨¼çš„åŸºæº–
        # åˆ†æçµæœ: G1å¹³å‡1,480ä¸‡å††ã€G2å¹³å‡757ä¸‡å††ã€G3å¹³å‡477ä¸‡å††
        # G1ã‚’ãƒ¬ãƒ™ãƒ«åˆ¥ã«åˆ†ã‘ã‚‹
        thresholds = [
            (10000, 1),  # G1æœ€é«˜ãƒ¬ãƒ™ãƒ«: 10,000ä¸‡å††ä»¥ä¸Šï¼ˆã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—ãƒ»æœ‰é¦¬è¨˜å¿µãƒ¬ãƒ™ãƒ«ï¼‰
            (5000, 11),  # G1é«˜ãƒ¬ãƒ™ãƒ«: 5,000ä¸‡å††ä»¥ä¸Šï¼ˆå¤©çš‡è³ãƒ»å®å¡šè¨˜å¿µãƒ¬ãƒ™ãƒ«ï¼‰
            (2000, 12),  # G1æ¨™æº–ãƒ¬ãƒ™ãƒ«: 2,000ä¸‡å††ä»¥ä¸Šï¼ˆçšæœˆè³ãƒ»èŠèŠ±è³ãƒ¬ãƒ™ãƒ«ï¼‰
            (1000, 2),   # G2: 1,000ä¸‡å††ä»¥ä¸Šï¼ˆG2ãƒ¬ãƒ¼ã‚¹ï¼‰
            (500, 3),    # G3: 500ä¸‡å††ä»¥ä¸Šï¼ˆG3ãƒ¬ãƒ¼ã‚¹ï¼‰
            (200, 6),    # Lï¼ˆãƒªã‚¹ãƒ†ãƒƒãƒ‰ï¼‰: 200ä¸‡å††ä»¥ä¸Š
            (100, 5)     # ç‰¹åˆ¥/OP: 100ä¸‡å††ä»¥ä¸Š
        ]

        for min_prize, grade_value in thresholds:
            mask = (df[prize_col] >= min_prize) & df[grade_column].isnull()
            df.loc[mask, grade_column] = grade_value

        # ã€è¿½åŠ ã€‘æ®‹å­˜æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†
        remaining_missing = df[grade_column].isnull().sum()
        if remaining_missing > 0:
            logger.info(f"      ğŸ”§ æ®‹å­˜æ¬ æå€¤{remaining_missing:,}ä»¶ã®æœ€çµ‚å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
            
            # 1. æœ¬è³é‡‘ã‹ã‚‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if 'æœ¬è³é‡‘' in df.columns:
                df = self._estimate_grade_from_base_prize(df, grade_column)
            
            # 2. ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if 'ãƒ¬ãƒ¼ã‚¹å' in df.columns:
                df = self._estimate_grade_from_race_name_fallback(df, grade_column)
            
            # 3. è·é›¢ãƒ»å‡ºèµ°é ­æ•°ã‹ã‚‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            df = self._estimate_grade_from_features_fallback(df, grade_column)
            
            # 4. æœ€çµ‚çš„ã«æ¨å®šã§ããªã„å ´åˆã¯æ¡ä»¶æˆ¦ï¼ˆ5ï¼‰ã¨ã—ã¦è¨­å®š
            final_missing = df[grade_column].isnull().sum()
            if final_missing > 0:
                logger.info(f"      ğŸ¯ æœ€çµ‚æ¨å®šå¤±æ•—{final_missing:,}ä»¶ã‚’æ¡ä»¶æˆ¦ï¼ˆ5ï¼‰ã¨ã—ã¦è¨­å®š")
                df.loc[df[grade_column].isnull(), grade_column] = 5
                self.processing_log.append(f"{grade_column}: æœ€çµ‚æ¨å®šå¤±æ•—{final_missing}ä»¶â†’æ¡ä»¶æˆ¦(5)è¨­å®š")

        return df
    
    def _estimate_grade_from_base_prize(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """æœ¬è³é‡‘ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼‰"""
        if 'æœ¬è³é‡‘' not in df.columns:
            return df
        
        df['æœ¬è³é‡‘'] = pd.to_numeric(df['æœ¬è³é‡‘'], errors='coerce')
        
        # æœ¬è³é‡‘ãƒ™ãƒ¼ã‚¹ã®ã—ãã„å€¤ï¼ˆformattedãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã«åŸºã¥ãå®Ÿè¨¼çš„åŸºæº–ï¼‰
        # G1ã‚’ãƒ¬ãƒ™ãƒ«åˆ¥ã«åˆ†ã‘ã‚‹
        base_thresholds = [
            (10000, 1),  # G1æœ€é«˜ãƒ¬ãƒ™ãƒ«: 10,000ä¸‡å††ä»¥ä¸Šï¼ˆã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—ãƒ»æœ‰é¦¬è¨˜å¿µãƒ¬ãƒ™ãƒ«ï¼‰
            (5000, 11),  # G1é«˜ãƒ¬ãƒ™ãƒ«: 5,000ä¸‡å††ä»¥ä¸Šï¼ˆå¤©çš‡è³ãƒ»å®å¡šè¨˜å¿µãƒ¬ãƒ™ãƒ«ï¼‰
            (2000, 12),  # G1æ¨™æº–ãƒ¬ãƒ™ãƒ«: 2,000ä¸‡å††ä»¥ä¸Šï¼ˆçšæœˆè³ãƒ»èŠèŠ±è³ãƒ¬ãƒ™ãƒ«ï¼‰
            (1000, 2),   # G2: 1,000ä¸‡å††ä»¥ä¸Šï¼ˆG2ãƒ¬ãƒ¼ã‚¹ï¼‰
            (500, 3),    # G3: 500ä¸‡å††ä»¥ä¸Šï¼ˆG3ãƒ¬ãƒ¼ã‚¹ï¼‰
            (200, 6),    # L: 200ä¸‡å††ä»¥ä¸Š
            (100, 5)     # ç‰¹åˆ¥: 100ä¸‡å††ä»¥ä¸Š
        ]
        
        for min_prize, grade_value in base_thresholds:
            mask = (df['æœ¬è³é‡‘'] >= min_prize) & df[grade_column].isnull()
            df.loc[mask, grade_column] = grade_value
        
        # æœ¬è³é‡‘ã§æ¨å®šã§ããªã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰æ¨å®š
        remaining_missing = df[grade_column].isnull().sum()
        if remaining_missing > 0 and 'ãƒ¬ãƒ¼ã‚¹å' in df.columns:
            logger.info(f"      ğŸ”§ æœ¬è³é‡‘ã§æ¨å®šã§ããªã‹ã£ãŸ{remaining_missing:,}ä»¶ã‚’ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰æ¨å®šä¸­...")
            df = self._estimate_grade_from_race_name_fallback(df, grade_column)
        
        return df
    
    def _calculate_horse_age_from_registration(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¡€çµ±ç™»éŒ²ç•ªå·ã¨å¹´æœˆæ—¥ã‹ã‚‰é¦¬é½¢ã‚’è¨ˆç®—ã—ã¦åˆ—ã‚’è¿½åŠ """
        try:
            from datetime import datetime
            
            # å¿…è¦ãªåˆ—ã®ç¢ºèª
            if 'è¡€çµ±ç™»éŒ²ç•ªå·' not in df.columns or 'å¹´æœˆæ—¥' not in df.columns:
                logger.warning("âš ï¸ è¡€çµ±ç™»éŒ²ç•ªå·ã¾ãŸã¯å¹´æœˆæ—¥åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return df
            
            # é¦¬é½¢åˆ—ã‚’åˆæœŸåŒ–
            df['é¦¬é½¢'] = None
            
            # é¦¬ã”ã¨ã«æœ€åˆã®ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
            horse_first_race = df.groupby('é¦¬å').first()
            
            horse_age_map = {}
            
            for horse_name, row in horse_first_race.iterrows():
                try:
                    # è¡€çµ±ç™»éŒ²ç•ªå·ã‹ã‚‰ç”Ÿå¹´æœˆæ—¥ã‚’æ¨å®š
                    registration_number = str(row['è¡€çµ±ç™»éŒ²ç•ªå·'])
                    race_date_str = str(row['å¹´æœˆæ—¥'])
                    
                    # è¡€çµ±ç™»éŒ²ç•ªå·ã®æœ€åˆã®2æ¡ãŒç”Ÿå¹´ï¼ˆè¥¿æš¦ï¼‰
                    if len(registration_number) >= 2:
                        birth_year = int(registration_number[:2])
                        
                        # 2æ¡å¹´ã‚’4æ¡å¹´ã«å¤‰æ›ï¼ˆ00-30ã¯2000å¹´ä»£ã€31-99ã¯1900å¹´ä»£ï¼‰
                        if birth_year <= 30:
                            birth_year += 2000
                        else:
                            birth_year += 1900
                        
                        # ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜ã‚’è§£æ
                        if len(race_date_str) == 8:  # YYYYMMDDå½¢å¼
                            race_year = int(race_date_str[:4])
                            race_month = int(race_date_str[4:6])
                            race_day = int(race_date_str[6:8])
                            
                            # é¦¬é½¢è¨ˆç®—ï¼ˆç«¶é¦¬ã§ã¯1æœˆ1æ—¥ã‚’åŸºæº–ã¨ã™ã‚‹ï¼‰
                            if race_month >= 1:
                                age = race_year - birth_year
                            else:
                                age = race_year - birth_year - 1
                            
                            # å¹´é½¢ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆ2-20æ­³ã®ç¯„å›²ï¼‰
                            if 2 <= age <= 20:
                                horse_age_map[horse_name] = age
                            else:
                                logger.debug(f"âš ï¸ ç•°å¸¸ãªå¹´é½¢: {horse_name} (ç”Ÿå¹´:{birth_year}, ãƒ¬ãƒ¼ã‚¹å¹´:{race_year}, è¨ˆç®—å¹´é½¢:{age})")
                                horse_age_map[horse_name] = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                        else:
                            logger.debug(f"âš ï¸ æ—¥ä»˜å½¢å¼ã‚¨ãƒ©ãƒ¼: {horse_name} - {race_date_str}")
                            horse_age_map[horse_name] = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    else:
                        logger.debug(f"âš ï¸ è¡€çµ±ç™»éŒ²ç•ªå·å½¢å¼ã‚¨ãƒ©ãƒ¼: {horse_name} - {registration_number}")
                        horse_age_map[horse_name] = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                        
                except (ValueError, TypeError) as e:
                    logger.debug(f"âš ï¸ å¹´é½¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {horse_name} - {str(e)}")
                    horse_age_map[horse_name] = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # é¦¬é½¢åˆ—ã«å€¤ã‚’è¨­å®š
            df['é¦¬é½¢'] = df['é¦¬å'].map(horse_age_map)
            
            # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            age_counts = {}
            for age in horse_age_map.values():
                age_counts[age] = age_counts.get(age, 0) + 1
            
            logger.info(f"âœ… é¦¬é½¢è¨ˆç®—å®Œäº†: {len(horse_age_map)}é ­")
            logger.info(f"ğŸ“Š å¹´é½¢åˆ†å¸ƒ: {dict(sorted(age_counts.items()))}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ é¦¬é½¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return df
    
    def _estimate_grade_from_race_name_fallback(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼‰"""
        if 'ãƒ¬ãƒ¼ã‚¹å' not in df.columns:
            return df
        
        # formattedãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã«åŸºã¥ãåŒ…æ‹¬çš„ãªãƒ¬ãƒ¼ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³
        # åˆ†æçµæœã‹ã‚‰åˆ¤æ˜ã—ãŸå®Ÿéš›ã®G1ãƒ¬ãƒ¼ã‚¹åã‚’ç¶²ç¾…çš„ã«è¿½åŠ 
        race_patterns = {
            1: [
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ50,000ä¸‡å††ä»¥ä¸Šï¼‰
                'ã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—', 'æœ‰é¦¬è¨˜å¿µ',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ30,000ä¸‡å††ä»¥ä¸Šï¼‰
                'å¤§é˜ªæ¯', 'æ±äº¬å„ªé§¿',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ22,000ä¸‡å††ä»¥ä¸Šï¼‰
                'å¤©çš‡è³', 'å®å¡šè¨˜å¿µ',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ20,000ä¸‡å††ä»¥ä¸Šï¼‰
                'çšæœˆè³', 'èŠèŠ±è³',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ18,000ä¸‡å††ä»¥ä¸Šï¼‰
                'å®‰ç”°è¨˜å¿µ', 'ãƒã‚¤ãƒ«ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚·ãƒƒãƒ—',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ17,000ä¸‡å††ä»¥ä¸Šï¼‰
                'é«˜æ¾å®®è¨˜å¿µ', 'ã‚¹ãƒ—ãƒªãƒ³ã‚¿ãƒ¼ã‚ºã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ15,000ä¸‡å††ä»¥ä¸Šï¼‰
                'å„ªé§¿ç‰é¦¬',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ14,000ä¸‡å††ä»¥ä¸Šï¼‰
                'æ¡œèŠ±è³',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ13,000ä¸‡å††ä»¥ä¸Šï¼‰
                'ãƒ´ã‚£ã‚¯ãƒˆãƒªã‚¢ãƒã‚¤ãƒ«', 'ã‚¨ãƒªã‚¶ãƒ™ã‚¹å¥³ç‹æ¯', 'ã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—ãƒ€ãƒ¼ãƒˆ', 'ï¼®ï¼¨ï¼«ãƒã‚¤ãƒ«ã‚«ãƒƒãƒ—',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ12,000ä¸‡å††ä»¥ä¸Šï¼‰
                'ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚ºã‚«ãƒƒãƒ—', 'ãƒ•ã‚§ãƒ–ãƒ©ãƒªãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ11,000ä¸‡å††ä»¥ä¸Šï¼‰
                'ç§‹è¯è³',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ9,000ä¸‡å††ä»¥ä¸Šï¼‰
                'ï¼ªï¼¢ï¼£ã‚¯ãƒ©ã‚·ãƒƒã‚¯',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ7,500ä¸‡å††ä»¥ä¸Šï¼‰
                'ä¸­å±±ã‚°ãƒ©ãƒ³ãƒ‰ã‚¸ãƒ£ãƒ³ãƒ—', 'ä¸­å±±å¤§éšœå®³',
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG1ãƒ¬ãƒ¼ã‚¹ï¼ˆ7,000ä¸‡å††ä»¥ä¸Šï¼‰
                'æœæ—¥æ¯ãƒ•ãƒ¥ãƒ¼ãƒãƒ¥ãƒªãƒ†ã‚£ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ï¼ªï¼¢ï¼£ã‚¹ãƒ—ãƒªãƒ³ãƒˆ',
                # ãã®ä»–ã®G1ãƒ¬ãƒ¼ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆäºˆæ¸¬ï¼‰
                'ãƒ€ãƒ¼ãƒ“ãƒ¼', 'ã‚ªãƒ¼ã‚¯ã‚¹', 'ãƒã‚¤ãƒ«', 'ãƒ•ãƒ¥ãƒ¼ãƒãƒ¥ãƒªãƒ†ã‚£', 'ãƒ•ãƒ¥ãƒ¼ãƒãƒ¥ãƒªãƒ†ã‚£ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹',
                # äºˆæ¸¬ã•ã‚Œã‚‹G1ãƒ¬ãƒ¼ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³
                'ã‚¯ãƒ©ã‚·ãƒƒã‚¯', 'ã‚¯ãƒ©ã‚·ãƒƒã‚¯ä¸‰å† ', 'ç‰é¦¬ä¸‰å† ', 'ç‰é¦¬ã‚¯ãƒ©ã‚·ãƒƒã‚¯',
                'ãƒã‚¤ãƒ«ç‹åº§', 'ã‚¹ãƒ—ãƒªãƒ³ãƒˆç‹åº§', 'é•·è·é›¢ç‹åº§', 'ä¸­è·é›¢ç‹åº§',
                'å›½éš›', 'ãƒ¯ãƒ¼ãƒ«ãƒ‰', 'ã‚°ãƒ­ãƒ¼ãƒãƒ«', 'ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³', 'ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚·ãƒƒãƒ—',
                'ã‚°ãƒ©ãƒ³ãƒ—ãƒª', 'ã‚°ãƒ©ãƒ³ãƒ‰', 'ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«', 'ã‚«ãƒƒãƒ—', 'ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹',
                # äºˆæ¸¬ã•ã‚Œã‚‹éšœå®³G1ãƒ¬ãƒ¼ã‚¹
                'ã‚°ãƒ©ãƒ³ãƒ‰ã‚¸ãƒ£ãƒ³ãƒ—', 'å¤§éšœå®³', 'éšœå®³', 'ãƒãƒ¼ãƒ‰ãƒ«',
                # äºˆæ¸¬ã•ã‚Œã‚‹åœ°æ–¹G1ãƒ¬ãƒ¼ã‚¹
                'åœ°æ–¹', 'ãƒ€ãƒ¼ãƒˆ', 'ãƒ€ãƒ¼ãƒˆç‹åº§', 'ãƒ€ãƒ¼ãƒˆãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³',
                # äºˆæ¸¬ã•ã‚Œã‚‹å¹´é½¢åˆ¥G1ãƒ¬ãƒ¼ã‚¹
                '2æ­³', '3æ­³', '4æ­³', 'å¤é¦¬', 'ç‰é¦¬é™å®š', 'ç‰¡é¦¬é™å®š',
                # äºˆæ¸¬ã•ã‚Œã‚‹è·é›¢åˆ¥G1ãƒ¬ãƒ¼ã‚¹
                'çŸ­è·é›¢', 'ãƒã‚¤ãƒ«', 'ä¸­è·é›¢', 'é•·è·é›¢', 'è¶…é•·è·é›¢'
            ],
            2: [
                # åˆ†æçµæœã§ç¢ºèªã•ã‚ŒãŸG2ãƒ¬ãƒ¼ã‚¹
                'æœ­å¹Œè¨˜å¿µ', 'é˜ªç¥ã‚«ãƒƒãƒ—',
                # äºˆæ¸¬ã•ã‚Œã‚‹G2ãƒ¬ãƒ¼ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³
                'è¨˜å¿µ', 'å¤§è³å…¸', 'ç‹å† ', 'ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ã‚«ãƒƒãƒ—',
                'æº–é‡è³', 'æº–G1', 'G2', 'é‡è³', 'ã‚ªãƒ¼ãƒ—ãƒ³ç‰¹åˆ¥',
                # äºˆæ¸¬ã•ã‚Œã‚‹åœ°æ–¹G2ãƒ¬ãƒ¼ã‚¹
                'åœ°æ–¹é‡è³', 'åœ°æ–¹è¨˜å¿µ', 'åœ°æ–¹ã‚«ãƒƒãƒ—',
                # äºˆæ¸¬ã•ã‚Œã‚‹éšœå®³G2ãƒ¬ãƒ¼ã‚¹
                'éšœå®³é‡è³', 'éšœå®³è¨˜å¿µ', 'éšœå®³ã‚«ãƒƒãƒ—'
            ],
            3: ['è³', 'ç‰¹åˆ¥'],
            4: ['é‡è³', 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰', 'L'],
            5: ['æ¡ä»¶', 'æ–°é¦¬', 'æœªå‹åˆ©', '1å‹ã‚¯ãƒ©ã‚¹', '2å‹ã‚¯ãƒ©ã‚¹', '3å‹ã‚¯ãƒ©ã‚¹']
        }
        
        for grade, patterns in race_patterns.items():
            for pattern in patterns:
                mask = (df['ãƒ¬ãƒ¼ã‚¹å'].str.contains(pattern, case=False, na=False)) & df[grade_column].isnull()
                df.loc[mask, grade_column] = grade
        
        return df
    
    def _estimate_grade_from_features_fallback(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """è·é›¢ãƒ»å‡ºèµ°é ­æ•°ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼‰"""
        # è·é›¢ã«ã‚ˆã‚‹æ¨å®š
        if 'è·é›¢' in df.columns:
            df['è·é›¢'] = pd.to_numeric(df['è·é›¢'], errors='coerce')
            
            # é•·è·é›¢ãƒ¬ãƒ¼ã‚¹ï¼ˆ3000mä»¥ä¸Šï¼‰ã¯é‡è³ã®å¯èƒ½æ€§ãŒé«˜ã„
            long_distance_mask = (df['è·é›¢'] >= 3000) & df[grade_column].isnull()
            df.loc[long_distance_mask, grade_column] = 4  # é‡è³
            
            # æ¥µç«¯ãªçŸ­è·é›¢ï¼ˆ1000mæœªæº€ï¼‰ã¯ç‰¹åˆ¥ãƒ¬ãƒ¼ã‚¹
            short_distance_mask = (df['è·é›¢'] < 1000) & df[grade_column].isnull()
            df.loc[short_distance_mask, grade_column] = 5  # ç‰¹åˆ¥
        
        # å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹æ¨å®š
        if 'é ­æ•°' in df.columns:
            df['é ­æ•°'] = pd.to_numeric(df['é ­æ•°'], errors='coerce')
            
            # å‡ºèµ°é ­æ•°ãŒå¤šã„ï¼ˆ16é ­ä»¥ä¸Šï¼‰ã¯é‡è³ã®å¯èƒ½æ€§
            large_field_mask = (df['é ­æ•°'] >= 16) & df[grade_column].isnull()
            df.loc[large_field_mask, grade_column] = 4  # é‡è³
            
            # å‡ºèµ°é ­æ•°ãŒå°‘ãªã„ï¼ˆ8é ­æœªæº€ï¼‰ã¯æ¡ä»¶æˆ¦
            small_field_mask = (df['é ­æ•°'] < 8) & df[grade_column].isnull()
            df.loc[small_field_mask, grade_column] = 5  # æ¡ä»¶æˆ¦
        
        return df
    
    def _estimate_grade_from_race_name(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰"""
        if 'ãƒ¬ãƒ¼ã‚¹å' not in df.columns:
            return df
        
        # ãƒ¬ãƒ¼ã‚¹åã®ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰
        race_patterns = {
            1: [  # G1ãƒ‘ã‚¿ãƒ¼ãƒ³
                'ãƒ€ãƒ¼ãƒ“ãƒ¼', 'ã‚ªãƒ¼ã‚¯ã‚¹', 'èŠèŠ±è³', 'çšæœˆè³', 'æ¡œèŠ±è³', 'ãƒã‚¤ãƒ«', 
                'æœ‰é¦¬è¨˜å¿µ', 'å®å¡šè¨˜å¿µ', 'å¤©çš‡è³', 'ã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—', 'ã‚¹ãƒ—ãƒªãƒ³ã‚¿ãƒ¼ã‚º',
                'ã‚¨ãƒªã‚¶ãƒ™ã‚¹å¥³ç‹æ¯', 'ãƒ•ã‚§ãƒ–ãƒ©ãƒªãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚ºã‚«ãƒƒãƒ—',
                'é«˜æ¾å®®è¨˜å¿µ', 'å®‰ç”°è¨˜å¿µ', 'ãƒ´ã‚£ã‚¯ãƒˆãƒªã‚¢', 'ç§‹è¯è³'
            ],
            2: [  # G2ãƒ‘ã‚¿ãƒ¼ãƒ³  
                'äº¬éƒ½è¨˜å¿µ', 'é˜ªç¥å¤§è³å…¸', 'ç›®é»’è¨˜å¿µ', 'æ¯æ—¥ç‹å† ', 'äº¬éƒ½å¤§è³å…¸',
                'ã‚¢ãƒ«ã‚¼ãƒ³ãƒãƒ³å…±å’Œå›½æ¯', 'ä¸­å±±è¨˜å¿µ', 'é‡‘é¯±è³', 'äº¬ç‹æ¯', 'åºœä¸­ç‰é¦¬',
                'ã‚»ãƒ³ãƒˆã‚¦ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ã‚¹ãƒ¯ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'å°å€‰è¨˜å¿µ'
            ],
            3: [  # G3ãƒ‘ã‚¿ãƒ¼ãƒ³
                'å‡½é¤¨è¨˜å¿µ', 'ä¸­äº¬è¨˜å¿µ', 'æ–°æ½Ÿè¨˜å¿µ', 'ä¸ƒå¤•è³', 'ç¦å³¶è¨˜å¿µ', 
                'ãã•ã‚‰ãè³', 'å¼¥ç”Ÿè³', 'ã‚¹ãƒ—ãƒªãƒ³ã‚°', 'ã‚»ãƒ³ãƒˆãƒ©ã‚¤ãƒˆ', 'ã‚¢ãƒ«ãƒ†ãƒŸã‚¹',
                'æœæ—¥æ¯', 'ãƒ›ãƒ¼ãƒ—ãƒ•ãƒ«', 'ãƒ©ã‚¸ã‚ª', 'ã‚¯ã‚¤ãƒ¼ãƒ³', 'ã‚ªãƒ¼ãƒ—ãƒ³'
            ],
            4: [  # é‡è³ï¼ˆãƒªã‚¹ãƒ†ãƒƒãƒ‰ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³
                'é‡è³', 'ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ã‚«ãƒƒãƒ—', 'è³', 'è¨˜å¿µ', 'ç‰¹åˆ¥',
                'ã‚ªãƒ¼ãƒ—ãƒ³', 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰', 'L'
            ]
        }
        
        for grade, patterns in race_patterns.items():
            for pattern in patterns:
                mask = (df['ãƒ¬ãƒ¼ã‚¹å'].str.contains(pattern, case=False, na=False)) & df[grade_column].isnull()
                df.loc[mask, grade_column] = grade
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è£œå®Œã¯è¡Œã‚ãªã„ï¼ˆæ¨å®šå¤±æ•—ã®å ´åˆã¯å¾Œã§ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤ï¼‰
        
        return df
    
    def _adjust_grade_by_field_size(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹ã‚°ãƒ¬ãƒ¼ãƒ‰è£œæ­£ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«èª¿æ•´ï¼‰"""
        if 'é ­æ•°' not in df.columns:
            return df
        
        df['é ­æ•°'] = pd.to_numeric(df['é ­æ•°'], errors='coerce')
        
        # å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯
        # å¤§ããªãƒ¬ãƒ¼ã‚¹ã»ã©å‡ºèµ°é ­æ•°ãŒå¤šã„å‚¾å‘
        for idx, row in df.iterrows():
            if pd.notnull(row[grade_column]) and pd.notnull(row['é ­æ•°']):
                current_grade = row[grade_column]
                field_size = row['é ­æ•°']
                
                # å‡ºèµ°é ­æ•°ãŒç•°å¸¸ã«å°‘ãªã„å ´åˆã¯ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ä¸‹ã’ã‚‹
                if field_size < 8 and current_grade <= 3:  # G3ä»¥ä¸Šã§8é ­æœªæº€ã¯æ€ªã—ã„
                    df.loc[idx, grade_column] = min(current_grade + 1, 6)
                # å‡ºèµ°é ­æ•°ãŒå¤šã„å ´åˆã¯ã‚°ãƒ¬ãƒ¼ãƒ‰ç¶­æŒã¾ãŸã¯å‘ä¸Š
                elif field_size >= 16 and current_grade >= 5:  # 16é ­ä»¥ä¸Šã§æ¡ä»¶æˆ¦ã¯é‡è³ã®å¯èƒ½æ€§
                    df.loc[idx, grade_column] = max(current_grade - 1, 4)
        
        return df
    
    def _adjust_grade_by_distance(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """è·é›¢ã«ã‚ˆã‚‹ã‚°ãƒ¬ãƒ¼ãƒ‰è£œæ­£ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«èª¿æ•´ï¼‰"""
        if 'è·é›¢' not in df.columns:
            return df
        
        df['è·é›¢'] = pd.to_numeric(df['è·é›¢'], errors='coerce')
        
        # è·é›¢ã«ã‚ˆã‚‹è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯
        # ç‰¹æ®Šè·é›¢ï¼ˆ3000mä»¥ä¸Šï¼‰ã¯é‡è³ã®å¯èƒ½æ€§ãŒé«˜ã„
        for idx, row in df.iterrows():
            if pd.notnull(row[grade_column]) and pd.notnull(row['è·é›¢']):
                current_grade = row[grade_column]
                distance = row['è·é›¢']
                
                # é•·è·é›¢ãƒ¬ãƒ¼ã‚¹ï¼ˆ3000mä»¥ä¸Šï¼‰ã®å ´åˆ
                if distance >= 3000 and current_grade >= 5:
                    df.loc[idx, grade_column] = min(current_grade - 1, 4)  # é‡è³ä»¥ä¸Šã«æ ¼ä¸Šã’
                
                # æ¥µç«¯ãªçŸ­è·é›¢ï¼ˆ1000mæœªæº€ï¼‰ã‚„é•·è·é›¢ï¼ˆ3600mè¶…ï¼‰ã¯ç‰¹åˆ¥ãƒ¬ãƒ¼ã‚¹
                if (distance < 1000 or distance > 3600) and current_grade >= 4:
                    df.loc[idx, grade_column] = min(current_grade - 1, 3)  # G3ä»¥ä¸Šã«æ ¼ä¸Šã’
        
        return df
    
    def _add_grade_name_column(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """
        æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‹ã‚‰ã€Œã‚°ãƒ¬ãƒ¼ãƒ‰åã€åˆ—ã‚’ä½œæˆ
        
        Args:
            df: å‡¦ç†å¯¾è±¡DataFrame
            grade_column: ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—å
            
        Returns:
            ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
        """
        # ã‚°ãƒ¬ãƒ¼ãƒ‰å¤‰æ›ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜æº–æ‹ ï¼‰
        grade_mapping = {
            1: 'ï¼§ï¼‘',
            2: 'ï¼§ï¼’', 
            3: 'ï¼§ï¼“',
            4: 'é‡è³',
            5: 'ç‰¹åˆ¥',
            6: 'ï¼¬ï¼ˆãƒªã‚¹ãƒ†ãƒƒãƒ‰ï¼‰'
        }
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã‚’æ•°å€¤å‹ã¨ã—ã¦ä¿æŒï¼ˆå…ƒã®åˆ—ã¯ãã®ã¾ã¾ï¼‰
        df[grade_column] = pd.to_numeric(df[grade_column], errors='coerce')
        
        # NaNå€¤ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè£œå®Œã¯è¡Œã‚ãªã„
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆNaNå€¤ã¯ãã®ã¾ã¾ä¿æŒï¼‰
        grade_names = df[grade_column].map(grade_mapping)
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if 'ã‚°ãƒ¬ãƒ¼ãƒ‰å' in df.columns:
            # æ—¢å­˜ã®åˆ—ã‚’æ›´æ–°
            df['ã‚°ãƒ¬ãƒ¼ãƒ‰å'] = grade_names
        else:
            # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ç›´å¾Œã«ã€Œã‚°ãƒ¬ãƒ¼ãƒ‰åã€åˆ—ã‚’æŒ¿å…¥
            grade_col_index = df.columns.get_loc(grade_column)
            df.insert(grade_col_index + 1, 'ã‚°ãƒ¬ãƒ¼ãƒ‰å', grade_names)
        
        return df
    
    def _save_processing_log(self, df: pd.DataFrame):
        """å‡¦ç†ãƒ­ã‚°ã®ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰"""
        log_path = Path('export/missing_value_processing_log.txt')
        
        try:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            write_header = not log_path.exists()
            
            with open(log_path, 'a', encoding='utf-8') as f:  # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
                if write_header:
                    f.write(f"æ¬ æå€¤å‡¦ç†ãƒ­ã‚° - {datetime.now()}\n")
                    f.write("=" * 50 + "\n\n")
                
                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãƒ­ã‚°ã‚’è¿½è¨˜
                for log_entry in self.processing_log:
                    f.write(f"â€¢ {log_entry}\n")
                
                # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚’è¿½è¨˜
                f.write(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}\n")
                f.write(f"æ®‹å­˜æ¬ æå€¤: {df.isnull().sum().sum()}ä»¶\n\n")
            
            logger.info(f"   ğŸ“ å‡¦ç†ãƒ­ã‚°ä¿å­˜: {log_path}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ å‡¦ç†ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¯ãƒ©ã‚¹ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
    
    def __init__(self):
        self.start_time = time.time()
    
    def log_system_status(self, stage_name: str):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        
        logger.info(f"ğŸ’» [{stage_name}] ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:")
        logger.info(f"   â±ï¸ çµŒéæ™‚é–“: {elapsed_time:.1f}ç§’")

def ensure_export_dirs():
    """
    å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªã¨ä½œæˆ
    å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ç®¡ç†æ©Ÿèƒ½ä»˜ã
    """
    dirs = [
        'export/BAC', 
        'export/SRB', 
        'export/SED', 
        'export/dataset',          # å®Ÿéš›ã®SED+SRBçµ±åˆãƒ‡ãƒ¼ã‚¿å‡ºåŠ›å…ˆ
        'export/quality_reports',     # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ç”¨
        'export/logs'                 # ãƒ­ã‚°ä¿å­˜ç”¨
    ]
    
    created_dirs = []
    
    for dir_path in dirs:
        path_obj = Path(dir_path)
        if not path_obj.exists():
            path_obj.mkdir(parents=True, exist_ok=True)
            created_dirs.append(dir_path)
            logger.info(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {dir_path}")
    
    if created_dirs:
        logger.info(f"âœ… {len(created_dirs)}å€‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ")
    else:
        logger.info("ğŸ“ ã™ã¹ã¦ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™")

def save_quality_report(quality_checker: DataQualityChecker):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
    report_path = Path('export/quality_reports/data_quality_report.json')
    
    try:
        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(quality_checker.quality_report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        
    except Exception as e:
        logger.warning(f"âš ï¸ å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

def display_deletion_statistics():
    """
    ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æã«ã‚ˆã‚‹å‰Šé™¤çµ±è¨ˆã®è¡¨ç¤º
    SEDã¨datasetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¯”è¼ƒã—ã¦å‰Šé™¤çµ±è¨ˆã‚’å‡ºåŠ›
    """
    try:
        from pathlib import Path
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        sed_dir = Path('export/SED/formatted')
        bias_dir = Path('export/dataset')
        
        if not sed_dir.exists() or not bias_dir.exists():
            logger.warning("âš ï¸ æ¯”è¼ƒç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        sed_files = list(sed_dir.glob('*.csv'))
        bias_files = list(bias_dir.glob('*.csv'))
        
        if not sed_files or not bias_files:
            logger.warning("âš ï¸ æ¯”è¼ƒç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # çµ±è¨ˆã‚’åé›†
        total_sed = 0
        total_bias = 0
        total_deleted = 0
        deletion_files = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒãƒƒãƒ”ãƒ³ã‚°
        sed_files_dict = {f.stem.replace('_formatted', ''): f for f in sed_files}
        
        for bias_file in bias_files:
            base_name = bias_file.stem.replace('_formatted_dataset', '')
            
            if base_name in sed_files_dict:
                sed_file = sed_files_dict[base_name]
                
                try:
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’æ•°ãˆã‚‹ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰
                    with open(sed_file, 'r', encoding='utf-8') as f:
                        sed_count = sum(1 for line in f) - 1
                    
                    with open(bias_file, 'r', encoding='utf-8') as f:
                        bias_count = sum(1 for line in f) - 1
                    
                    deleted = sed_count - bias_count
                    total_sed += sed_count
                    total_bias += bias_count
                    total_deleted += deleted
                    
                    if deleted > 0:
                        deletion_rate = (deleted / sed_count * 100) if sed_count > 0 else 0
                        deletion_files.append({
                            'file': base_name,
                            'deleted': deleted,
                            'deletion_rate': deletion_rate
                        })
                
                except Exception:
                    continue
        
        # çµ±è¨ˆè¡¨ç¤º
        logger.info("ğŸ“ˆ å…¨ä½“å‰Šé™¤çµ±è¨ˆ:")
        logger.info(f"   ğŸ“¥ å‡¦ç†å‰ç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {total_sed:,}ä»¶")
        logger.info(f"   ğŸ“¤ å‡¦ç†å¾Œç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {total_bias:,}ä»¶")
        logger.info(f"   âŒ å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_deleted:,}ä»¶")
        logger.info(f"   ğŸ“‰ å…¨ä½“å‰Šé™¤ç‡: {(total_deleted/total_sed*100 if total_sed > 0 else 0):.2f}%")
        logger.info(f"   ğŸ—‚ï¸ å‰Šé™¤ç™ºç”Ÿãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(deletion_files)}")
        logger.info(f"   ğŸ“Š å‰Šé™¤ç™ºç”Ÿç‡: {(len(deletion_files)/len(sed_files_dict)*100 if sed_files_dict else 0):.1f}%")
        
        if deletion_files:
            logger.info("\nğŸ“‹ å‰Šé™¤ã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸Šä½10ä»¶ï¼‰:")
            deletion_files.sort(key=lambda x: x['deleted'], reverse=True)
            for i, item in enumerate(deletion_files[:10], 1):
                logger.info(f"   {i:2d}. {item['file']}: -{item['deleted']:,}ä»¶ (-{item['deletion_rate']:.1f}%)")
        else:
            logger.info("âœ… ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æã«ã‚ˆã‚‹å‰Šé™¤ã¯ç™ºç”Ÿã—ã¦ã„ã¾ã›ã‚“")
    
    except Exception as e:
        logger.warning(f"âš ï¸ å‰Šé™¤çµ±è¨ˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(e)}")

def summarize_processing_log():
    """
    å®Ÿå‹™ãƒ¬ãƒ™ãƒ«æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
    å†—é•·ãªãƒ­ã‚°ã‚’ã¾ã¨ã‚ã¦çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ
    """
    log_file = Path('export/missing_value_processing_log.txt')
    backup_file = Path('export/missing_value_processing_log_original.txt')
    summary_file = Path('export/missing_value_processing_summary.txt')
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if not log_file.exists():
        logger.info("ğŸ“ æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return
    
    logger.info("ğŸ“Š æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã‚’ã‚µãƒãƒªãƒ¼å½¢å¼ã«æ•´ç†ä¸­...")
    
    try:
        # ãƒ­ã‚°è§£æ
        stats = _parse_processing_log(log_file)
        
        if not stats:
            logger.warning("âš ï¸ ãƒ­ã‚°è§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        _generate_summary_report(stats, summary_file)
        
        # å…ƒãƒ­ã‚°ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if backup_file.exists():
            backup_file.unlink()  # æ—¢å­˜ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤
        log_file.rename(backup_file)
        
        # ã‚µãƒãƒªãƒ¼ã‚’æ–°ã—ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«
        summary_file.rename(log_file)
        
        logger.info("âœ… æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®æ•´ç†å®Œäº†")
        logger.info(f"   ğŸ“‹ ã‚µãƒãƒªãƒ¼: {log_file}")
        logger.info(f"   ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
        logger.info(f"   ğŸ“Š å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
        if stats['idm_deletions']:
            total_idm = sum(stats['idm_deletions'])
            logger.info(f"   ğŸ¯ IDMå‰Šé™¤: {total_idm:,}è¡Œ ({len(stats['idm_deletions'])}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
        if stats['grade_estimations']:
            total_grade = sum(stats['grade_estimations'])
            logger.info(f"   ğŸ† ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š: {total_grade:,}ä»¶ ({len(stats['grade_estimations'])}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ­ã‚°ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

def _parse_processing_log(log_file: Path) -> Dict[str, Any]:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦å‡¦ç†çµ±è¨ˆã‚’ä½œæˆ"""
    
    # çµ±è¨ˆæƒ…å ±æ ¼ç´ç”¨
    stats = {
        'idm_deletions': [],
        'grade_estimations': [],
        'median_imputations': defaultdict(list),
        'dropped_columns': set(),
        'categorical_imputations': defaultdict(list),
        'other_imputations': defaultdict(list),
        'total_files': 0,
        'final_shapes': []
    }
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        logger.error(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('==') or line.startswith('æ¬ æå€¤å‡¦ç†ãƒ­ã‚°'):
            continue
            
        # IDMå‰Šé™¤
        if 'IDM:' in line and 'è¡Œã‚’å‰Šé™¤ï¼ˆé‡è¦åˆ—ï¼‰' in line:
            match = re.search(r'IDM: (\d+)è¡Œã‚’å‰Šé™¤', line)
            if match:
                stats['idm_deletions'].append(int(match.group(1)))
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š
        elif 'ã‚°ãƒ¬ãƒ¼ãƒ‰:' in line and 'æ¨å®šâ†’ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ' in line:
            match = re.search(r'ã‚°ãƒ¬ãƒ¼ãƒ‰: è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰(\d+)ä»¶æ¨å®š', line)
            if match:
                stats['grade_estimations'].append(int(match.group(1)))
        
        # ä¸­å¤®å€¤è£œå®Œ
        elif 'medianã§' in line and 'ä»¶è£œå®Œ' in line:
            match = re.search(r'â€¢ ([^:]+): medianã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                count = int(match.group(2))
                stats['median_imputations'][column_name].append(count)
        
        # é«˜æ¬ æç‡ã«ã‚ˆã‚‹åˆ—å‰Šé™¤
        elif 'é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤' in line:
            match = re.search(r'â€¢ ([^:]+): é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤', line)
            if match:
                stats['dropped_columns'].add(match.group(1))
        
        # ã‚«ãƒ†ã‚´ãƒªè£œå®Œï¼ˆãƒ¬ãƒ¼ã‚¹åã€é¦¬ä½“é‡å¢—æ¸›ï¼‰
        elif line.startswith('â€¢ ãƒ¬ãƒ¼ã‚¹å:') or line.startswith('â€¢ ãƒ¬ãƒ¼ã‚¹åç•¥ç§°:') or line.startswith('â€¢ é¦¬ä½“é‡å¢—æ¸›:'):
            match = re.search(r'â€¢ ([^:]+): (.+)ã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                value = match.group(2)
                count = int(match.group(3))
                stats['categorical_imputations'][column_name].append((value, count))
        
        # ãã®ä»–ã®è£œå®Œå‡¦ç†
        elif 'ä»¶è£œå®Œ' in line and 'median' not in line:
            match = re.search(r'â€¢ ([^:]+): (.+)ã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                value = match.group(2)
                count = int(match.group(3))
                stats['other_imputations'][column_name].append((value, count))
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
        elif 'æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:' in line:
            match = re.search(r'æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: \((\d+), (\d+)\)', line)
            if match:
                rows = int(match.group(1))
                cols = int(match.group(2))
                stats['final_shapes'].append((rows, cols))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ¨å®šï¼ˆIDMå‰Šé™¤ã®å›æ•°ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šã®å›æ•°ã®åˆè¨ˆï¼‰
    stats['total_files'] = len(stats['idm_deletions']) + len(stats['grade_estimations'])
    
    return stats

def _generate_summary_report(stats: Dict[str, Any], output_file: Path):
    """çµ±è¨ˆæƒ…å ±ã‹ã‚‰ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ğŸ“Š æ¬ æå€¤å‡¦ç†ãƒ­ã‚° ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        f.write(f"ğŸ“ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        
        # IDMå‰Šé™¤çµ±è¨ˆ
        if stats['idm_deletions']:
            total_idm = sum(stats['idm_deletions'])
            f.write("ğŸ¯ IDMæ¬ æå€¤å‰Šé™¤å‡¦ç†:\n")
            f.write(f"   â€¢ å‡¦ç†å›æ•°: {len(stats['idm_deletions'])}å›\n")
            f.write(f"   â€¢ ç·å‰Šé™¤è¡Œæ•°: {total_idm:,}è¡Œ\n")
            f.write(f"   â€¢ å¹³å‡å‰Šé™¤è¡Œæ•°: {total_idm/len(stats['idm_deletions']):.1f}è¡Œ\n\n")
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šçµ±è¨ˆ
        if stats['grade_estimations']:
            total_grade = sum(stats['grade_estimations'])
            f.write("ğŸ† ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†:\n")
            f.write(f"   â€¢ å‡¦ç†å›æ•°: {len(stats['grade_estimations'])}å›\n")
            f.write(f"   â€¢ ç·æ¨å®šä»¶æ•°: {total_grade:,}ä»¶\n")
            f.write(f"   â€¢ å¹³å‡æ¨å®šä»¶æ•°: {total_grade/len(stats['grade_estimations']):.1f}ä»¶\n\n")
        
        # ä¸­å¤®å€¤è£œå®Œçµ±è¨ˆ
        if stats['median_imputations']:
            f.write("ğŸ”¢ ä¸­å¤®å€¤è£œå®Œå‡¦ç†:\n")
            for column, counts in stats['median_imputations'].items():
                total_count = sum(counts)
                f.write(f"   â€¢ {column}: {len(counts)}å›, ç·è£œå®Œ{total_count:,}ä»¶ (å¹³å‡{total_count/len(counts):.1f}ä»¶)\n")
            f.write("\n")
        
        # é«˜æ¬ æç‡åˆ—å‰Šé™¤
        if stats['dropped_columns']:
            f.write("âŒ é«˜æ¬ æç‡ã«ã‚ˆã‚Šå‰Šé™¤ã•ã‚ŒãŸåˆ—:\n")
            sorted_columns = sorted(stats['dropped_columns'])
            for i, column in enumerate(sorted_columns, 1):
                f.write(f"   {i:2d}. {column}\n")
            f.write(f"\n   ğŸ“Š å‰Šé™¤åˆ—æ•°: {len(sorted_columns)}åˆ—\n\n")
        
        # ã‚«ãƒ†ã‚´ãƒªè£œå®Œçµ±è¨ˆ
        if stats['categorical_imputations']:
            f.write("ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªè£œå®Œå‡¦ç†:\n")
            for column, values in stats['categorical_imputations'].items():
                total_count = sum(count for _, count in values)
                unique_values = len(set(value for value, _ in values))
                f.write(f"   â€¢ {column}: {len(values)}å›, ç·è£œå®Œ{total_count:,}ä»¶, {unique_values}ç¨®é¡ã®å€¤\n")
            f.write("\n")
        
        # ãã®ä»–è£œå®Œçµ±è¨ˆ
        if stats['other_imputations']:
            f.write("ğŸ”§ ãã®ä»–è£œå®Œå‡¦ç†:\n")
            for column, values in stats['other_imputations'].items():
                total_count = sum(count for _, count in values)
                f.write(f"   â€¢ {column}: {len(values)}å›, ç·è£œå®Œ{total_count:,}ä»¶\n")
            f.write("\n")
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        if stats['final_shapes']:
            total_rows = sum(rows for rows, _ in stats['final_shapes'])
            total_cols = sum(cols for _, cols in stats['final_shapes'])
            avg_rows = total_rows / len(stats['final_shapes']) if stats['final_shapes'] else 0
            avg_cols = total_cols / len(stats['final_shapes']) if stats['final_shapes'] else 0
            
            f.write("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\n")
            f.write(f"   â€¢ ç·è¡Œæ•°: {total_rows:,}è¡Œ\n")
            f.write(f"   â€¢ å¹³å‡è¡Œæ•°: {avg_rows:.1f}è¡Œ/ãƒ•ã‚¡ã‚¤ãƒ«\n")
            f.write(f"   â€¢ å¹³å‡åˆ—æ•°: {avg_cols:.1f}åˆ—/ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ‰ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«æ¬ æå€¤å‡¦ç† å®Œäº†ã‚µãƒãƒªãƒ¼\n")
        f.write("=" * 80 + "\n")

def process_race_data(exclude_turf=False, turf_only=False, 
                     enable_missing_value_handling=True, enable_quality_check=True):
    """
    ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ï¼ˆæ¨™æº–ç‰ˆï¼‰
    è¨ˆç”»æ›¸Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã®å®Ÿè£…
    
    Args:
        exclude_turf (bool): èŠã‚³ãƒ¼ã‚¹ã‚’é™¤å¤–ã™ã‚‹ã‹ã©ã†ã‹
        turf_only (bool): èŠã‚³ãƒ¼ã‚¹ã®ã¿ã‚’å‡¦ç†ã™ã‚‹ã‹ã©ã†ã‹
        enable_missing_value_handling (bool): æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
        enable_quality_check (bool): ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
    """
    logger.info("ğŸ‡ â–  ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ â– ")
    
    # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
    monitor = SystemMonitor()
    
    # å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    if exclude_turf and turf_only:
        logger.error("âŒ èŠã‚³ãƒ¼ã‚¹ã‚’é™¤å¤–ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨èŠã‚³ãƒ¼ã‚¹ã®ã¿ã‚’å‡¦ç†ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        return
    
    # é€šå¸¸ã®å‡¦ç†è¨­å®šã®ãƒ­ã‚°å‡ºåŠ›
    logger.info("ğŸ“‹ å‡¦ç†è¨­å®š:")
    logger.info(f"   ğŸŒ± èŠã‚³ãƒ¼ã‚¹é™¤å¤–: {'ã¯ã„' if exclude_turf else 'ã„ã„ãˆ'}")
    logger.info(f"   ğŸŒ± èŠã‚³ãƒ¼ã‚¹ã®ã¿: {'ã¯ã„' if turf_only else 'ã„ã„ãˆ'}")
    logger.info(f"   ğŸ”§ æ¬ æå€¤å‡¦ç†: {'æœ‰åŠ¹' if enable_missing_value_handling else 'ç„¡åŠ¹'}")
    logger.info(f"   ğŸ“ˆ å“è³ªãƒã‚§ãƒƒã‚¯: {'æœ‰åŠ¹' if enable_quality_check else 'ç„¡åŠ¹'}")
    
    # ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
    quality_checker = DataQualityChecker() if enable_quality_check else None
    
    # å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    ensure_export_dirs()
    monitor.log_system_status("åˆæœŸåŒ–å®Œäº†")
    
    try:
        # 1. BACãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-1: BACãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ï¼‰ã®å‡¦ç†")
        logger.info("="*60)
        
        process_all_bac_files(exclude_turf=exclude_turf, turf_only=turf_only)
        monitor.log_system_status("BACå‡¦ç†å®Œäº†")
    
        # 2. SRBãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-2: SRBãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¬ãƒ¼ã‚¹è©³ç´°æƒ…å ±ï¼‰ã®å‡¦ç†")
        logger.info("="*60)
        
        process_all_srb_files(exclude_turf=exclude_turf, turf_only=turf_only)
        monitor.log_system_status("SRBå‡¦ç†å®Œäº†")
    
        # 3. SEDãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨SRBãƒ»BACãƒ‡ãƒ¼ã‚¿ã¨ã®ç´ã¥ã‘
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-3: SEDãƒ‡ãƒ¼ã‚¿ï¼ˆç«¶èµ°æˆç¸¾ï¼‰ã®å‡¦ç†ã¨ç´ã¥ã‘")
        logger.info("="*60)
        
        process_all_sed_files(exclude_turf=exclude_turf, turf_only=turf_only)
    
        # 4. SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®ç´ã¥ã‘
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-4: SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ")
        logger.info("="*60)
        logger.info("ğŸ“‹ ãƒã‚¤ã‚¢ã‚¹æƒ…å ±å®Œå‚™ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿æŒã—ã¾ã™")
        
        merge_result = merge_srb_with_sed(
            separate_output=True, 
            exclude_turf=exclude_turf, 
            turf_only=turf_only
        )
        
        if not merge_result:
            logger.error("âŒ SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®ç´ã¥ã‘ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†:")
        logger.info("   ğŸ“ SEDãƒ‡ãƒ¼ã‚¿: export/SED/")
        logger.info("   ğŸ“ SRBãƒ‡ãƒ¼ã‚¿: export/SRB/")
        logger.info("   ğŸ“ çµ±åˆãƒ‡ãƒ¼ã‚¿: export/dataset/")
        
        monitor.log_system_status("ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†")
        
        # 5. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆå¾Œï¼‰
        if enable_quality_check:
            logger.info("\n" + "="*60)
            logger.info("ğŸ“Š Phase 0-5: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯")
            logger.info("="*60)
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            sample_files = list(Path('export/dataset').glob('*.csv'))
            if sample_files:
                sample_file = sample_files[0]
                logger.info(f"ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§å“è³ªãƒã‚§ãƒƒã‚¯: {sample_file.name}")
                
                try:
                    sample_df = pd.read_csv(sample_file, encoding='utf-8')
                    quality_checker.check_data_quality(sample_df, "çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿")
                except Exception as e:
                    logger.warning(f"âš ï¸ å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # 7. å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
        if enable_quality_check and quality_checker:
            save_quality_report(quality_checker)
        
        # 8. æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰
        if enable_missing_value_handling:
            logger.info("\n" + "="*60)
            logger.info("ğŸ“ Phase 0-7: æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®è‡ªå‹•æ•´ç†")
            logger.info("="*60)
            summarize_processing_log()
        
        # 9. ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æå‰Šé™¤çµ±è¨ˆã®è¡¨ç¤º
        if enable_missing_value_handling:
            logger.info("\n" + "="*60)
            logger.info("ğŸ“Š Phase 0-8: ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æå‰Šé™¤çµ±è¨ˆ")
            logger.info("="*60)
            display_deletion_statistics()
        
        # 10. å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ å®Œäº†")
        logger.info("="*60)
        
        total_time = time.time() - monitor.start_time
        logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†)")
        monitor.log_system_status("å…¨å‡¦ç†å®Œäº†")
        
        logger.info("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿:")
        if Path('export/dataset').exists():
            bias_files = list(Path('export/dataset').glob('*.csv'))
            logger.info(f"   ğŸ”— çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(bias_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        if enable_quality_check and Path('export/quality_reports').exists():
            logger.info("   ğŸ“ˆ å“è³ªãƒ¬ãƒãƒ¼ãƒˆ: export/quality_reports/")
        
        logger.info("\nğŸ“ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æ•´å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        logger.error("ğŸ”§ ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:", exc_info=True)
        return False

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(
        description='ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ï¼ˆè¨ˆç”»æ›¸Phase 0ï¼šãƒ‡ãƒ¼ã‚¿æ•´å‚™å¯¾å¿œç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ ä½¿ç”¨ä¾‹:
  python process_race_data.py                                    # åŸºæœ¬å‡¦ç†
  python process_race_data.py --turf-only                      # èŠã‚³ãƒ¼ã‚¹ã®ã¿ã§å‡¦ç†
  python process_race_data.py --no-missing-handling              # æ¬ æå€¤å‡¦ç†ã‚’ç„¡åŠ¹åŒ–
  python process_race_data.py --no-quality-check                 # å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–

ğŸ”§ ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å½¹å‰²:
  ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€è¤‡æ•°ã®å½¢å¼ã®ç”Ÿãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆBAC, SRB, SEDï¼‰ã‚’èª­ã¿è¾¼ã¿ã€
  ãã‚Œã‚‰ã‚’ä¸€ã¤ã®æ•´å½¢ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµ±åˆã—ã¾ã™ã€‚
  æœ€çµ‚çš„ãªæˆæœç‰©ã¯ `export/dataset/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‡ºåŠ›ã•ã‚Œã€
  ã“ã‚ŒãŒå¾Œç¶šã®åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆä¾‹: analyze_horse_racelevel.pyï¼‰ã®å…¥åŠ›ã¨ãªã‚Šã¾ã™ã€‚

ğŸ”§ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®å“è³ªç®¡ç†:
  âœ… æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†
  âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒ¬ãƒãƒ¼ãƒˆ
  âœ… æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®è‡ªå‹•ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
  âœ… ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
  âœ… æ®µéšçš„å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›
  âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾©æ—§æ©Ÿèƒ½
        """
    )
    
    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    track_group = parser.add_mutually_exclusive_group()
    track_group.add_argument('--exclude-turf', '--èŠã‚³ãƒ¼ã‚¹é™¤å¤–', action='store_true', 
                           help='èŠã‚³ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ã™ã‚‹')
    track_group.add_argument('--turf-only', '--èŠã‚³ãƒ¼ã‚¹ã®ã¿', action='store_true', 
                           help='èŠã‚³ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‡¦ç†ã™ã‚‹')
    
    # æ©Ÿèƒ½ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--no-missing-handling', '--æ¬ æå€¤å‡¦ç†ç„¡åŠ¹', action='store_true',
                       help='æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹')
    
    parser.add_argument('--no-quality-check', '--å“è³ªãƒã‚§ãƒƒã‚¯ç„¡åŠ¹', action='store_true',
                       help='ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹')
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š')
    
    parser.add_argument('--log-file', help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ã¿ï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–
    log_file = args.log_file
    
    if log_file is None:
        # è‡ªå‹•ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚‚å«ã‚€ï¼‰
        log_dir = Path('export/logs')
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = f'export/logs/process_race_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    
    setup_logging(log_level=args.log_level, log_file=log_file)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¬ãƒ¼ã§ã®é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    logger.info("ğŸš€ ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
    logger.info(f"ğŸ–¥ï¸ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {args.log_level}")
    if log_file:
        logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
    
    # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè¡Œ
    success = process_race_data(
        exclude_turf=args.exclude_turf, 
        turf_only=args.turf_only,
        enable_missing_value_handling=not args.no_missing_handling,
        enable_quality_check=not args.no_quality_check
    )
    
    if success:
        logger.info("ğŸ‰ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        exit_code = 0
    else:
        logger.error("âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        exit_code = 1
    
    logger.info(f"ğŸ ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº† (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code})")
    exit(exit_code) 