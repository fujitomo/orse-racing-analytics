"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
è¨ˆç”»æ›¸Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«å¯¾å¿œç‰ˆï¼‰

å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´ï¼š
1. æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ï¼ˆCSVä½œæˆæ™‚ï¼‰
2. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒ¬ãƒãƒ¼ãƒˆ
3. æ®µéšçš„å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾©æ—§æ©Ÿèƒ½
5. å‡¦ç†æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
"""
from horse_racing.data.processors.bac_processor import process_all_bac_files
from horse_racing.data.processors.sed_processor import process_all_sed_files
from horse_racing.data.processors.srb_processor import process_all_srb_files, merge_srb_with_sed
from horse_racing.data.processors.race_level_processor import process_race_level_analysis_data
import os
import argparse
import logging
import time
import pandas as pd
from pathlib import Path
from datetime import datetime
from output_utils import OutputUtils
from typing import Dict, Any, Tuple, List
import numpy as np

# å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°è¨­å®š
def setup_logging(log_level='INFO', log_file=None):
    """å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°è¨­å®š"""
    import logging
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®š
    if log_file:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(log_file, encoding='utf-8')
            ]
        )
    else:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

# ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¬ãƒ¼
logger = logging.getLogger(__name__)

class DataQualityChecker:
    """
    ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
    å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã«å¿…è¦ãªå“è³ªç®¡ç†æ©Ÿèƒ½
    """
    
    def __init__(self):
        self.quality_report = {}
        
    def check_data_quality(self, df: pd.DataFrame, stage_name: str) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        
        Args:
            df: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®DataFrame
            stage_name: å‡¦ç†æ®µéšå
            
        Returns:
            å“è³ªãƒ¬ãƒãƒ¼ãƒˆè¾æ›¸
        """
        logger.info(f"ğŸ“Š {stage_name} - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        start_time = time.time()
        
        report = {
            'stage': stage_name,
            'timestamp': datetime.now().isoformat(),
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
            'missing_values': {},
            'data_types': {},
            'duplicates': 0,
            'outliers': {},
            'warnings': [],
            'recommendations': []
        }
        
        try:
            # 1. æ¬ æå€¤åˆ†æ
            logger.info("   ğŸ” æ¬ æå€¤åˆ†æä¸­...")
            missing_analysis = self._analyze_missing_values(df)
            report['missing_values'] = missing_analysis
            
            # 2. ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            logger.info("   ğŸ·ï¸ ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯ä¸­...")
            report['data_types'] = self._check_data_types(df)
            
            # 3. é‡è¤‡ãƒã‚§ãƒƒã‚¯
            logger.info("   ğŸ”„ é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
            report['duplicates'] = df.duplicated().sum()
            
            # 4. å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
            logger.info("   ğŸ“ˆ å¤–ã‚Œå€¤æ¤œå‡ºä¸­...")
            report['outliers'] = self._detect_outliers(df)
            
            # 5. ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼
            logger.info("   ğŸ“‹ ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼ä¸­...")
            warnings, recommendations = self._validate_business_rules(df)
            report['warnings'] = warnings
            report['recommendations'] = recommendations
            
            execution_time = time.time() - start_time
            report['execution_time_seconds'] = execution_time
            
            logger.info(f"âœ… {stage_name} - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº† ({execution_time:.2f}ç§’)")
            
            # ãƒ¬ãƒãƒ¼ãƒˆè¦ç´„ã‚’ãƒ­ã‚°å‡ºåŠ›
            self._log_quality_summary(report)
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            report['error'] = str(e)
        
        self.quality_report[stage_name] = report
        return report
    
    def _analyze_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ¬ æå€¤ã®è©³ç´°åˆ†æ"""
        missing_counts = df.isnull().sum()
        missing_percentages = (missing_counts / len(df)) * 100
        
        analysis = {
            'total_missing_cells': missing_counts.sum(),
            'columns_with_missing': missing_counts[missing_counts > 0].to_dict(),
            'missing_percentages': missing_percentages[missing_percentages > 0].to_dict(),
            'critical_columns': []  # 50%ä»¥ä¸Šæ¬ æã®åˆ—
        }
        
        # é‡è¦ãªæ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š
        for col, pct in missing_percentages.items():
            if pct >= 50:
                analysis['critical_columns'].append(col)
        
        return analysis
    
    def _check_data_types(self, df: pd.DataFrame) -> Dict[str, str]:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        return {col: str(dtype) for col, dtype in df.dtypes.items()}
    
    def _detect_outliers(self, df: pd.DataFrame) -> Dict[str, int]:
        """IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º"""
        outlier_counts = {}
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if df[col].notna().sum() > 0:  # æ¬ æå€¤ã§ãªã„å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                outlier_counts[col] = len(outliers)
        
        return outlier_counts
    
    def _validate_business_rules(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼"""
        warnings = []
        recommendations = []
        
        # ç€é †ã®ãƒã‚§ãƒƒã‚¯
        if 'ç€é †' in df.columns:
            invalid_positions = df[df['ç€é †'] < 0]
            if len(invalid_positions) > 0:
                warnings.append(f"ä¸æ­£ãªç€é †ãƒ‡ãƒ¼ã‚¿: {len(invalid_positions)}ä»¶")
        
        # ã‚¿ã‚¤ãƒ ã®ãƒã‚§ãƒƒã‚¯
        if 'ã‚¿ã‚¤ãƒ ' in df.columns:
            # ç•°å¸¸ã«é€Ÿã„/é…ã„ã‚¿ã‚¤ãƒ ã®æ¤œå‡º
            if df['ã‚¿ã‚¤ãƒ '].notna().sum() > 0:
                median_time = df['ã‚¿ã‚¤ãƒ '].median()
                if median_time and (median_time < 60 or median_time > 300):
                    warnings.append(f"ç•°å¸¸ãªã‚¿ã‚¤ãƒ ä¸­å¤®å€¤: {median_time}ç§’")
        
        # è·é›¢ã®ãƒã‚§ãƒƒã‚¯
        if 'è·é›¢' in df.columns:
            if df['è·é›¢'].notna().sum() > 0:
                min_distance = df['è·é›¢'].min()
                max_distance = df['è·é›¢'].max()
                if min_distance < 1000 or max_distance > 4000:
                    warnings.append(f"ç•°å¸¸ãªè·é›¢ç¯„å›²: {min_distance}m - {max_distance}m")
        
        # æ¨å¥¨äº‹é …
        if len(warnings) == 0:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿å“è³ªã¯è‰¯å¥½ã§ã™")
        else:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return warnings, recommendations
    
    def _log_quality_summary(self, report: Dict[str, Any]):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚µãƒãƒªãƒ¼ã®ãƒ­ã‚°å‡ºåŠ›"""
        logger.info(f"ğŸ“Š ã€{report['stage']}ã€‘å“è³ªã‚µãƒãƒªãƒ¼:")
        logger.info(f"   ğŸ“ ãƒ‡ãƒ¼ã‚¿è¦æ¨¡: {report['total_rows']:,}è¡Œ x {report['total_columns']}åˆ—")
        logger.info(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {report['memory_usage_mb']:.1f}MB")
        logger.info(f"   â“ æ¬ æã‚»ãƒ«æ•°: {report['missing_values']['total_missing_cells']:,}")
        logger.info(f"   ğŸ”„ é‡è¤‡è¡Œæ•°: {report['duplicates']:,}")
        
        if report['warnings']:
            logger.warning(f"   âš ï¸ è­¦å‘Š: {len(report['warnings'])}ä»¶")
            for warning in report['warnings']:
                logger.warning(f"      â€¢ {warning}")

class MissingValueHandler:
    """
    æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚¯ãƒ©ã‚¹
    è¨ˆç”»æ›¸Phase 0ã®è¦ä»¶ã«åŸºã¥ãå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®æ¬ æå€¤å‡¦ç†
    """
    
    def __init__(self):
        self.processing_log = []
        
    def handle_missing_values(self, df: pd.DataFrame, strategy_config: Dict[str, Any] = None) -> pd.DataFrame:
        """
        æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã®å®Ÿè¡Œ
        
        Args:
            df: å‡¦ç†å¯¾è±¡DataFrame
            strategy_config: å‡¦ç†æˆ¦ç•¥è¨­å®š
            
        Returns:
            æ¬ æå€¤å‡¦ç†æ¸ˆã¿DataFrame
        """
        logger.info("ğŸ”§ æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†é–‹å§‹")
        start_time = time.time()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆ¦ç•¥è¨­å®š
        if strategy_config is None:
            strategy_config = self._get_default_strategy()
        
        df_processed = df.copy()
        original_rows = len(df_processed)
        
        try:
            # 1. é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_critical_columns(df_processed, strategy_config)
            
            # 2. æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_numeric_columns(df_processed, strategy_config)
            
            # 3. ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_categorical_columns(df_processed, strategy_config)
            
            # 4. æ®‹å­˜æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†
            df_processed = self._handle_remaining_missing(df_processed, strategy_config)
            
            execution_time = time.time() - start_time
            final_rows = len(df_processed)
            
            logger.info(f"âœ… æ¬ æå€¤å‡¦ç†å®Œäº† ({execution_time:.2f}ç§’)")
            logger.info(f"   ğŸ“Š å‡¦ç†å‰: {original_rows:,}è¡Œ")
            logger.info(f"   ğŸ“Š å‡¦ç†å¾Œ: {final_rows:,}è¡Œ")
            logger.info(f"   ğŸ“‰ é™¤å»è¡Œæ•°: {original_rows - final_rows:,}è¡Œ ({((original_rows - final_rows) / original_rows) * 100:.1f}%)")
            
            # å‡¦ç†ãƒ­ã‚°ã®ä¿å­˜
            self._save_processing_log(df_processed)
            
        except Exception as e:
            logger.error(f"âŒ æ¬ æå€¤å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
        
        return df_processed
    
    def _get_default_strategy(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¬ æå€¤å‡¦ç†æˆ¦ç•¥"""
        return {
            'critical_columns': {
                'ç€é †': 'drop',  # ç€é †ãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                'ã‚¿ã‚¤ãƒ ': 'drop',  # ã‚¿ã‚¤ãƒ ãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                'è·é›¢': 'drop',   # è·é›¢ãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                'é¦¬å': 'drop',   # é¦¬åãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                'IDM': 'drop'     # IDMãŒæ¬ æã®è¡Œã¯å‰Šé™¤
            },
            'numeric_columns': {
                'method': 'median',  # ä¸­å¤®å€¤ã§è£œå®Œ
                'max_missing_rate': 0.5  # 50%ä»¥ä¸Šæ¬ æã®åˆ—ã¯å‰Šé™¤
            },
            'categorical_columns': {
                'method': 'mode',    # æœ€é »å€¤ã§è£œå®Œ
                'unknown_label': 'ä¸æ˜',
                'max_missing_rate': 0.8  # 80%ä»¥ä¸Šæ¬ æã®åˆ—ã¯å‰Šé™¤
            },
            'remaining_strategy': 'drop'  # æ®‹å­˜æ¬ æå€¤ã¯è¡Œå‰Šé™¤
        }
    
    def _handle_critical_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†"""
        logger.info("   ğŸ¯ é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        critical_config = config.get('critical_columns', {})
        
        for column, strategy in critical_config.items():
            if column in df.columns:
                missing_count = df[column].isnull().sum()
                if missing_count > 0:
                    logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã®æ¬ æå€¤ã‚’{strategy}å‡¦ç†")
                    
                    if strategy == 'drop':
                        df = df.dropna(subset=[column])
                        self.processing_log.append(f"{column}: {missing_count}è¡Œã‚’å‰Šé™¤ï¼ˆé‡è¦åˆ—ï¼‰")
        
        return df
    
    def _handle_numeric_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†ï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰å°‚ç”¨å‡¦ç†å«ã‚€ï¼‰"""
        logger.info("   ğŸ”¢ æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        numeric_config = config.get('numeric_columns', {})
        method = numeric_config.get('method', 'median')
        max_missing_rate = numeric_config.get('max_missing_rate', 0.5)
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for column in numeric_columns:
            missing_count = df[column].isnull().sum()
            missing_rate = missing_count / len(df)
            
            if missing_count > 0:
                # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰
                if column in ['ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade', 'ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰']:
                    logger.info(f"      â€¢ {column}: å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†ã‚’å®Ÿè¡Œ")
                    df = self._estimate_grade_from_features(df, column)
                    
                    # æ¨å®šå¾Œã®æ¬ ææ•°ã‚’ãƒã‚§ãƒƒã‚¯
                    remaining_missing = df[column].isnull().sum()
                    estimated_count = missing_count - remaining_missing
                    
                    if estimated_count > 0:
                        logger.info(f"      â€¢ {column}: {estimated_count:,}ä»¶ã‚’è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰æ¨å®šè£œå®Œ")
                        self.processing_log.append(f"{column}: è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰{estimated_count}ä»¶æ¨å®šâ†’ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ")
                    
                    # æ®‹ã‚Šã®æ¬ æå€¤ã¯ä¸­å¤®å€¤ã§è£œå®Œï¼ˆæ•°å€¤ã§å‡¦ç†å¾Œã€ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ï¼‰
                    if remaining_missing > 0:
                        fill_value = df[column].median()
                        # æ•°å€¤ã§è£œå®Œã—ã¦ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ å‡¦ç†ã«å§”ã­ã‚‹
                        df[column] = df[column].fillna(fill_value)
                        logger.info(f"      â€¢ {column}: æ®‹ã‚Š{remaining_missing:,}ä»¶ã‚’median({fill_value})ã§è£œå®Œå¾Œã€ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ")
                        self.processing_log.append(f"{column}: medianè£œå®Œ{remaining_missing}ä»¶â†’ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ")
                
                elif missing_rate > max_missing_rate:
                    logger.warning(f"      â€¢ {column}: æ¬ æç‡{missing_rate:.1%} > {max_missing_rate:.1%} â†’ åˆ—å‰Šé™¤")
                    df = df.drop(columns=[column])
                    self.processing_log.append(f"{column}: é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤")
                else:
                    if method == 'median':
                        fill_value = df[column].median()
                    elif method == 'mean':
                        fill_value = df[column].mean()
                    else:
                        fill_value = 0
                    
                    df[column] = df[column].fillna(fill_value)
                    logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’{method}({fill_value})ã§è£œå®Œ")
                    self.processing_log.append(f"{column}: {method}ã§{missing_count}ä»¶è£œå®Œ")
        
        return df
    
    def _handle_categorical_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†"""
        logger.info("   ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        categorical_config = config.get('categorical_columns', {})
        method = categorical_config.get('method', 'mode')
        unknown_label = categorical_config.get('unknown_label', 'ä¸æ˜')
        max_missing_rate = categorical_config.get('max_missing_rate', 0.8)
        
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns
        
        for column in categorical_columns:
            missing_count = df[column].isnull().sum()
            missing_rate = missing_count / len(df)
            
            if missing_count > 0:
                if missing_rate > max_missing_rate:
                    logger.warning(f"      â€¢ {column}: æ¬ æç‡{missing_rate:.1%} > {max_missing_rate:.1%} â†’ åˆ—å‰Šé™¤")
                    df = df.drop(columns=[column])
                    self.processing_log.append(f"{column}: é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤")
                else:
                    if method == 'mode' and not df[column].mode().empty:
                        fill_value = df[column].mode()[0]
                    else:
                        fill_value = unknown_label
                    
                    df[column] = df[column].fillna(fill_value)
                    logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’'{fill_value}'ã§è£œå®Œ")
                    self.processing_log.append(f"{column}: {fill_value}ã§{missing_count}ä»¶è£œå®Œ")
        
        return df
    
    def _handle_remaining_missing(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """æ®‹å­˜æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†"""
        remaining_missing = df.isnull().sum().sum()
        
        if remaining_missing > 0:
            logger.info(f"   ğŸ”§ æ®‹å­˜æ¬ æå€¤å‡¦ç†ä¸­: {remaining_missing:,}ä»¶")
            
            strategy = config.get('remaining_strategy', 'drop')
            
            if strategy == 'drop':
                initial_rows = len(df)
                df = df.dropna()
                dropped_rows = initial_rows - len(df)
                
                if dropped_rows > 0:
                    logger.info(f"      â€¢ æ®‹å­˜æ¬ æå€¤ã®ã‚ã‚‹{dropped_rows:,}è¡Œã‚’å‰Šé™¤")
                    self.processing_log.append(f"æ®‹å­˜æ¬ æå€¤: {dropped_rows}è¡Œå‰Šé™¤")
        
        return df
    
    def _estimate_grade_from_features(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """
        å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†
        è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åãƒ»å‡ºèµ°é ­æ•°ç­‰ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å®š
        
        Args:
            df: å‡¦ç†å¯¾è±¡DataFrame
            grade_column: ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—å
            
        Returns:
            ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šæ¸ˆã¿DataFrame
        """
        grade_missing_mask = df[grade_column].isnull()
        
        if not grade_missing_mask.any():
            # æ—¢å­˜ã®æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ã‚’ä½œæˆ
            df = self._add_grade_name_column(df, grade_column)
            return df
        
        # æ¨å®šå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        estimation_df = df[grade_missing_mask].copy()
        
        # 1. è³é‡‘ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š
        if 'æœ¬è³é‡‘' in df.columns:
            estimation_df = self._estimate_grade_from_prize(estimation_df, grade_column)
        
        # 2. ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š
        if 'ãƒ¬ãƒ¼ã‚¹å' in df.columns:
            estimation_df = self._estimate_grade_from_race_name(estimation_df, grade_column)
        
        # 3. å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹è£œæ­£
        if 'é ­æ•°' in df.columns:
            estimation_df = self._adjust_grade_by_field_size(estimation_df, grade_column)
        
        # 4. è·é›¢ã«ã‚ˆã‚‹è£œæ­£
        if 'è·é›¢' in df.columns:
            estimation_df = self._adjust_grade_by_distance(estimation_df, grade_column)
        
        # æ¨å®šçµæœã‚’å…ƒã®DataFrameã«åæ˜ 
        df.loc[grade_missing_mask, grade_column] = estimation_df[grade_column]
        
        # 5. æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ä¿æŒã—ã¤ã¤ã€Œã‚°ãƒ¬ãƒ¼ãƒ‰åã€åˆ—ã‚’ä½œæˆ
        df = self._add_grade_name_column(df, grade_column)
        
        return df
    
    def _estimate_grade_from_prize(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """è³é‡‘ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«åŸºæº–ï¼‰"""
        if 'æœ¬è³é‡‘' not in df.columns:
            return df
        
        # è³é‡‘ã‚’æ•°å€¤å‹ã«å¤‰æ›
        df['æœ¬è³é‡‘'] = pd.to_numeric(df['æœ¬è³é‡‘'], errors='coerce')
        
        # å®Ÿå‹™ãƒ¬ãƒ™ãƒ«è³é‡‘åŸºæº–ï¼ˆå˜ä½ï¼šä¸‡å††ï¼‰
        # å®Ÿéš›ã®ç«¶é¦¬ç•Œã®è³é‡‘ä½“ç³»ã«åŸºã¥ã
        prize_grade_mapping = [
            (15000, 1),    # G1: 1å„„5åƒä¸‡å††ä»¥ä¸Š
            (6000, 2),     # G2: 6åƒä¸‡å††ä»¥ä¸Š
            (4000, 3),     # G3: 4åƒä¸‡å††ä»¥ä¸Š
            (1500, 4),     # é‡è³: 1åƒ5ç™¾ä¸‡å††ä»¥ä¸Š
            (500, 5),      # ç‰¹åˆ¥: 500ä¸‡å††ä»¥ä¸Š
            (0, 6)         # ãã®ä»–: 500ä¸‡å††æœªæº€
        ]
        
        for min_prize, grade in prize_grade_mapping:
            mask = (df['æœ¬è³é‡‘'] >= min_prize) & df[grade_column].isnull()
            df.loc[mask, grade_column] = grade
        
        return df
    
    def _estimate_grade_from_race_name(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰"""
        if 'ãƒ¬ãƒ¼ã‚¹å' not in df.columns:
            return df
        
        # ãƒ¬ãƒ¼ã‚¹åã®ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰
        race_patterns = {
            1: [  # G1ãƒ‘ã‚¿ãƒ¼ãƒ³
                'ãƒ€ãƒ¼ãƒ“ãƒ¼', 'ã‚ªãƒ¼ã‚¯ã‚¹', 'èŠèŠ±è³', 'çšæœˆè³', 'æ¡œèŠ±è³', 'ãƒã‚¤ãƒ«', 
                'æœ‰é¦¬è¨˜å¿µ', 'å®å¡šè¨˜å¿µ', 'å¤©çš‡è³', 'ã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—', 'ã‚¹ãƒ—ãƒªãƒ³ã‚¿ãƒ¼ã‚º',
                'ã‚¨ãƒªã‚¶ãƒ™ã‚¹å¥³ç‹æ¯', 'ãƒ•ã‚§ãƒ–ãƒ©ãƒªãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚ºã‚«ãƒƒãƒ—',
                'é«˜æ¾å®®è¨˜å¿µ', 'å®‰ç”°è¨˜å¿µ', 'ãƒ´ã‚£ã‚¯ãƒˆãƒªã‚¢', 'ç§‹è¯è³'
            ],
            2: [  # G2ãƒ‘ã‚¿ãƒ¼ãƒ³  
                'äº¬éƒ½è¨˜å¿µ', 'é˜ªç¥å¤§è³å…¸', 'ç›®é»’è¨˜å¿µ', 'æ¯æ—¥ç‹å† ', 'äº¬éƒ½å¤§è³å…¸',
                'ã‚¢ãƒ«ã‚¼ãƒ³ãƒãƒ³å…±å’Œå›½æ¯', 'ä¸­å±±è¨˜å¿µ', 'é‡‘é¯±è³', 'äº¬ç‹æ¯', 'åºœä¸­ç‰é¦¬',
                'ã‚»ãƒ³ãƒˆã‚¦ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ã‚¹ãƒ¯ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'å°å€‰è¨˜å¿µ'
            ],
            3: [  # G3ãƒ‘ã‚¿ãƒ¼ãƒ³
                'å‡½é¤¨è¨˜å¿µ', 'ä¸­äº¬è¨˜å¿µ', 'æ–°æ½Ÿè¨˜å¿µ', 'ä¸ƒå¤•è³', 'ç¦å³¶è¨˜å¿µ', 
                'ãã•ã‚‰ãè³', 'å¼¥ç”Ÿè³', 'ã‚¹ãƒ—ãƒªãƒ³ã‚°', 'ã‚»ãƒ³ãƒˆãƒ©ã‚¤ãƒˆ', 'ã‚¢ãƒ«ãƒ†ãƒŸã‚¹',
                'æœæ—¥æ¯', 'ãƒ›ãƒ¼ãƒ—ãƒ•ãƒ«', 'ãƒ©ã‚¸ã‚ª', 'ã‚¯ã‚¤ãƒ¼ãƒ³', 'ã‚ªãƒ¼ãƒ—ãƒ³'
            ],
            4: [  # é‡è³ï¼ˆãƒªã‚¹ãƒ†ãƒƒãƒ‰ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³
                'é‡è³', 'ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ã‚«ãƒƒãƒ—', 'è³', 'è¨˜å¿µ', 'ç‰¹åˆ¥',
                'ã‚ªãƒ¼ãƒ—ãƒ³', 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰', 'L'
            ]
        }
        
        for grade, patterns in race_patterns.items():
            for pattern in patterns:
                mask = (df['ãƒ¬ãƒ¼ã‚¹å'].str.contains(pattern, case=False, na=False)) & df[grade_column].isnull()
                df.loc[mask, grade_column] = grade
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šæ¡ä»¶æˆ¦ãƒ»æœªå‹åˆ©æˆ¦
        default_mask = df[grade_column].isnull()
        df.loc[default_mask, grade_column] = 5  # ç‰¹åˆ¥æˆ¦ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        
        return df
    
    def _adjust_grade_by_field_size(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹ã‚°ãƒ¬ãƒ¼ãƒ‰è£œæ­£ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«èª¿æ•´ï¼‰"""
        if 'é ­æ•°' not in df.columns:
            return df
        
        df['é ­æ•°'] = pd.to_numeric(df['é ­æ•°'], errors='coerce')
        
        # å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯
        # å¤§ããªãƒ¬ãƒ¼ã‚¹ã»ã©å‡ºèµ°é ­æ•°ãŒå¤šã„å‚¾å‘
        for idx, row in df.iterrows():
            if pd.notnull(row[grade_column]) and pd.notnull(row['é ­æ•°']):
                current_grade = row[grade_column]
                field_size = row['é ­æ•°']
                
                # å‡ºèµ°é ­æ•°ãŒç•°å¸¸ã«å°‘ãªã„å ´åˆã¯ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ä¸‹ã’ã‚‹
                if field_size < 8 and current_grade <= 3:  # G3ä»¥ä¸Šã§8é ­æœªæº€ã¯æ€ªã—ã„
                    df.loc[idx, grade_column] = min(current_grade + 1, 6)
                # å‡ºèµ°é ­æ•°ãŒå¤šã„å ´åˆã¯ã‚°ãƒ¬ãƒ¼ãƒ‰ç¶­æŒã¾ãŸã¯å‘ä¸Š
                elif field_size >= 16 and current_grade >= 5:  # 16é ­ä»¥ä¸Šã§æ¡ä»¶æˆ¦ã¯é‡è³ã®å¯èƒ½æ€§
                    df.loc[idx, grade_column] = max(current_grade - 1, 4)
        
        return df
    
    def _adjust_grade_by_distance(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """è·é›¢ã«ã‚ˆã‚‹ã‚°ãƒ¬ãƒ¼ãƒ‰è£œæ­£ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«èª¿æ•´ï¼‰"""
        if 'è·é›¢' not in df.columns:
            return df
        
        df['è·é›¢'] = pd.to_numeric(df['è·é›¢'], errors='coerce')
        
        # è·é›¢ã«ã‚ˆã‚‹è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯
        # ç‰¹æ®Šè·é›¢ï¼ˆ3000mä»¥ä¸Šï¼‰ã¯é‡è³ã®å¯èƒ½æ€§ãŒé«˜ã„
        for idx, row in df.iterrows():
            if pd.notnull(row[grade_column]) and pd.notnull(row['è·é›¢']):
                current_grade = row[grade_column]
                distance = row['è·é›¢']
                
                # é•·è·é›¢ãƒ¬ãƒ¼ã‚¹ï¼ˆ3000mä»¥ä¸Šï¼‰ã®å ´åˆ
                if distance >= 3000 and current_grade >= 5:
                    df.loc[idx, grade_column] = min(current_grade - 1, 4)  # é‡è³ä»¥ä¸Šã«æ ¼ä¸Šã’
                
                # æ¥µç«¯ãªçŸ­è·é›¢ï¼ˆ1000mæœªæº€ï¼‰ã‚„é•·è·é›¢ï¼ˆ3600mè¶…ï¼‰ã¯ç‰¹åˆ¥ãƒ¬ãƒ¼ã‚¹
                if (distance < 1000 or distance > 3600) and current_grade >= 4:
                    df.loc[idx, grade_column] = min(current_grade - 1, 3)  # G3ä»¥ä¸Šã«æ ¼ä¸Šã’
        
        return df
    
    def _add_grade_name_column(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """
        æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‹ã‚‰ã€Œã‚°ãƒ¬ãƒ¼ãƒ‰åã€åˆ—ã‚’ä½œæˆ
        
        Args:
            df: å‡¦ç†å¯¾è±¡DataFrame
            grade_column: ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—å
            
        Returns:
            ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
        """
        # ã‚°ãƒ¬ãƒ¼ãƒ‰å¤‰æ›ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«æ­£å¼è¡¨è¨˜ï¼‰
        grade_mapping = {
            1: 'ï¼§ï¼‘',
            2: 'ï¼§ï¼’', 
            3: 'ï¼§ï¼“',
            4: 'é‡è³',
            5: 'ç‰¹åˆ¥',
            6: 'ï¼¬'
        }
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã‚’æ•°å€¤å‹ã¨ã—ã¦ä¿æŒï¼ˆå…ƒã®åˆ—ã¯ãã®ã¾ã¾ï¼‰
        df[grade_column] = pd.to_numeric(df[grade_column], errors='coerce')
        
        # NaNå€¤ãŒã‚ã‚‹å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆ5: ç‰¹åˆ¥ï¼‰ã‚’è¨­å®š
        df[grade_column] = df[grade_column].fillna(5)
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        grade_names = df[grade_column].map(grade_mapping).fillna('ç‰¹åˆ¥')
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if 'ã‚°ãƒ¬ãƒ¼ãƒ‰å' in df.columns:
            # æ—¢å­˜ã®åˆ—ã‚’æ›´æ–°
            df['ã‚°ãƒ¬ãƒ¼ãƒ‰å'] = grade_names
        else:
            # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ç›´å¾Œã«ã€Œã‚°ãƒ¬ãƒ¼ãƒ‰åã€åˆ—ã‚’æŒ¿å…¥
            grade_col_index = df.columns.get_loc(grade_column)
            df.insert(grade_col_index + 1, 'ã‚°ãƒ¬ãƒ¼ãƒ‰å', grade_names)
        
        return df
    
    def _save_processing_log(self, df: pd.DataFrame):
        """å‡¦ç†ãƒ­ã‚°ã®ä¿å­˜"""
        log_path = Path('export/missing_value_processing_log.txt')
        
        try:
            with open(log_path, 'w', encoding='utf-8') as f:
                f.write(f"æ¬ æå€¤å‡¦ç†ãƒ­ã‚° - {datetime.now()}\n")
                f.write("=" * 50 + "\n\n")
                
                for log_entry in self.processing_log:
                    f.write(f"â€¢ {log_entry}\n")
                
                f.write(f"\næœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}\n")
                f.write(f"æ®‹å­˜æ¬ æå€¤: {df.isnull().sum().sum()}ä»¶\n")
            
            logger.info(f"   ğŸ“ å‡¦ç†ãƒ­ã‚°ä¿å­˜: {log_path}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ å‡¦ç†ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¯ãƒ©ã‚¹ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
    
    def __init__(self):
        self.start_time = time.time()
    
    def log_system_status(self, stage_name: str):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        
        logger.info(f"ğŸ’» [{stage_name}] ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:")
        logger.info(f"   â±ï¸ çµŒéæ™‚é–“: {elapsed_time:.1f}ç§’")

def ensure_export_dirs():
    """
    å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªã¨ä½œæˆ
    å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ç®¡ç†æ©Ÿèƒ½ä»˜ã
    """
    dirs = [
        'export/BAC', 
        'export/SRB', 
        'export/SED', 
        'export/with_bias',          # å®Ÿéš›ã®SED+SRBçµ±åˆãƒ‡ãƒ¼ã‚¿å‡ºåŠ›å…ˆ
        'export/race_level_analysis',  # è¨ˆç”»æ›¸ç¬¬0æ®µéšç”¨
        'export/quality_reports',     # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ç”¨
        'export/logs'                 # ãƒ­ã‚°ä¿å­˜ç”¨
    ]
    
    created_dirs = []
    
    for dir_path in dirs:
        path_obj = Path(dir_path)
        if not path_obj.exists():
            path_obj.mkdir(parents=True, exist_ok=True)
            created_dirs.append(dir_path)
            logger.info(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {dir_path}")
    
    if created_dirs:
        logger.info(f"âœ… {len(created_dirs)}å€‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ")
    else:
        logger.info("ğŸ“ ã™ã¹ã¦ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™")

def save_quality_report(quality_checker: DataQualityChecker):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
    report_path = Path('export/quality_reports/data_quality_report.json')
    
    try:
        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(quality_checker.quality_report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        
    except Exception as e:
        logger.warning(f"âš ï¸ å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

def process_race_data(exclude_turf=False, turf_only=False, enable_race_level_analysis=False, 
                     enable_missing_value_handling=True, enable_quality_check=True,
                     race_level_analysis_only=False):
    """
    ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ï¼ˆæ¨™æº–ç‰ˆï¼‰
    è¨ˆç”»æ›¸Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã®å®Ÿè£…
    
    Args:
        exclude_turf (bool): èŠã‚³ãƒ¼ã‚¹ã‚’é™¤å¤–ã™ã‚‹ã‹ã©ã†ã‹
        turf_only (bool): èŠã‚³ãƒ¼ã‚¹ã®ã¿ã‚’å‡¦ç†ã™ã‚‹ã‹ã©ã†ã‹
        enable_race_level_analysis (bool): ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
        enable_missing_value_handling (bool): æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
        enable_quality_check (bool): ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
        race_level_analysis_only (bool): ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
    """
    logger.info("ğŸ‡ â–  ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ â– ")
    
    # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
    monitor = SystemMonitor()
    
    # å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    if exclude_turf and turf_only:
        logger.error("âŒ èŠã‚³ãƒ¼ã‚¹ã‚’é™¤å¤–ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨èŠã‚³ãƒ¼ã‚¹ã®ã¿ã‚’å‡¦ç†ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        return
    
    # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã®å°‚ç”¨å‡¦ç†
    if race_level_analysis_only:
        logger.info("ğŸ”¬ ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
        with_bias_dir = Path('export/with_bias')
        if not with_bias_dir.exists() or not list(with_bias_dir.glob('*.csv')):
            logger.error("âŒ çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«åŸºæœ¬å‡¦ç†ï¼ˆ--race-level-analysisï¼‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            logger.error("   ğŸ“ å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: export/with_bias/")
            return False
        
        # å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªï¼ˆãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ã®ã¿ï¼‰
        race_analysis_dir = Path('export/race_level_analysis')
        race_analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        quality_checker = DataQualityChecker() if enable_quality_check else None
        missing_handler = MissingValueHandler() if enable_missing_value_handling else None
        
        logger.info("ğŸ”¬ Phase 0-6: ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        logger.info("ğŸ“‹ è¨ˆç”»æ›¸2.2.2ã€Œåˆ†æã§ä½¿ã†ä¸»è¦ãªã‚‚ã®ã•ã—ã€ã«åŸºã¥ãåŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ")
        
        race_level_result = process_race_level_analysis_data(
            input_dir='export/with_bias',
            output_dir='export/race_level_analysis',
            exclude_turf=exclude_turf,
            turf_only=turf_only,
            enable_missing_value_handling=enable_missing_value_handling,
            quality_checker=quality_checker,
            missing_handler=missing_handler
        )
        
        if race_level_result:
            logger.info("âœ… ã€Phase 0-6ã®ã¿ã€‘ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†")
            logger.info("   ğŸ“ åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿: export/race_level_analysis/")
            logger.info("   ğŸ“Š ç‰¹å¾´é‡ã‚µãƒãƒªãƒ¼: export/race_level_analysis/feature_summary.json")
            logger.info("   ğŸš€ Phase 1åˆ†æã®æº–å‚™å®Œäº†")
        else:
            logger.error("âŒ ã€Phase 0-6ã®ã¿ã€‘ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        logger.info("ğŸ‰ ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æå°‚ç”¨å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return True
    
    # é€šå¸¸ã®å‡¦ç†è¨­å®šã®ãƒ­ã‚°å‡ºåŠ›
    logger.info("ğŸ“‹ å‡¦ç†è¨­å®š:")
    logger.info(f"   ğŸŒ± èŠã‚³ãƒ¼ã‚¹é™¤å¤–: {'ã¯ã„' if exclude_turf else 'ã„ã„ãˆ'}")
    logger.info(f"   ğŸŒ± èŠã‚³ãƒ¼ã‚¹ã®ã¿: {'ã¯ã„' if turf_only else 'ã„ã„ãˆ'}")
    logger.info(f"   ğŸ“Š ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ: {'æœ‰åŠ¹' if enable_race_level_analysis else 'ç„¡åŠ¹'}")
    logger.info(f"   ğŸ”§ æ¬ æå€¤å‡¦ç†: {'æœ‰åŠ¹' if enable_missing_value_handling else 'ç„¡åŠ¹'}")
    logger.info(f"   ğŸ“ˆ å“è³ªãƒã‚§ãƒƒã‚¯: {'æœ‰åŠ¹' if enable_quality_check else 'ç„¡åŠ¹'}")
    
    # ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
    quality_checker = DataQualityChecker() if enable_quality_check else None
    missing_handler = MissingValueHandler() if enable_missing_value_handling else None
    
    # å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    ensure_export_dirs()
    monitor.log_system_status("åˆæœŸåŒ–å®Œäº†")
    
    try:
        # 1. BACãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-1: BACãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ï¼‰ã®å‡¦ç†")
        logger.info("="*60)
        
        process_all_bac_files(exclude_turf=exclude_turf, turf_only=turf_only)
        monitor.log_system_status("BACå‡¦ç†å®Œäº†")
    
        # 2. SRBãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-2: SRBãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¬ãƒ¼ã‚¹è©³ç´°æƒ…å ±ï¼‰ã®å‡¦ç†")
        logger.info("="*60)
        
        process_all_srb_files(exclude_turf=exclude_turf, turf_only=turf_only)
        monitor.log_system_status("SRBå‡¦ç†å®Œäº†")
    
        # 3. SEDãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨SRBãƒ»BACãƒ‡ãƒ¼ã‚¿ã¨ã®ç´ã¥ã‘
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-3: SEDãƒ‡ãƒ¼ã‚¿ï¼ˆç«¶èµ°æˆç¸¾ï¼‰ã®å‡¦ç†ã¨ç´ã¥ã‘")
        logger.info("="*60)
        
        process_all_sed_files(exclude_turf=exclude_turf, turf_only=turf_only)
    
        # 4. SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®ç´ã¥ã‘
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-4: SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ")
        logger.info("="*60)
        logger.info("ğŸ“‹ ãƒã‚¤ã‚¢ã‚¹æƒ…å ±å®Œå‚™ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿æŒã—ã¾ã™")
        
        merge_result = merge_srb_with_sed(
            separate_output=True, 
            exclude_turf=exclude_turf, 
            turf_only=turf_only
        )
        
        if not merge_result:
            logger.error("âŒ SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®ç´ã¥ã‘ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†:")
        logger.info("   ğŸ“ SEDãƒ‡ãƒ¼ã‚¿: export/SED/")
        logger.info("   ğŸ“ SRBãƒ‡ãƒ¼ã‚¿: export/SRB/")
        logger.info("   ğŸ“ çµ±åˆãƒ‡ãƒ¼ã‚¿: export/with_bias/")
        
        monitor.log_system_status("ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†")
        
        # 5. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆå¾Œï¼‰
        if enable_quality_check:
            logger.info("\n" + "="*60)
            logger.info("ğŸ“Š Phase 0-5: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯")
            logger.info("="*60)
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            sample_files = list(Path('export/with_bias').glob('*.csv'))
            if sample_files:
                sample_file = sample_files[0]
                logger.info(f"ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§å“è³ªãƒã‚§ãƒƒã‚¯: {sample_file.name}")
                
                try:
                    sample_df = pd.read_csv(sample_file, encoding='utf-8')
                    quality_checker.check_data_quality(sample_df, "çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿")
                except Exception as e:
                    logger.warning(f"âš ï¸ å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # 6. ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        if enable_race_level_analysis:
            logger.info("\n" + "="*60)
            logger.info("ğŸ”¬ Phase 0-6: ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
            logger.info("="*60)
            logger.info("ğŸ“‹ è¨ˆç”»æ›¸2.2.2ã€Œåˆ†æã§ä½¿ã†ä¸»è¦ãªã‚‚ã®ã•ã—ã€ã«åŸºã¥ãåŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ")
            
            race_level_result = process_race_level_analysis_data(
                input_dir='export/with_bias',
                output_dir='export/race_level_analysis',
                exclude_turf=exclude_turf,
                turf_only=turf_only,
                enable_missing_value_handling=enable_missing_value_handling,
                quality_checker=quality_checker,
                missing_handler=missing_handler
            )
            
            if race_level_result:
                logger.info("âœ… ã€Phase 0ã€‘ ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†")
                logger.info("   ğŸ“ åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿: export/race_level_analysis/")
                logger.info("   ğŸ“Š ç‰¹å¾´é‡ã‚µãƒãƒªãƒ¼: export/race_level_analysis/feature_summary.json")
                logger.info("   ğŸš€ Phase 1åˆ†æã®æº–å‚™å®Œäº†")
                
                logger.info("\nğŸ¯ ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘:")
                logger.info("   ğŸ“ˆ åŸºç¤åˆ†æ: python analyze_race_level.py export/race_level_analysis")
                logger.info("   ğŸ•’ æ™‚ç³»åˆ—åˆ†æ: python analyze_race_level.py export/race_level_analysis --three-year-periods")
                logger.info("   ğŸƒ ã‚¿ã‚¤ãƒ åˆ†æ: python analyze_race_level.py export/race_level_analysis --enable-time-analysis")
                
                monitor.log_system_status("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†")
                
            else:
                logger.error("âŒ ã€Phase 0ã€‘ ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
        
        # 7. å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
        if enable_quality_check and quality_checker:
            save_quality_report(quality_checker)
        
        # 8. å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ å®Œäº†")
        logger.info("="*60)
        
        total_time = time.time() - monitor.start_time
        logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†)")
        monitor.log_system_status("å…¨å‡¦ç†å®Œäº†")
        
        logger.info("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿:")
        if Path('export/with_bias').exists():
            bias_files = list(Path('export/with_bias').glob('*.csv'))
            logger.info(f"   ğŸ”— çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(bias_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        if enable_race_level_analysis and Path('export/race_level_analysis').exists():
            analysis_files = list(Path('export/race_level_analysis').glob('*.csv'))
            logger.info(f"   ğŸ“Š åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿: {len(analysis_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        if enable_quality_check and Path('export/quality_reports').exists():
            logger.info("   ğŸ“ˆ å“è³ªãƒ¬ãƒãƒ¼ãƒˆ: export/quality_reports/")
        
        logger.info("\nğŸ“ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æ•´å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        logger.error("ğŸ”§ ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:", exc_info=True)
        return False

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(
        description='ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ï¼ˆè¨ˆç”»æ›¸Phase 0ï¼šãƒ‡ãƒ¼ã‚¿æ•´å‚™å®Œå…¨å¯¾å¿œç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ ä½¿ç”¨ä¾‹ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«æ¨™æº–ç‰ˆï¼‰:
  python process_race_data.py                                    # åŸºæœ¬å‡¦ç†ã®ã¿
  python process_race_data.py --race-level-analysis              # Phase 0å®Œå…¨ç‰ˆï¼ˆè‹±èªï¼‰
  python process_race_data.py --ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ                    # Phase 0å®Œå…¨ç‰ˆï¼ˆæ—¥æœ¬èªï¼‰
  python process_race_data.py --race-level-analysis-only         # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿å®Ÿè¡Œï¼ˆè‹±èªï¼‰
  python process_race_data.py --ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿                 # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿å®Ÿè¡Œï¼ˆæ—¥æœ¬èªï¼‰
  python process_race_data.py --turf-only --race-level-analysis  # èŠã‚³ãƒ¼ã‚¹ã®ã¿ã§å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†
  python process_race_data.py --no-missing-handling              # æ¬ æå€¤å‡¦ç†ã‚’ç„¡åŠ¹åŒ–
  python process_race_data.py --no-quality-check                 # å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–
  
ğŸ”¬ ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰:
  # æ—¢å­˜ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ï¼ˆexport/with_bias/ï¼‰ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿ã‚’å®Ÿè¡Œ
  python process_race_data.py --race-level-analysis-only         # é«˜é€Ÿå®Ÿè¡Œ
  python process_race_data.py --ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿ --èŠã‚³ãƒ¼ã‚¹ã®ã¿     # èŠã‚³ãƒ¼ã‚¹ã®ã¿ã§åˆ†æ
  
ğŸ“Š Phase 0ã§ä½œæˆã•ã‚Œã‚‹ç‰¹å¾´é‡ï¼ˆè¨ˆç”»æ›¸æº–æ‹ ï¼‰:
  âœ… ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«: G1ã‹ã‚‰æœªå‹åˆ©ã¾ã§ã®æ®µéšåˆ†ã‘ + è·é›¢è£œæ­£
  âœ… é¦¬èƒ½åŠ›: IDMãƒ»ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ç­‰ã®çµ±åˆæŒ‡æ¨™ï¼ˆãƒã‚¤ã‚¢ã‚¹è£œæ­£ç‰ˆå«ã‚€ï¼‰
  âœ… ãƒˆãƒ©ãƒƒã‚¯ãƒã‚¤ã‚¢ã‚¹: è„šè³ªãƒ»æ é †ãƒ»é¦¬å ´çŠ¶æ…‹ã®ç·åˆæ•°å€¤åŒ–
  âœ… èµ°ç ´ã‚¿ã‚¤ãƒ : è·é›¢è£œæ­£ã‚¿ã‚¤ãƒ ã€Z-scoreæ­£è¦åŒ–ã€é€Ÿåº¦æŒ‡æ¨™
  âœ… è¤‡å‹ç‡ãƒ•ãƒ©ã‚°: is_win, is_placed
  âœ… ãã®ä»–è¦å› : é¨æ‰‹ãƒ»æ–¤é‡ãƒ»è¡€çµ±ç­‰ã®ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–
  
ğŸ”§ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®å“è³ªç®¡ç†:
  âœ… æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ï¼ˆCSVä½œæˆæ™‚ï¼‰
  âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒ¬ãƒãƒ¼ãƒˆ
  âœ… ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
  âœ… æ®µéšçš„å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›
  âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾©æ—§æ©Ÿèƒ½
        """
    )
    
    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    track_group = parser.add_mutually_exclusive_group()
    track_group.add_argument('--exclude-turf', '--èŠã‚³ãƒ¼ã‚¹é™¤å¤–', action='store_true', 
                           help='èŠã‚³ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ã™ã‚‹')
    track_group.add_argument('--turf-only', '--èŠã‚³ãƒ¼ã‚¹ã®ã¿', action='store_true', 
                           help='èŠã‚³ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‡¦ç†ã™ã‚‹')
    
    # æ©Ÿèƒ½ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--race-level-analysis', '--ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ', action='store_true', 
                       help='ã€Phase 0ã€‘ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆè¨ˆç”»æ›¸è¦ä»¶å®Œå…¨å¯¾å¿œï¼‰')
    
    parser.add_argument('--race-level-analysis-only', '--ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿', action='store_true',
                       help='ã€Phase 0-6ã®ã¿ã€‘æ—¢å­˜ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æç”¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ã¿ã‚’å®Ÿè¡Œ')
    
    parser.add_argument('--no-missing-handling', '--æ¬ æå€¤å‡¦ç†ç„¡åŠ¹', action='store_true',
                       help='æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹')
    
    parser.add_argument('--no-quality-check', '--å“è³ªãƒã‚§ãƒƒã‚¯ç„¡åŠ¹', action='store_true',
                       help='ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹')
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š')
    
    parser.add_argument('--log-file', help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ã¿ï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–
    log_file = args.log_file
    race_level_analysis = args.race_level_analysis or getattr(args, 'ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ', False)
    race_level_analysis_only = args.race_level_analysis_only or getattr(args, 'ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ã¿', False)
    
    if log_file is None and (race_level_analysis or race_level_analysis_only):
        # è‡ªå‹•ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚‚å«ã‚€ï¼‰
        log_dir = Path('export/logs')
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = f'export/logs/process_race_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    
    setup_logging(log_level=args.log_level, log_file=log_file)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¬ãƒ¼ã§ã®é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    logger.info("ğŸš€ ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
    logger.info(f"ğŸ–¥ï¸ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {args.log_level}")
    if log_file:
        logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
    
    # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè¡Œ
    success = process_race_data(
        exclude_turf=args.exclude_turf, 
        turf_only=args.turf_only,
        enable_race_level_analysis=race_level_analysis,
        enable_missing_value_handling=not args.no_missing_handling,
        enable_quality_check=not args.no_quality_check,
        race_level_analysis_only=race_level_analysis_only
    )
    
    if success:
        logger.info("ğŸ‰ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        exit_code = 0
    else:
        logger.error("âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        exit_code = 1
    
    logger.info(f"ğŸ ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº† (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code})")
    exit(exit_code) 