"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
è¨ˆç”»æ›¸Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«å¯¾å¿œç‰ˆï¼‰

å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´ï¼š
1. æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ï¼ˆCSVä½œæˆæ™‚ï¼‰
2. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒ¬ãƒãƒ¼ãƒˆ
3. æ®µéšçš„å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾©æ—§æ©Ÿèƒ½
5. å‡¦ç†æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
"""
from horse_racing.data.processors.bac_processor import process_all_bac_files
from horse_racing.data.processors.sed_processor import process_all_sed_files
from horse_racing.data.processors.srb_processor import process_all_srb_files, merge_srb_with_sed
import argparse
import logging
import time
import pandas as pd
from pathlib import Path
from datetime import datetime

from typing import Dict, Any, Tuple, List, Optional
import numpy as np
import re
from collections import defaultdict
from dataclasses import dataclass

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…±é€šãƒ­ã‚¬ãƒ¼
logger = logging.getLogger(__name__)

# =====================================
# åˆ—åã®å®šç¾©ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›ç”¨ï¼‰
# =====================================

class ColumnNames:
    """ãƒ‡ãƒ¼ã‚¿åˆ—åã®é›†ä¸­å®šç¾©ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚

    æ—¢å­˜ã®æ—¥æœ¬èªåˆ—åã‚’å¯¾è±¡ã«ã€æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ãŒå‚ç…§ã™ã‚‹åˆ—åã‚’æä¾›ã™ã‚‹ã€‚
    """
    # åŸºæœ¬åˆ—
    RACE_NAME = 'ãƒ¬ãƒ¼ã‚¹å'
    DISTANCE = 'è·é›¢'
    HORSE_COUNT = 'é ­æ•°'
    POSITION = 'ç€é †'
    HORSE_NAME = 'é¦¬å'
    HORSE_AGE = 'é¦¬é½¢'
    IDM = 'IDM'
    GRADE = 'ã‚°ãƒ¬ãƒ¼ãƒ‰'
    GRADE_Y = 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y'
    GRADE_NAME = 'ã‚°ãƒ¬ãƒ¼ãƒ‰å'

    # æ—¥ä»˜ãƒ»è­˜åˆ¥
    REGISTRATION_NUMBER = 'è¡€çµ±ç™»éŒ²ç•ªå·'
    RACE_DATE = 'å¹´æœˆæ—¥'

    # è³é‡‘é–¢é€£
    PRIZE_1ST_WITH_BONUS = '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)'
    PRIZE_MAIN = 'æœ¬è³é‡‘'

    def get_grade_columns(self):
        return [self.GRADE, 'grade', 'ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰']

    def get_prize_columns(self):
        return [
            '2ç€è³é‡‘', '3ç€è³é‡‘', '4ç€è³é‡‘', '5ç€è³é‡‘',
            '1ç€ç®—å…¥è³é‡‘', '2ç€ç®—å…¥è³é‡‘',
            self.PRIZE_1ST_WITH_BONUS, '2ç€è³é‡‘(2ç€ç®—å…¥è³é‡‘è¾¼ã¿)', 'å¹³å‡è³é‡‘',
            self.PRIZE_MAIN
        ]

# =====================================
# ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šç”¨ã®è¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼è§£æ¶ˆï¼‰
# =====================================

@dataclass
class GradeThresholds:
    """ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šç”¨ã®è³é‡‘é–¾å€¤è¨­å®šï¼ˆformattedãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã«åŸºã¥ãå®Ÿè¨¼çš„åŸºæº–ï¼‰"""
    G1_MIN: int = 3407    # G1: 3,407ä¸‡å††ä»¥ä¸Šï¼ˆG1ãƒ¬ãƒ¼ã‚¹å¹³å‡ï¼‰
    G2_MIN: int = 2177    # G2: 2,177ä¸‡å††ä»¥ä¸Šï¼ˆG2ãƒ¬ãƒ¼ã‚¹å¹³å‡ï¼‰
    G3_MIN: int = 1438    # G3: 1,438ä¸‡å††ä»¥ä¸Šï¼ˆG3ãƒ¬ãƒ¼ã‚¹å¹³å‡ï¼‰
    LISTED_MIN: int = 903  # Lï¼ˆãƒªã‚¹ãƒ†ãƒƒãƒ‰ï¼‰: 903ä¸‡å††ä»¥ä¸Šï¼ˆLãƒ¬ãƒ¼ã‚¹å¹³å‡ï¼‰
    SPECIAL_MIN: int = 552 # ç‰¹åˆ¥/OP: 552ä¸‡å††ä»¥ä¸Šï¼ˆç‰¹åˆ¥ãƒ¬ãƒ¼ã‚¹å¹³å‡ï¼‰
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰åãƒãƒƒãƒ”ãƒ³ã‚°
    GRADE_NAME_MAPPING: Dict[int, str] = None
    
    def __post_init__(self):
        if self.GRADE_NAME_MAPPING is None:
            object.__setattr__(self, 'GRADE_NAME_MAPPING', {
                1: 'ï¼§ï¼‘',
                2: 'ï¼§ï¼’', 
                3: 'ï¼§ï¼“',
                4: 'é‡è³',
                5: 'ç‰¹åˆ¥',
                6: 'ï¼¬ï¼ˆãƒªã‚¹ãƒ†ãƒƒãƒ‰ï¼‰'
            })
    
    def to_thresholds_list(self) -> List[Tuple[int, int]]:
        """è³é‡‘ã—ãã„å€¤ã‚’é™é †ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚

        Returns:
            List[Tuple[int, int]]: æœ€ä½è³é‡‘ã¨å¯¾å¿œã™ã‚‹ã‚°ãƒ¬ãƒ¼ãƒ‰å€¤ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã€‚
        """
        return [
            (self.G1_MIN, 1),
            (self.G2_MIN, 2),
            (self.G3_MIN, 3),
            (self.LISTED_MIN, 6),
            (self.SPECIAL_MIN, 5)
        ]

@dataclass
class RacePatterns:
    """ãƒ¬ãƒ¼ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    G1_PATTERNS: List[str] = None
    G2_PATTERNS: List[str] = None
    G3_PATTERNS: List[str] = None
    STAKES_PATTERNS: List[str] = None
    CONDITIONS_PATTERNS: List[str] = None
    
    def __post_init__(self):
        if self.G1_PATTERNS is None:
            self.G1_PATTERNS = [
                'ã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—', 'æœ‰é¦¬è¨˜å¿µ', 'å¤§é˜ªæ¯', 'æ±äº¬å„ªé§¿',
                'å¤©çš‡è³', 'å®å¡šè¨˜å¿µ', 'çšæœˆè³', 'èŠèŠ±è³',
                'å®‰ç”°è¨˜å¿µ', 'ãƒã‚¤ãƒ«ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚·ãƒƒãƒ—',
                'é«˜æ¾å®®è¨˜å¿µ', 'ã‚¹ãƒ—ãƒªãƒ³ã‚¿ãƒ¼ã‚ºã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹',
                'å„ªé§¿ç‰é¦¬', 'æ¡œèŠ±è³', 'ãƒ´ã‚£ã‚¯ãƒˆãƒªã‚¢ãƒã‚¤ãƒ«',
                'ã‚¨ãƒªã‚¶ãƒ™ã‚¹å¥³ç‹æ¯', 'ã‚¸ãƒ£ãƒ‘ãƒ³ã‚«ãƒƒãƒ—ãƒ€ãƒ¼ãƒˆ',
                'ï¼®ï¼¨ï¼«ãƒã‚¤ãƒ«ã‚«ãƒƒãƒ—', 'ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚ºã‚«ãƒƒãƒ—',
                'ãƒ•ã‚§ãƒ–ãƒ©ãƒªãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ç§‹è¯è³', 'ï¼ªï¼¢ï¼£ã‚¯ãƒ©ã‚·ãƒƒã‚¯',
                'ä¸­å±±ã‚°ãƒ©ãƒ³ãƒ‰ã‚¸ãƒ£ãƒ³ãƒ—', 'ä¸­å±±å¤§éšœå®³',
                'æœæ—¥æ¯ãƒ•ãƒ¥ãƒ¼ãƒãƒ¥ãƒªãƒ†ã‚£ã‚¹ãƒ†ãƒ¼ã‚¯ã‚¹', 'ï¼ªï¼¢ï¼£ã‚¹ãƒ—ãƒªãƒ³ãƒˆ',
                'ãƒ€ãƒ¼ãƒ“ãƒ¼', 'ã‚ªãƒ¼ã‚¯ã‚¹', 'ãƒã‚¤ãƒ«', 'ãƒ•ãƒ¥ãƒ¼ãƒãƒ¥ãƒªãƒ†ã‚£'
            ]
        
        if self.G2_PATTERNS is None:
            self.G2_PATTERNS = ['æœ­å¹Œè¨˜å¿µ', 'é˜ªç¥ã‚«ãƒƒãƒ—', 'è¨˜å¿µ', 'å¤§è³å…¸']
        
        if self.G3_PATTERNS is None:
            self.G3_PATTERNS = ['è³', 'ç‰¹åˆ¥']
        
        if self.STAKES_PATTERNS is None:
            self.STAKES_PATTERNS = ['é‡è³', 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰', 'L']
        
        if self.CONDITIONS_PATTERNS is None:
            self.CONDITIONS_PATTERNS = ['æ¡ä»¶', 'æ–°é¦¬', 'æœªå‹åˆ©', '1å‹ã‚¯ãƒ©ã‚¹', '2å‹ã‚¯ãƒ©ã‚¹', '3å‹ã‚¯ãƒ©ã‚¹']

# =====================================
# ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå°‚ç”¨ã‚¯ãƒ©ã‚¹ï¼ˆSRPéµå®ˆï¼‰
# =====================================

class GradeEstimator:
    """ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå°‚ç”¨ã‚¯ãƒ©ã‚¹ï¼ˆå˜ä¸€è²¬ä»»åŸå‰‡ã‚’éµå®ˆï¼‰"""
    
    def __init__(self, thresholds: Optional[GradeThresholds] = None, 
                 patterns: Optional[RacePatterns] = None,
                 columns: Optional[ColumnNames] = None):
        """æ¨å®šã«å¿…è¦ãªä¾å­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Args:
            thresholds (GradeThresholds, optional): è³é‡‘ã«åŸºã¥ãé–¾å€¤è¨­å®šã€‚
            patterns (RacePatterns, optional): ãƒ¬ãƒ¼ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³è¨­å®šã€‚
            columns (ColumnNames, optional): åˆ—åè¨­å®šã€‚
        """
        self.thresholds = thresholds or GradeThresholds()
        self.patterns = patterns or RacePatterns()
        self.columns = columns or ColumnNames()
        self.logger = logging.getLogger(__name__)
    
    def estimate_grade(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            grade_column (str): æ¨å®šå¯¾è±¡ã¨ãªã‚‹ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—åã€‚

        Returns:
            pd.DataFrame: ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šçµæœã‚’åæ˜ ã—ãŸ DataFrameï¼ˆã‚³ãƒ”ãƒ¼ï¼‰ã€‚
        """
        # DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆä¸å¤‰æ€§ã‚’ä¿è¨¼ï¼‰
        df_result = df.copy()
        
        initial_rows = len(df_result)
        grade_missing_mask = df_result[grade_column].isnull()
        initial_missing_count = grade_missing_mask.sum()
        
        if not grade_missing_mask.any():
            # æ—¢å­˜ã®æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ã‚’ä½œæˆ
            df_result = self._add_grade_name_column(df_result, grade_column)
            return df_result
        
        self.logger.info(f"ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æå€¤: {initial_missing_count:,}ä»¶ ({initial_missing_count/initial_rows*100:.1f}%)")
        
        # æ¨å®šå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        estimation_df = df_result[grade_missing_mask].copy()
        
        # 1. è³é‡‘ãƒ™ãƒ¼ã‚¹ã®æ¨å®š
        if self.columns.PRIZE_1ST_WITH_BONUS in df_result.columns:
            estimation_df = self._estimate_from_prize(estimation_df, grade_column, self.columns.PRIZE_1ST_WITH_BONUS)
        
        # 2. æœ¬è³é‡‘ã‹ã‚‰ã®æ¨å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if self.columns.PRIZE_MAIN in df_result.columns:
            estimation_df = self._estimate_from_prize(estimation_df, grade_column, self.columns.PRIZE_MAIN)
        
        # 3. ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰ã®æ¨å®š
        if self.columns.RACE_NAME in df_result.columns:
            estimation_df = self._estimate_from_race_name(estimation_df, grade_column)
        
        # 4. ç‰¹å¾´é‡ã‹ã‚‰ã®æ¨å®šï¼ˆè·é›¢ãƒ»å‡ºèµ°é ­æ•°ï¼‰
        estimation_df = self._estimate_from_features(estimation_df, grade_column)
        
        # 5. æœ€çµ‚çš„ã«æ¨å®šã§ããªã„å ´åˆã¯æ¡ä»¶æˆ¦ï¼ˆ5ï¼‰ã¨ã—ã¦è¨­å®š
        final_missing = estimation_df[grade_column].isnull().sum()
        if final_missing > 0:
            self.logger.info(f"      ğŸ¯ æœ€çµ‚æ¨å®šå¤±æ•—{final_missing:,}ä»¶ã‚’æ¡ä»¶æˆ¦ï¼ˆ5ï¼‰ã¨ã—ã¦è¨­å®š")
            estimation_df.loc[estimation_df[grade_column].isnull(), grade_column] = 5
        
        # æ¨å®šçµæœã‚’å…ƒã®DataFrameã«åæ˜ 
        df_result.loc[grade_missing_mask, grade_column] = estimation_df[grade_column]
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ã‚’è¿½åŠ 
        df_result = self._add_grade_name_column(df_result, grade_column)
        
        estimated_count = initial_missing_count - df_result[grade_column].isnull().sum()
        if estimated_count > 0:
            self.logger.info(f"      âœ… ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šæˆåŠŸ: {estimated_count:,}ä»¶")
        
        return df_result
    
    def _estimate_from_prize(self, df: pd.DataFrame, grade_column: str, prize_col: str) -> pd.DataFrame:
        """è³é‡‘æƒ…å ±ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å®šã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            grade_column (str): ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ ¼ç´ã™ã‚‹åˆ—åã€‚
            prize_col (str): å‚ç…§ã™ã‚‹è³é‡‘åˆ—åã€‚

        Returns:
            pd.DataFrame: æ¨å®šçµæœã‚’åæ˜ ã—ãŸ DataFrameï¼ˆã‚³ãƒ”ãƒ¼ï¼‰ã€‚
        """
        if prize_col not in df.columns:
            return df
        
        # DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        df_result = df.copy()
        
        # æ•°å€¤åŒ–
        df_result[prize_col] = pd.to_numeric(df_result[prize_col], errors='coerce')
        
        # ã—ãã„å€¤ã‚’é©ç”¨
        thresholds_list = self.thresholds.to_thresholds_list()
        for min_prize, grade_value in thresholds_list:
            mask = (df_result[prize_col] >= min_prize) & df_result[grade_column].isnull()
            df_result.loc[mask, grade_column] = grade_value
        
        return df_result
    
    def _estimate_from_race_name(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """ãƒ¬ãƒ¼ã‚¹åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å®šã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            grade_column (str): ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ ¼ç´ã™ã‚‹åˆ—åã€‚

        Returns:
            pd.DataFrame: æ¨å®šçµæœã‚’åæ˜ ã—ãŸ DataFrameï¼ˆã‚³ãƒ”ãƒ¼ï¼‰ã€‚
        """
        if self.columns.RACE_NAME not in df.columns:
            return df
        
        # DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        df_result = df.copy()
        
        race_patterns = {
            1: self.patterns.G1_PATTERNS,
            2: self.patterns.G2_PATTERNS,
            3: self.patterns.G3_PATTERNS,
            4: self.patterns.STAKES_PATTERNS,
            5: self.patterns.CONDITIONS_PATTERNS
        }
        
        for grade, patterns in race_patterns.items():
            for pattern in patterns:
                mask = (df_result[self.columns.RACE_NAME].str.contains(pattern, case=False, na=False)) & df_result[grade_column].isnull()
                df_result.loc[mask, grade_column] = grade
        
        return df_result
    
    def _estimate_from_features(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """è·é›¢ã‚„é ­æ•°ãªã©ã®ç‰¹å¾´é‡ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å®šã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            grade_column (str): ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ ¼ç´ã™ã‚‹åˆ—åã€‚

        Returns:
            pd.DataFrame: æ¨å®šçµæœã‚’åæ˜ ã—ãŸ DataFrameï¼ˆã‚³ãƒ”ãƒ¼ï¼‰ã€‚
        """
        # DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        df_result = df.copy()
        
        # è·é›¢ã«ã‚ˆã‚‹æ¨å®š
        if self.columns.DISTANCE in df_result.columns:
            df_result[self.columns.DISTANCE] = pd.to_numeric(df_result[self.columns.DISTANCE], errors='coerce')
            
            long_distance_mask = (df_result[self.columns.DISTANCE] >= 3000) & df_result[grade_column].isnull()
            df_result.loc[long_distance_mask, grade_column] = 4  # é‡è³
            
            short_distance_mask = (df_result[self.columns.DISTANCE] < 1000) & df_result[grade_column].isnull()
            df_result.loc[short_distance_mask, grade_column] = 5  # ç‰¹åˆ¥
        
        # å‡ºèµ°é ­æ•°ã«ã‚ˆã‚‹æ¨å®š
        if self.columns.HORSE_COUNT in df_result.columns:
            df_result[self.columns.HORSE_COUNT] = pd.to_numeric(df_result[self.columns.HORSE_COUNT], errors='coerce')
            
            large_field_mask = (df_result[self.columns.HORSE_COUNT] >= 16) & df_result[grade_column].isnull()
            df_result.loc[large_field_mask, grade_column] = 4  # é‡è³
            
            small_field_mask = (df_result[self.columns.HORSE_COUNT] < 8) & df_result[grade_column].isnull()
            df_result.loc[small_field_mask, grade_column] = 5  # æ¡ä»¶æˆ¦
        
        return df_result
    
    def _add_grade_name_column(self, df: pd.DataFrame, grade_column: str) -> pd.DataFrame:
        """æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ã‚°ãƒ¬ãƒ¼ãƒ‰åã«å¤‰æ›ã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            grade_column (str): æ•°å€¤ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒæ ¼ç´ã•ã‚ŒãŸåˆ—åã€‚

        Returns:
            pd.DataFrame: ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—ã‚’ä»˜ä¸ã—ãŸ DataFrameï¼ˆã‚³ãƒ”ãƒ¼ï¼‰ã€‚
        """
        # DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        df_result = df.copy()
        
        df_result[grade_column] = pd.to_numeric(df_result[grade_column], errors='coerce')
        grade_names = df_result[grade_column].map(self.thresholds.GRADE_NAME_MAPPING)
        
        if self.columns.GRADE_NAME in df_result.columns:
            df_result[self.columns.GRADE_NAME] = grade_names
        else:
            grade_col_index = df_result.columns.get_loc(grade_column)
            df_result.insert(grade_col_index + 1, self.columns.GRADE_NAME, grade_names)
        
        return df_result

# =====================================
# é¦¬é½¢è¨ˆç®—å°‚ç”¨ã‚¯ãƒ©ã‚¹ï¼ˆSRPéµå®ˆï¼‰
# =====================================

class HorseAgeCalculator:
    """é¦¬é½¢è¨ˆç®—å°‚ç”¨ã‚¯ãƒ©ã‚¹"""
    
    DEFAULT_HORSE_AGE = 3  # æ—¥æœ¬ç«¶é¦¬ã®ä¸€èˆ¬çš„ãªãƒ‡ãƒ“ãƒ¥ãƒ¼å¹´é½¢
    VALID_AGE_RANGE = (2, 20)  # ç«¶èµ°é¦¬ã®å¦¥å½“ãªå¹´é½¢ç¯„å›²
    
    def __init__(self, columns: Optional[ColumnNames] = None):
        """
        Args:
            columns: åˆ—åè¨­å®š
        """
        self.columns = columns or ColumnNames()
        self.logger = logging.getLogger(__name__)
    
    def calculate_horse_age(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¡€çµ±ç™»éŒ²ç•ªå·ã¨å¹´æœˆæ—¥ã‹ã‚‰é¦¬é½¢ã‚’ç®—å‡ºã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚

        Returns:
            pd.DataFrame: é¦¬é½¢åˆ—ã‚’è¿½åŠ ã—ãŸ DataFrameï¼ˆã‚³ãƒ”ãƒ¼ï¼‰ã€‚
        """
        try:
            # DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆä¸å¤‰æ€§ã‚’ä¿è¨¼ï¼‰
            df_result = df.copy()
            
            # å¿…è¦ãªåˆ—ã®ç¢ºèª
            if self.columns.REGISTRATION_NUMBER not in df_result.columns or self.columns.RACE_DATE not in df_result.columns:
                self.logger.warning("âš ï¸ è¡€çµ±ç™»éŒ²ç•ªå·ã¾ãŸã¯å¹´æœˆæ—¥åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return df_result
            
            # ãƒ¬ãƒ¼ã‚¹æ—¥ã§å®‰å®šã‚½ãƒ¼ãƒˆã—ã€åˆå‡ºèµ°ãƒ¬ãƒ¼ã‚¹ã‚’ç¢ºå®Ÿã«å–å¾—
            if self.columns.RACE_DATE in df_result.columns:
                df_result = df_result.sort_values(by=self.columns.RACE_DATE, kind='stable')

            # é¦¬é½¢åˆ—ã‚’åˆæœŸåŒ–
            df_result[self.columns.HORSE_AGE] = None
            
            # é¦¬ã”ã¨ã«æœ€åˆã®ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
            horse_first_race = df_result.groupby(self.columns.HORSE_NAME, sort=False).first()
            
            horse_age_map = {}
            
            for horse_name, row in horse_first_race.iterrows():
                try:
                    registration_raw = row[self.columns.REGISTRATION_NUMBER]
                    race_date_raw = row[self.columns.RACE_DATE]

                    registration_number = re.sub(r'\D', '', str(registration_raw))
                    if len(registration_number) < 2:
                        self.logger.debug(f"âš ï¸ è¡€çµ±ç™»éŒ²ç•ªå·å½¢å¼ã‚¨ãƒ©ãƒ¼: {horse_name}")
                        horse_age_map[horse_name] = self.DEFAULT_HORSE_AGE
                        continue

                    birth_year = int(registration_number[:2])
                    birth_year = birth_year + 2000 if birth_year <= 30 else birth_year + 1900

                    if pd.isna(race_date_raw):
                        self.logger.debug(f"âš ï¸ æ—¥ä»˜æ¬ æ: {horse_name}")
                        horse_age_map[horse_name] = self.DEFAULT_HORSE_AGE
                        continue

                    race_date_digits = re.sub(r'\D', '', str(race_date_raw))
                    if len(race_date_digits) != 8:
                        self.logger.debug(f"âš ï¸ æ—¥ä»˜å½¢å¼ã‚¨ãƒ©ãƒ¼: {horse_name}")
                        horse_age_map[horse_name] = self.DEFAULT_HORSE_AGE
                        continue

                    race_year = int(race_date_digits[:4])

                    # é¦¬é½¢è¨ˆç®—ï¼ˆæ—¥æœ¬ç«¶é¦¬ã§ã¯1æœˆ1æ—¥ã«å…¨é¦¬ãŒåŠ é½¢ï¼‰
                    age = race_year - birth_year

                    if self.VALID_AGE_RANGE[0] <= age <= self.VALID_AGE_RANGE[1]:
                        horse_age_map[horse_name] = age
                    else:
                        self.logger.debug(f"âš ï¸ ç•°å¸¸ãªå¹´é½¢: {horse_name} (è¨ˆç®—å¹´é½¢:{age})")
                        horse_age_map[horse_name] = self.DEFAULT_HORSE_AGE
                        
                except (ValueError, TypeError) as e:
                    self.logger.debug(f"âš ï¸ å¹´é½¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {horse_name} - {str(e)}")
                    horse_age_map[horse_name] = self.DEFAULT_HORSE_AGE
            
            # é¦¬é½¢åˆ—ã«å€¤ã‚’è¨­å®š
            df_result[self.columns.HORSE_AGE] = df_result[self.columns.HORSE_NAME].map(horse_age_map)
            
            # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            age_counts = {}
            for age in horse_age_map.values():
                age_counts[age] = age_counts.get(age, 0) + 1
            
            self.logger.info(f"âœ… é¦¬é½¢è¨ˆç®—å®Œäº†: {len(horse_age_map)}é ­")
            self.logger.info(f"ğŸ“Š å¹´é½¢åˆ†å¸ƒ: {dict(sorted(age_counts.items()))}")
            
            return df_result
            
        except Exception as e:
            self.logger.error(f"âŒ é¦¬é½¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return df

# å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°è¨­å®š
def setup_logging(log_level: str = 'INFO', log_file: Optional[str] = None) -> None:
    """å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚

    Args:
        log_level (str): ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ï¼ˆä¾‹: ``INFO``, ``DEBUG``ï¼‰ã€‚
        log_file (str, optional): ãƒ­ã‚°å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚``None`` ã®å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ã¿ã€‚
    """
    # ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®š
    if log_file:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(log_file, encoding='utf-8')
            ]
        )
    else:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

class DataQualityChecker:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã€‚

    å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã«å¿…è¦ãªå“è³ªç®¡ç†æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚
    """
    
    def __init__(self):
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚"""
        self.quality_report = {}  # å„å‡¦ç†æ®µéšã®ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
        self.logger = logging.getLogger(__name__)
        
    def check_data_quality(self, df: pd.DataFrame, stage_name: str) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã® DataFrameã€‚
            stage_name (str): å‡¦ç†æ®µéšåï¼ˆä¾‹: ``BACå‡¦ç†å¾Œ``ï¼‰ã€‚

        Returns:
            Dict[str, Any]: å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’æ ¼ç´ã—ãŸè¾æ›¸ã€‚
        """
        self.logger.info(f"ğŸ“Š {stage_name} - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        start_time = time.time()
        
        report = {
            'stage': stage_name,  # å‡¦ç†æ®µéšåï¼ˆä¾‹ï¼š'BACå‡¦ç†å¾Œ', 'çµ±åˆå¾Œ'ï¼‰
            'timestamp': datetime.now().isoformat(),  # å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œæ™‚åˆ»ï¼ˆISOå½¢å¼ï¼‰
            'total_rows': len(df),  # ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼‰
            'total_columns': len(df.columns),  # ãƒ‡ãƒ¼ã‚¿åˆ—æ•°ï¼ˆã‚«ãƒ©ãƒ æ•°ï¼‰
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            'missing_values': {},  # æ¬ æå€¤åˆ†æçµæœï¼ˆåˆ—åˆ¥ã®æ¬ ææ•°ãƒ»å‰²åˆï¼‰
            'data_types': {},  # ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±ï¼ˆåˆ—åã¨ãƒ‡ãƒ¼ã‚¿å‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
            'duplicates': 0,  # é‡è¤‡è¡Œæ•°
            'outliers': {},  # å¤–ã‚Œå€¤æ¤œå‡ºçµæœï¼ˆåˆ—åˆ¥ã®å¤–ã‚Œå€¤æ•°ï¼‰
            'warnings': [],  # å“è³ªè­¦å‘Šãƒªã‚¹ãƒˆï¼ˆç•°å¸¸å€¤ã€ä¸æ­£ãƒ‡ãƒ¼ã‚¿ãªã©ï¼‰
            'recommendations': []  # æ”¹å–„æ¨å¥¨äº‹é …ãƒªã‚¹ãƒˆ
        }
        
        try:
            # 1. æ¬ æå€¤åˆ†æ
            self.logger.info("   ğŸ” æ¬ æå€¤åˆ†æä¸­...")
            missing_analysis = self._analyze_missing_values(df)
            report['missing_values'] = missing_analysis
            
            # 2. ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            self.logger.info("   ğŸ·ï¸ ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯ä¸­...")
            report['data_types'] = self._check_data_types(df)
            
            # 3. é‡è¤‡ãƒã‚§ãƒƒã‚¯
            self.logger.info("   ğŸ”„ é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
            report['duplicates'] = int(df.duplicated().sum())
            
            # 4. å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
            self.logger.info("   ğŸ“ˆ å¤–ã‚Œå€¤æ¤œå‡ºä¸­...")
            report['outliers'] = self._detect_outliers(df)
            
            # 5. ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼
            self.logger.info("   ğŸ“‹ ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼ä¸­...")
            warnings, recommendations = self._validate_business_rules(df)
            report['warnings'] = warnings
            report['recommendations'] = recommendations
            
            execution_time = time.time() - start_time
            report['execution_time_seconds'] = execution_time
            
            self.logger.info(f"âœ… {stage_name} - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº† ({execution_time:.2f}ç§’)")
            
            # ãƒ¬ãƒãƒ¼ãƒˆè¦ç´„ã‚’ãƒ­ã‚°å‡ºåŠ›
            self._log_quality_summary(report)
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            report['error'] = str(e)
        
        self.quality_report[stage_name] = report
        return report
    
    def _analyze_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ¬ æå€¤ã®è©³ç´°åˆ†æã‚’è¡Œã†ã€‚

        Args:
            df (pd.DataFrame): å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‚

        Returns:
            Dict[str, Any]: æ¬ æã‚»ãƒ«ç·æ•°ã‚„åˆ—åˆ¥å†…è¨³ãªã©ã®åˆ†æçµæœã€‚
        """
        missing_counts = df.isnull().sum()
        # æ¬ æå€¤ã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
        missing_percentages = (missing_counts / len(df)) * 100
        
        analysis = {
            'total_missing_cells': int(missing_counts.sum()),
            'columns_with_missing': {k: int(v) for k, v in missing_counts[missing_counts > 0].to_dict().items()},
            'missing_percentages': missing_percentages[missing_percentages > 0].to_dict(),
            'critical_columns': []  # 50%ä»¥ä¸Šæ¬ æã®åˆ—
        }
        
        # é‡è¦ãªæ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š
        for col, pct in missing_percentages.items():
            if pct >= 50:
                analysis['critical_columns'].append(col)
        
        return analysis
    
    def _check_data_types(self, df: pd.DataFrame) -> Dict[str, str]:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã€‚

        Args:
            df (pd.DataFrame): å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‚

        Returns:
            Dict[str, str]: åˆ—åã¨ãƒ‡ãƒ¼ã‚¿å‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã€‚
        """
        return {col: str(dtype) for col, dtype in df.dtypes.items()}
    
    def _detect_outliers(self, df: pd.DataFrame) -> Dict[str, int]:
        """IQR æ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡ºã‚’è¡Œã†ã€‚

        Args:
            df (pd.DataFrame): å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‚

        Returns:
            Dict[str, int]: åˆ—åˆ¥ã®å¤–ã‚Œå€¤ä»¶æ•°ã€‚
        """
        outlier_counts = {}
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if df[col].notna().sum() > 0:  # æ¬ æå€¤ã§ãªã„å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                outlier_counts[col] = int(len(outliers))
        
        return outlier_counts
    
    def _validate_business_rules(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼ã‚’è¡Œã†ã€‚

        Args:
            df (pd.DataFrame): å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‚

        Returns:
            Tuple[List[str], List[str]]: è­¦å‘Šãƒªã‚¹ãƒˆã¨æ¨å¥¨ãƒªã‚¹ãƒˆã€‚
        """
        warnings = []
        recommendations = []
        
        # ç€é †ã®ãƒã‚§ãƒƒã‚¯
        if 'ç€é †' in df.columns:
            invalid_positions = df[df['ç€é †'] < 0]
            if len(invalid_positions) > 0:
                warnings.append(f"ä¸æ­£ãªç€é †ãƒ‡ãƒ¼ã‚¿: {len(invalid_positions)}ä»¶")
        
        # ã‚¿ã‚¤ãƒ ã®ãƒã‚§ãƒƒã‚¯
        if 'ã‚¿ã‚¤ãƒ ' in df.columns:
            # ç•°å¸¸ã«é€Ÿã„/é…ã„ã‚¿ã‚¤ãƒ ã®æ¤œå‡º
            if df['ã‚¿ã‚¤ãƒ '].notna().sum() > 0:
                median_time = df['ã‚¿ã‚¤ãƒ '].median()
                if median_time and (median_time < 60 or median_time > 300):
                    warnings.append(f"ç•°å¸¸ãªã‚¿ã‚¤ãƒ ä¸­å¤®å€¤: {median_time}ç§’")
        
        # è·é›¢ã®ãƒã‚§ãƒƒã‚¯
        if 'è·é›¢' in df.columns:
            if df['è·é›¢'].notna().sum() > 0:
                min_distance = df['è·é›¢'].min()
                max_distance = df['è·é›¢'].max()
                if min_distance < 1000 or max_distance > 4000:
                    warnings.append(f"ç•°å¸¸ãªè·é›¢ç¯„å›²: {min_distance}m - {max_distance}m")
        
        # æ¨å¥¨äº‹é …
        if len(warnings) == 0:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿å“è³ªã¯è‰¯å¥½ã§ã™")
        else:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return warnings, recommendations
    
    def _log_quality_summary(self, report: Dict[str, Any]):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã€‚

        Args:
            report (Dict[str, Any]): å“è³ªãƒ¬ãƒãƒ¼ãƒˆè¾æ›¸ã€‚
        """
        self.logger.info(f"ğŸ“Š ã€{report['stage']}ã€‘å“è³ªã‚µãƒãƒªãƒ¼:")
        self.logger.info(f"   ğŸ“ ãƒ‡ãƒ¼ã‚¿è¦æ¨¡: {report['total_rows']:,}è¡Œ x {report['total_columns']}åˆ—")
        self.logger.info(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {report['memory_usage_mb']:.1f}MB")
        self.logger.info(f"   â“ æ¬ æã‚»ãƒ«æ•°: {report['missing_values']['total_missing_cells']:,}")
        self.logger.info(f"   ğŸ”„ é‡è¤‡è¡Œæ•°: {report['duplicates']:,}")
        
        if report['warnings']:
            self.logger.warning(f"   âš ï¸ è­¦å‘Š: {len(report['warnings'])}ä»¶")
            for warning in report['warnings']:
                self.logger.warning(f"      â€¢ {warning}")

class MissingValueHandler:
    """æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚¯ãƒ©ã‚¹ã€‚

    è¨ˆç”»æ›¸ Phase 0 ã®è¦ä»¶ã«åŸºã¥ãå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®æ¬ æå€¤å‡¦ç†ã‚’æä¾›ã™ã‚‹ã€‚
    """
    
    def __init__(self, columns: Optional[ColumnNames] = None):
        """æ¬ æå€¤å‡¦ç†ã§åˆ©ç”¨ã™ã‚‹ä¾å­˜ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Args:
            columns (ColumnNames, optional): åˆ—åè¨­å®šã€‚
        """
        self.columns = columns or ColumnNames()
        self.processing_log = []
        self.grade_estimator = GradeEstimator(columns=self.columns)  # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå°‚ç”¨ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
        self.age_calculator = HorseAgeCalculator(columns=self.columns)  # é¦¬é½¢è¨ˆç®—å°‚ç”¨ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
        self.logger = logging.getLogger(__name__)
        
    def handle_missing_values(self, df: pd.DataFrame, strategy_config: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            strategy_config (Dict[str, Any], optional): æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã€‚

        Returns:
            pd.DataFrame: æ¬ æå€¤å‡¦ç†ã‚’æ–½ã—ãŸ DataFrameã€‚
        """
        self.logger.info("ğŸ”§ æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†é–‹å§‹")
        start_time = time.time()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆ¦ç•¥è¨­å®š
        if strategy_config is None:
            strategy_config = self._get_default_strategy()
        
        df_processed = df.copy()
        original_rows = len(df_processed)
        
        try:
            # 1. é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_critical_columns(df_processed, strategy_config)
            
            # 2. æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_numeric_columns(df_processed, strategy_config)
            
            # 3. ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†
            df_processed = self._handle_categorical_columns(df_processed, strategy_config)
            
            # 4. æ®‹å­˜æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†
            df_processed = self._handle_remaining_missing(df_processed, strategy_config)
            
            # 5. é¦¬é½¢è¨ˆç®—ï¼ˆè¡€çµ±ç™»éŒ²ç•ªå·ã¨å¹´æœˆæ—¥ã‹ã‚‰ï¼‰- å°‚ç”¨ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
            df_processed = self.age_calculator.calculate_horse_age(df_processed)
            
            execution_time = time.time() - start_time
            final_rows = len(df_processed)
            
            self.logger.info(f"âœ… æ¬ æå€¤å‡¦ç†å®Œäº† ({execution_time:.2f}ç§’)")
            self.logger.info(f"   ğŸ“Š å‡¦ç†å‰: {original_rows:,}è¡Œ")
            self.logger.info(f"   ğŸ“Š å‡¦ç†å¾Œ: {final_rows:,}è¡Œ")
            self.logger.info(f"   ğŸ“‰ é™¤å»è¡Œæ•°: {original_rows - final_rows:,}è¡Œ ({((original_rows - final_rows) / original_rows) * 100:.1f}%)")
            
            # å‡¦ç†ãƒ­ã‚°ã®ä¿å­˜
            self._save_processing_log(df_processed)
            
        except Exception as e:
            self.logger.error(f"âŒ æ¬ æå€¤å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
        
        return df_processed
    
    def _get_default_strategy(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã‚’è¿”ã—ã¾ã™ã€‚

        Returns:
            Dict[str, Any]: æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã®è¨­å®šè¾æ›¸ã€‚
        """
        return {
            'critical_columns': {
                self.columns.POSITION: 'drop',  # ç€é †ãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                self.columns.DISTANCE: 'drop',  # è·é›¢ãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                self.columns.HORSE_NAME: 'drop',  # é¦¬åãŒæ¬ æã®è¡Œã¯å‰Šé™¤
                self.columns.IDM: 'drop'  # IDMãŒæ¬ æã®è¡Œã¯å‰Šé™¤
            },
            'numeric_columns': {
                'method': 'median',  # ä¸­å¤®å€¤ã§è£œå®Œ
                'max_missing_rate': 0.5  # 50%ä»¥ä¸Šæ¬ æã®åˆ—ã¯å‰Šé™¤
            },
            'categorical_columns': {
                'method': 'mode',  # æœ€é »å€¤ã§è£œå®Œ
                'unknown_label': 'ä¸æ˜',
                'max_missing_rate': 0.8  # 80%ä»¥ä¸Šæ¬ æã®åˆ—ã¯å‰Šé™¤
            },
            # æ®‹å­˜æ¬ æå€¤ã¯é‡è¦åˆ—ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã®ã¿è¡Œå‰Šé™¤ï¼ˆå®Ÿå‹™ãƒ¬ãƒãƒ¼ãƒˆæ–¹é‡ï¼‰
            'remaining_strategy': 'drop_subset',
            'remaining_subset': [
                self.columns.POSITION, 
                self.columns.DISTANCE, 
                self.columns.HORSE_NAME, 
                self.columns.IDM, 
                self.columns.GRADE
            ]
        }
    
    def _handle_critical_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """é‡è¦åˆ—ã«å¯¾ã™ã‚‹æ¬ æå€¤å‡¦ç†ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            config (Dict[str, Any]): æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã€‚

        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã® DataFrameã€‚
        """
        self.logger.info("   ğŸ¯ é‡è¦åˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        critical_config = config.get('critical_columns', {})
        
        for column, strategy in critical_config.items():
            if column in df.columns:
                missing_count = df[column].isnull().sum()
                if missing_count > 0:
                    self.logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã®æ¬ æå€¤ã‚’{strategy}å‡¦ç†")
                    
                    if strategy == 'drop':
                        df = df.dropna(subset=[column])
                        self.processing_log.append(f"{column}: {missing_count}è¡Œã‚’å‰Šé™¤ï¼ˆé‡è¦åˆ—ï¼‰")
        
        return df
    
    def _handle_numeric_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            config (Dict[str, Any]): æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã€‚

        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã® DataFrameã€‚
        """
        self.logger.info("   ğŸ”¢ æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        numeric_config = config.get('numeric_columns', {})
        method = numeric_config.get('method', 'median')
        max_missing_rate = numeric_config.get('max_missing_rate', 0.5)
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ãŒæ–‡å­—åˆ—ã§ã‚‚æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ãŒå‹•ãã‚ˆã†ã«æ•°å€¤åŒ–ã‚’è©¦ã¿ã‚‹
        grade_columns = self.columns.get_grade_columns()
        for grade_col in grade_columns:
            if grade_col in df.columns:
                df[grade_col] = pd.to_numeric(df[grade_col], errors='coerce')

        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        # è³é‡‘é–¢é€£ã®åˆ—ã‚’æ¬ æå€¤å‡¦ç†ã®å¯¾è±¡ã‹ã‚‰é™¤å¤–ï¼ˆæ¬ æãŒå¤šãã¦å‰Šé™¤ã•ã‚Œã‚‹ã®ã‚’é˜²ãï¼‰
        prize_columns = self.columns.get_prize_columns()
        columns_to_process = [
            col for col in numeric_columns 
            if col not in prize_columns
        ]

        for column in columns_to_process:
            missing_count = df[column].isnull().sum()
            missing_rate = missing_count / len(df) if len(df) > 0 else 0
            
            if missing_count > 0:
                # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰- å°‚ç”¨ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
                if column in grade_columns:
                    self.logger.info(f"      â€¢ {column}: å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†ã‚’å®Ÿè¡Œ")
                    df = self.grade_estimator.estimate_grade(df, column)
                    
                    # æ¨å®šå¾Œã®æ¬ ææ•°ã‚’ãƒã‚§ãƒƒã‚¯
                    remaining_missing = df[column].isnull().sum()
                    estimated_count = missing_count - remaining_missing
                    
                    if estimated_count > 0:
                        self.processing_log.append(f"{column}: è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰{estimated_count}ä»¶æ¨å®šâ†’ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ")
                
                elif missing_rate > max_missing_rate:
                    self.logger.warning(f"      â€¢ {column}: æ¬ æç‡{missing_rate:.1%} > {max_missing_rate:.1%} â†’ åˆ—å‰Šé™¤")
                    df = df.drop(columns=[column])
                    self.processing_log.append(f"{column}: é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤")
                else:
                    if method == 'median':
                        fill_value = df[column].median()
                    elif method == 'mean':
                        fill_value = df[column].mean()
                    else:
                        fill_value = 0
                    
                    df[column] = df[column].fillna(fill_value)
                    self.logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’{method}({fill_value})ã§è£œå®Œ")
                    self.processing_log.append(f"{column}: {method}ã§{missing_count}ä»¶è£œå®Œ")
        
        return df
    
    def _handle_categorical_columns(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            config (Dict[str, Any]): æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã€‚

        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã® DataFrameã€‚
        """
        self.logger.info("   ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†ä¸­...")
        
        categorical_config = config.get('categorical_columns', {})
        method = categorical_config.get('method', 'mode')
        unknown_label = categorical_config.get('unknown_label', 'ä¸æ˜')
        max_missing_rate = categorical_config.get('max_missing_rate', 0.8)
        
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns
        grade_columns = self.columns.get_grade_columns() + [self.columns.GRADE_NAME]
        
        for column in categorical_columns:
            # ã‚°ãƒ¬ãƒ¼ãƒ‰ã¯ãƒ¢ãƒ¼ãƒ‰è£œå®Œã®å¯¾è±¡ã‹ã‚‰é™¤å¤–ï¼ˆæ¨å®šãƒ­ã‚¸ãƒƒã‚¯ã«å§”ã­ã‚‹ï¼‰
            if column in grade_columns:
                continue
            
            # ã‚°ãƒ¬ãƒ¼ãƒ‰_yã®ç‰¹åˆ¥å‡¦ç†ï¼ˆäºˆæ¸¬ãƒãƒ¼ã‚¯ä»˜ãï¼‰
            if column == self.columns.GRADE_Y:
                missing_count = df[column].isnull().sum()
                if missing_count > 0:
                    self.logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’mode(ç‰¹åˆ¥)ã§è£œå®Œï¼ˆäºˆæ¸¬ãƒãƒ¼ã‚¯ä»˜ãï¼‰")
                    df[column] = df[column].fillna('ç‰¹åˆ¥ï¼ˆäºˆæ¸¬ï¼‰')
                    self.processing_log.append(f"{column}: {missing_count}ä»¶ã‚’mode(ç‰¹åˆ¥)ã§è£œå®Œï¼ˆäºˆæ¸¬ãƒãƒ¼ã‚¯ä»˜ãï¼‰")
                continue
            
            missing_count = df[column].isnull().sum()
            missing_rate = missing_count / len(df) if len(df) > 0 else 0
            
            if missing_count > 0:
                if missing_rate > max_missing_rate:
                    self.logger.warning(f"      â€¢ {column}: æ¬ æç‡{missing_rate:.1%} > {max_missing_rate:.1%} â†’ åˆ—å‰Šé™¤")
                    df = df.drop(columns=[column])
                    self.processing_log.append(f"{column}: é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤")
                else:
                    if method == 'mode':
                        mode_values = df[column].mode()
                        fill_value = mode_values.iloc[0] if not mode_values.empty else unknown_label
                    else:
                        fill_value = unknown_label
                    
                    df[column] = df[column].fillna(fill_value)
                    self.logger.info(f"      â€¢ {column}: {missing_count:,}ä»¶ã‚’{method}({fill_value})ã§è£œå®Œ")
                    self.processing_log.append(f"{column}: {method}ã§{missing_count}ä»¶è£œå®Œ")
        
        return df
    
    def _handle_remaining_missing(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """æ®‹å­˜ã™ã‚‹æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã® DataFrameã€‚
            config (Dict[str, Any]): æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã€‚

        Returns:
            pd.DataFrame: æ®‹å­˜æ¬ æå€¤ã‚’å‡¦ç†ã—ãŸ DataFrameã€‚
        """
        remaining_missing = df.isnull().sum().sum()
        
        if remaining_missing > 0:
            self.logger.info(f"   ğŸ”§ æ®‹å­˜æ¬ æå€¤å‡¦ç†ä¸­: {remaining_missing:,}ä»¶")
            
            strategy = config.get('remaining_strategy', 'drop')
            
            if strategy == 'drop':
                initial_rows = len(df)
                df = df.dropna()
                dropped_rows = initial_rows - len(df)
                
                if dropped_rows > 0:
                    self.logger.info(f"      â€¢ æ®‹å­˜æ¬ æå€¤ã®ã‚ã‚‹{dropped_rows:,}è¡Œã‚’å‰Šé™¤")
                    self.processing_log.append(f"æ®‹å­˜æ¬ æå€¤: {dropped_rows}è¡Œå‰Šé™¤")
            elif strategy == 'drop_subset':
                subset = config.get('remaining_subset', [])
                subset = [col for col in subset if col in df.columns]
                if subset:
                    initial_rows = len(df)
                    df = df.dropna(subset=subset)
                    dropped_rows = initial_rows - len(df)
                    if dropped_rows > 0:
                        self.logger.info(f"      â€¢ é‡è¦åˆ—({', '.join(subset)})ã®æ®‹å­˜æ¬ æ{dropped_rows:,}è¡Œã‚’å‰Šé™¤")
                        self.processing_log.append(f"æ®‹å­˜æ¬ æ(é‡è¦åˆ—): {dropped_rows}è¡Œå‰Šé™¤")
        
        return df
    
    
    def _save_processing_log(self, df: pd.DataFrame):
        """å‡¦ç†ãƒ­ã‚°ã‚’è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜ã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): æœ€çµ‚çš„ãªå‡¦ç†çµæœã® DataFrameã€‚
        """
        log_path = Path('export/missing_value_processing_log.txt')
        
        try:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            write_header = not log_path.exists()
            
            with open(log_path, 'a', encoding='utf-8') as f:  # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
                if write_header:
                    f.write(f"æ¬ æå€¤å‡¦ç†ãƒ­ã‚° - {datetime.now()}\n")
                    f.write("=" * 50 + "\n\n")
                
                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãƒ­ã‚°ã‚’è¿½è¨˜
                for log_entry in self.processing_log:
                    f.write(f"â€¢ {log_entry}\n")
                
                # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚’è¿½è¨˜
                f.write(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}\n")
                f.write(f"æ®‹å­˜æ¬ æå€¤: {df.isnull().sum().sum()}ä»¶\n\n")
            
            self.logger.info(f"   ğŸ“ å‡¦ç†ãƒ­ã‚°ä¿å­˜: {log_path}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ å‡¦ç†ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        finally:
            self.processing_log.clear()

class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¯ãƒ©ã‚¹ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
    
    def __init__(self):
        self.start_time = time.time()
        self.logger = logging.getLogger(__name__)
    
    def log_system_status(self, stage_name: str):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ã—ã¾ã™ã€‚

        Args:
            stage_name (str): å‡ºåŠ›å¯¾è±¡ã®å‡¦ç†æ®µéšåã€‚
        """
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        
        logger.info(f"ğŸ’» [{stage_name}] ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:")
        logger.info(f"   â±ï¸ çµŒéæ™‚é–“: {elapsed_time:.1f}ç§’")

def ensure_export_dirs():
    """å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªã¨ä½œæˆã‚’è¡Œã†ã€‚"""
    logger = logging.getLogger(__name__)
    
    dirs = [
        'export/BAC', 
        'export/SRB', 
        'export/SED', 
        'export/dataset',          # å®Ÿéš›ã®SED+SRBçµ±åˆãƒ‡ãƒ¼ã‚¿å‡ºåŠ›å…ˆ
        'export/quality_reports',     # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ç”¨
        'export/logs'                 # ãƒ­ã‚°ä¿å­˜ç”¨
    ]
    
    created_dirs = []
    
    for dir_path in dirs:
        path_obj = Path(dir_path)
        if not path_obj.exists():
            path_obj.mkdir(parents=True, exist_ok=True)
            created_dirs.append(dir_path)
            logger.info(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {dir_path}")
    
    if created_dirs:
        logger.info(f"âœ… {len(created_dirs)}å€‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ")
    else:
        logger.info("ğŸ“ ã™ã¹ã¦ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™")

def save_quality_report(quality_checker: DataQualityChecker):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ JSON ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

    Args:
        quality_checker (DataQualityChecker): å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿æŒã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    """
    import json
    
    logger = logging.getLogger(__name__)
    report_path = Path('export/quality_reports/data_quality_report.json')
    
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(quality_checker.quality_report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        
    except Exception as e:
        logger.warning(f"âš ï¸ å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

def display_deletion_statistics():
    """ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æã«ã‚ˆã‚‹å‰Šé™¤çµ±è¨ˆã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    logger = logging.getLogger(__name__)
    
    try:
        def _count_csv_rows(file_path: Path) -> int:
            buffer_size = 1024 * 1024
            newline_count = 0
            last_char = b'\n'

            with file_path.open('rb') as f:
                while True:
                    chunk = f.read(buffer_size)
                    if not chunk:
                        break
                    newline_count += chunk.count(b'\n')
                    last_char = chunk[-1:]

            line_count = newline_count
            if last_char not in (b'\n', b''):
                line_count += 1

            return max(line_count - 1, 0)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        sed_dir = Path('export/SED/formatted')
        bias_dir = Path('export/dataset')
        
        if not sed_dir.exists() or not bias_dir.exists():
            logger.warning("âš ï¸ æ¯”è¼ƒç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        sed_files = list(sed_dir.glob('*.csv'))
        bias_files = list(bias_dir.glob('*.csv'))
        
        if not sed_files or not bias_files:
            logger.warning("âš ï¸ æ¯”è¼ƒç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # çµ±è¨ˆã‚’åé›†
        total_sed = 0
        total_bias = 0
        total_deleted = 0
        deletion_files = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒãƒƒãƒ”ãƒ³ã‚°
        sed_files_dict = {f.stem.replace('_formatted', ''): f for f in sed_files}
        
        for bias_file in bias_files:
            base_name = bias_file.stem.replace('_formatted_dataset', '')
            
            if base_name in sed_files_dict:
                sed_file = sed_files_dict[base_name]
                
                try:
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’æ•°ãˆã‚‹ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰
                    sed_count = _count_csv_rows(sed_file)
                    bias_count = _count_csv_rows(bias_file)
                    
                    deleted = sed_count - bias_count
                    total_sed += sed_count
                    total_bias += bias_count
                    total_deleted += deleted
                    
                    if deleted > 0:
                        deletion_rate = (deleted / sed_count * 100) if sed_count > 0 else 0
                        deletion_files.append({
                            'file': base_name,
                            'deleted': deleted,
                            'deletion_rate': deletion_rate
                        })
                
                except Exception:
                    continue
        
        # çµ±è¨ˆè¡¨ç¤º
        logger.info("ğŸ“ˆ å…¨ä½“å‰Šé™¤çµ±è¨ˆ:")
        logger.info(f"   ğŸ“¥ å‡¦ç†å‰ç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {total_sed:,}ä»¶")
        logger.info(f"   ğŸ“¤ å‡¦ç†å¾Œç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {total_bias:,}ä»¶")
        logger.info(f"   âŒ å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_deleted:,}ä»¶")
        logger.info(f"   ğŸ“‰ å…¨ä½“å‰Šé™¤ç‡: {(total_deleted/total_sed*100 if total_sed > 0 else 0):.2f}%")
        logger.info(f"   ğŸ—‚ï¸ å‰Šé™¤ç™ºç”Ÿãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(deletion_files)}")
        logger.info(f"   ğŸ“Š å‰Šé™¤ç™ºç”Ÿç‡: {(len(deletion_files)/len(sed_files_dict)*100 if sed_files_dict else 0):.1f}%")
        
        if deletion_files:
            logger.info("\nğŸ“‹ å‰Šé™¤ã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸Šä½10ä»¶ï¼‰:")
            deletion_files.sort(key=lambda x: x['deleted'], reverse=True)
            for i, item in enumerate(deletion_files[:10], 1):
                logger.info(f"   {i:2d}. {item['file']}: -{item['deleted']:,}ä»¶ (-{item['deletion_rate']:.1f}%)")
        else:
            logger.info("âœ… ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æã«ã‚ˆã‚‹å‰Šé™¤ã¯ç™ºç”Ÿã—ã¦ã„ã¾ã›ã‚“")
    
    except Exception as e:
        logger.warning(f"âš ï¸ å‰Šé™¤çµ±è¨ˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(e)}")

def summarize_processing_log():
    """æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    logger = logging.getLogger(__name__)
    
    log_file = Path('export/missing_value_processing_log.txt')
    backup_file = Path('export/missing_value_processing_log_original.txt')
    summary_file = Path('export/missing_value_processing_summary.txt')
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if not log_file.exists():
        logger.info("ğŸ“ æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return
    
    logger.info("ğŸ“Š æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã‚’ã‚µãƒãƒªãƒ¼å½¢å¼ã«æ•´ç†ä¸­...")
    
    try:
        # ãƒ­ã‚°è§£æ
        stats = _parse_processing_log(log_file)
        
        if not stats:
            logger.warning("âš ï¸ ãƒ­ã‚°è§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        _generate_summary_report(stats, summary_file)
        
        # å…ƒãƒ­ã‚°ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if backup_file.exists():
            backup_file.unlink()  # æ—¢å­˜ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤
        log_file.rename(backup_file)
        
        # ã‚µãƒãƒªãƒ¼ã‚’æ–°ã—ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«
        summary_file.rename(log_file)
        
        logger.info("âœ… æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®æ•´ç†å®Œäº†")
        logger.info(f"   ğŸ“‹ ã‚µãƒãƒªãƒ¼: {log_file}")
        logger.info(f"   ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
        logger.info(f"   ğŸ“Š å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
        if stats['idm_deletions']:
            total_idm = sum(stats['idm_deletions'])
            logger.info(f"   ğŸ¯ IDMå‰Šé™¤: {total_idm:,}è¡Œ ({len(stats['idm_deletions'])}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
        if stats['grade_estimations']:
            total_grade = sum(stats['grade_estimations'])
            logger.info(f"   ğŸ† ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š: {total_grade:,}ä»¶ ({len(stats['grade_estimations'])}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ­ã‚°ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

def _parse_processing_log(log_file: Path) -> Optional[Dict[str, Any]]:
    """æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã‚’è§£æã—ã¦çµ±è¨ˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        log_file (Path): è§£æå¯¾è±¡ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚

    Returns:
        Optional[Dict[str, Any]]: ãƒ­ã‚°è§£æçµæœã®çµ±è¨ˆæƒ…å ±ã€‚
    """
    logger = logging.getLogger(__name__)
    
    # çµ±è¨ˆæƒ…å ±æ ¼ç´ç”¨
    stats = {
        'idm_deletions': [],
        'grade_estimations': [],
        'median_imputations': defaultdict(list),
        'dropped_columns': set(),
        'categorical_imputations': defaultdict(list),
        'other_imputations': defaultdict(list),
        'total_files': 0,
        'final_shapes': []
    }
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        logger.error(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}
    
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('==') or line.startswith('æ¬ æå€¤å‡¦ç†ãƒ­ã‚°'):
            continue
            
        # IDMå‰Šé™¤
        if 'IDM:' in line and 'è¡Œã‚’å‰Šé™¤ï¼ˆé‡è¦åˆ—ï¼‰' in line:
            match = re.search(r'IDM: (\d+)è¡Œã‚’å‰Šé™¤', line)
            if match:
                stats['idm_deletions'].append(int(match.group(1)))
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š
        elif 'ã‚°ãƒ¬ãƒ¼ãƒ‰:' in line and 'æ¨å®šâ†’ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ' in line:
            match = re.search(r'ã‚°ãƒ¬ãƒ¼ãƒ‰: è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰(\d+)ä»¶æ¨å®š', line)
            if match:
                stats['grade_estimations'].append(int(match.group(1)))
        
        # ä¸­å¤®å€¤è£œå®Œ
        elif 'medianã§' in line and 'ä»¶è£œå®Œ' in line:
            match = re.search(r'â€¢ ([^:]+): medianã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                count = int(match.group(2))
                stats['median_imputations'][column_name].append(count)
        
        # é«˜æ¬ æç‡ã«ã‚ˆã‚‹åˆ—å‰Šé™¤
        elif 'é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤' in line:
            match = re.search(r'â€¢ ([^:]+): é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤', line)
            if match:
                stats['dropped_columns'].add(match.group(1))
        
        # ã‚«ãƒ†ã‚´ãƒªè£œå®Œï¼ˆãƒ¬ãƒ¼ã‚¹åã€é¦¬ä½“é‡å¢—æ¸›ï¼‰
        elif line.startswith('â€¢ ãƒ¬ãƒ¼ã‚¹å:') or line.startswith('â€¢ ãƒ¬ãƒ¼ã‚¹åç•¥ç§°:') or line.startswith('â€¢ é¦¬ä½“é‡å¢—æ¸›:'):
            match = re.search(r'â€¢ ([^:]+): (.+)ã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                value = match.group(2)
                count = int(match.group(3))
                stats['categorical_imputations'][column_name].append((value, count))
        
        # ãã®ä»–ã®è£œå®Œå‡¦ç†
        elif 'ä»¶è£œå®Œ' in line and 'median' not in line:
            match = re.search(r'â€¢ ([^:]+): (.+)ã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                value = match.group(2)
                count = int(match.group(3))
                stats['other_imputations'][column_name].append((value, count))
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
        elif 'æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:' in line:
            match = re.search(r'æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: \((\d+), (\d+)\)', line)
            if match:
                rows = int(match.group(1))
                cols = int(match.group(2))
                stats['final_shapes'].append((rows, cols))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ¨å®šï¼ˆIDMå‰Šé™¤ã®å›æ•°ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šã®å›æ•°ã®åˆè¨ˆï¼‰
    stats['total_files'] = len(stats['idm_deletions']) + len(stats['grade_estimations'])
    
    return stats

def _generate_summary_report(stats: Dict[str, Any], output_file: Path):
    """çµ±è¨ˆæƒ…å ±ã‹ã‚‰ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        stats (Dict[str, Any]): ãƒ­ã‚°è§£æã«ã‚ˆã£ã¦å¾—ã‚‰ã‚ŒãŸçµ±è¨ˆæƒ…å ±ã€‚
        output_file (Path): å‡ºåŠ›å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
    """
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ğŸ“Š æ¬ æå€¤å‡¦ç†ãƒ­ã‚° ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        f.write(f"ğŸ“ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        
        # IDMå‰Šé™¤çµ±è¨ˆ
        if stats['idm_deletions']:
            total_idm = sum(stats['idm_deletions'])
            f.write("ğŸ¯ IDMæ¬ æå€¤å‰Šé™¤å‡¦ç†:\n")
            f.write(f"   â€¢ å‡¦ç†å›æ•°: {len(stats['idm_deletions'])}å›\n")
            f.write(f"   â€¢ ç·å‰Šé™¤è¡Œæ•°: {total_idm:,}è¡Œ\n")
            f.write(f"   â€¢ å¹³å‡å‰Šé™¤è¡Œæ•°: {total_idm/len(stats['idm_deletions']):.1f}è¡Œ\n\n")
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šçµ±è¨ˆ
        if stats['grade_estimations']:
            total_grade = sum(stats['grade_estimations'])
            f.write("ğŸ† ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†:\n")
            f.write(f"   â€¢ å‡¦ç†å›æ•°: {len(stats['grade_estimations'])}å›\n")
            f.write(f"   â€¢ ç·æ¨å®šä»¶æ•°: {total_grade:,}ä»¶\n")
            f.write(f"   â€¢ å¹³å‡æ¨å®šä»¶æ•°: {total_grade/len(stats['grade_estimations']):.1f}ä»¶\n\n")
        
        # ä¸­å¤®å€¤è£œå®Œçµ±è¨ˆ
        if stats['median_imputations']:
            f.write("ğŸ”¢ ä¸­å¤®å€¤è£œå®Œå‡¦ç†:\n")
            for column, counts in stats['median_imputations'].items():
                total_count = sum(counts)
                f.write(f"   â€¢ {column}: {len(counts)}å›, ç·è£œå®Œ{total_count:,}ä»¶ (å¹³å‡{total_count/len(counts):.1f}ä»¶)\n")
            f.write("\n")
        
        # é«˜æ¬ æç‡åˆ—å‰Šé™¤
        if stats['dropped_columns']:
            f.write("âŒ é«˜æ¬ æç‡ã«ã‚ˆã‚Šå‰Šé™¤ã•ã‚ŒãŸåˆ—:\n")
            sorted_columns = sorted(stats['dropped_columns'])
            for i, column in enumerate(sorted_columns, 1):
                f.write(f"   {i:2d}. {column}\n")
            f.write(f"\n   ğŸ“Š å‰Šé™¤åˆ—æ•°: {len(sorted_columns)}åˆ—\n\n")
        
        # ã‚«ãƒ†ã‚´ãƒªè£œå®Œçµ±è¨ˆ
        if stats['categorical_imputations']:
            f.write("ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªè£œå®Œå‡¦ç†:\n")
            for column, values in stats['categorical_imputations'].items():
                total_count = sum(count for _, count in values)
                unique_values = len(set(value for value, _ in values))
                f.write(f"   â€¢ {column}: {len(values)}å›, ç·è£œå®Œ{total_count:,}ä»¶, {unique_values}ç¨®é¡ã®å€¤\n")
            f.write("\n")
        
        # ãã®ä»–è£œå®Œçµ±è¨ˆ
        if stats['other_imputations']:
            f.write("ğŸ”§ ãã®ä»–è£œå®Œå‡¦ç†:\n")
            for column, values in stats['other_imputations'].items():
                total_count = sum(count for _, count in values)
                f.write(f"   â€¢ {column}: {len(values)}å›, ç·è£œå®Œ{total_count:,}ä»¶\n")
            f.write("\n")
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        if stats['final_shapes']:
            total_rows = sum(rows for rows, _ in stats['final_shapes'])
            total_cols = sum(cols for _, cols in stats['final_shapes'])
            avg_rows = total_rows / len(stats['final_shapes']) if stats['final_shapes'] else 0
            avg_cols = total_cols / len(stats['final_shapes']) if stats['final_shapes'] else 0
            
            f.write("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\n")
            f.write(f"   â€¢ ç·è¡Œæ•°: {total_rows:,}è¡Œ\n")
            f.write(f"   â€¢ å¹³å‡è¡Œæ•°: {avg_rows:.1f}è¡Œ/ãƒ•ã‚¡ã‚¤ãƒ«\n")
            f.write(f"   â€¢ å¹³å‡åˆ—æ•°: {avg_cols:.1f}åˆ—/ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ‰ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«æ¬ æå€¤å‡¦ç† å®Œäº†ã‚µãƒãƒªãƒ¼\n")
        f.write("=" * 80 + "\n")

def process_race_data(exclude_turf: bool = False, turf_only: bool = False, 
                     enable_missing_value_handling: bool = True, enable_quality_check: bool = True) -> bool:
    """ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ï¼ˆæ¨™æº–ç‰ˆï¼‰ã€‚

    è¨ˆç”»æ›¸ Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã®å®Ÿè£…ã€‚
    
    ã“ã®é–¢æ•°ã¯RaceDataProcessorã‚¯ãƒ©ã‚¹ã®ã‚·ãƒ³ãƒ©ãƒƒãƒ‘ãƒ¼ã§ã™ã€‚
    å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

    Args:
        exclude_turf (bool): èŠã‚³ãƒ¼ã‚¹ã‚’é™¤å¤–ã™ã‚‹ã‹ã©ã†ã‹ã€‚
        turf_only (bool): èŠã‚³ãƒ¼ã‚¹ã®ã¿ã‚’å‡¦ç†ã™ã‚‹ã‹ã©ã†ã‹ã€‚
        enable_missing_value_handling (bool): æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹ã€‚
        enable_quality_check (bool): ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹ã€‚

    Returns:
        bool: æˆåŠŸæ™‚ ``True``ã€å¤±æ•—æ™‚ ``False``ã€‚
    """
    logger.info("ğŸ‡ â–  ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ â– ")
    
    # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
    monitor = SystemMonitor()
    
    # å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    if exclude_turf and turf_only:
        logger.error("âŒ èŠã‚³ãƒ¼ã‚¹ã‚’é™¤å¤–ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨èŠã‚³ãƒ¼ã‚¹ã®ã¿ã‚’å‡¦ç†ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        return False
    
    # é€šå¸¸ã®å‡¦ç†è¨­å®šã®ãƒ­ã‚°å‡ºåŠ›
    logger.info("ğŸ“‹ å‡¦ç†è¨­å®š:")
    logger.info(f"   ğŸŒ± èŠã‚³ãƒ¼ã‚¹é™¤å¤–: {'ã¯ã„' if exclude_turf else 'ã„ã„ãˆ'}")
    logger.info(f"   ğŸŒ± èŠã‚³ãƒ¼ã‚¹ã®ã¿: {'ã¯ã„' if turf_only else 'ã„ã„ãˆ'}")
    logger.info(f"   ğŸ”§ æ¬ æå€¤å‡¦ç†: {'æœ‰åŠ¹' if enable_missing_value_handling else 'ç„¡åŠ¹'}")
    logger.info(f"   ğŸ“ˆ å“è³ªãƒã‚§ãƒƒã‚¯: {'æœ‰åŠ¹' if enable_quality_check else 'ç„¡åŠ¹'}")
    
    # ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
    quality_checker = DataQualityChecker() if enable_quality_check else None
    
    # å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    ensure_export_dirs()
    monitor.log_system_status("åˆæœŸåŒ–å®Œäº†")
    
    try:
        # 1. BACãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-1: BACãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ï¼‰ã®å‡¦ç†")
        logger.info("="*60)
        
        process_all_bac_files(exclude_turf=exclude_turf, turf_only=turf_only)
        monitor.log_system_status("BACå‡¦ç†å®Œäº†")
    
        # 2. SRBãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-2: SRBãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¬ãƒ¼ã‚¹è©³ç´°æƒ…å ±ï¼‰ã®å‡¦ç†")
        logger.info("="*60)
        
        process_all_srb_files(exclude_turf=exclude_turf, turf_only=turf_only)
        monitor.log_system_status("SRBå‡¦ç†å®Œäº†")
    
        # 3. SEDãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨SRBãƒ»BACãƒ‡ãƒ¼ã‚¿ã¨ã®ç´ã¥ã‘
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-3: SEDãƒ‡ãƒ¼ã‚¿ï¼ˆç«¶èµ°æˆç¸¾ï¼‰ã®å‡¦ç†ã¨ç´ã¥ã‘")
        logger.info("="*60)
        
        process_all_sed_files(exclude_turf=exclude_turf, turf_only=turf_only)
    
        # 4. SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®ç´ã¥ã‘
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‚ Phase 0-4: SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ")
        logger.info("="*60)
        logger.info("ğŸ“‹ ãƒã‚¤ã‚¢ã‚¹æƒ…å ±å®Œå‚™ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿æŒã—ã¾ã™")
        
        merge_result = merge_srb_with_sed(
            separate_output=True, 
            exclude_turf=exclude_turf, 
            turf_only=turf_only
        )
        
        if not merge_result:
            logger.error("âŒ SEDãƒ‡ãƒ¼ã‚¿ã¨SRBãƒ‡ãƒ¼ã‚¿ã®ç´ã¥ã‘ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†:")
        logger.info("   ğŸ“ SEDãƒ‡ãƒ¼ã‚¿: export/SED/")
        logger.info("   ğŸ“ SRBãƒ‡ãƒ¼ã‚¿: export/SRB/")
        logger.info("   ğŸ“ çµ±åˆãƒ‡ãƒ¼ã‚¿: export/dataset/")
        
        monitor.log_system_status("ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†")
        
        # 5. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆå¾Œï¼‰
        if enable_quality_check:
            logger.info("\n" + "="*60)
            logger.info("ğŸ“Š Phase 0-5: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯")
            logger.info("="*60)
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            sample_files = list(Path('export/dataset').glob('*.csv'))
            if sample_files:
                sample_file = sample_files[0]
                logger.info(f"ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§å“è³ªãƒã‚§ãƒƒã‚¯: {sample_file.name}")
                
                try:
                    sample_df = pd.read_csv(sample_file, encoding='utf-8')
                    quality_checker.check_data_quality(sample_df, "çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿")
                except Exception as e:
                    logger.warning(f"âš ï¸ å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # 7. å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
        if enable_quality_check and quality_checker:
            save_quality_report(quality_checker)
        
        # 8. æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰
        if enable_missing_value_handling:
            logger.info("\n" + "="*60)
            logger.info("ğŸ“ Phase 0-7: æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®è‡ªå‹•æ•´ç†")
            logger.info("="*60)
            summarize_processing_log()
        
        # 9. ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æå‰Šé™¤çµ±è¨ˆã®è¡¨ç¤º
        if enable_missing_value_handling:
            logger.info("\n" + "="*60)
            logger.info("ğŸ“Š Phase 0-8: ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æå‰Šé™¤çµ±è¨ˆ")
            logger.info("="*60)
            display_deletion_statistics()
        
        # 10. å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ Phase 0: ãƒ‡ãƒ¼ã‚¿æ•´å‚™ å®Œäº†")
        logger.info("="*60)
        
        total_time = time.time() - monitor.start_time
        logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†)")
        monitor.log_system_status("å…¨å‡¦ç†å®Œäº†")
        
        logger.info("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿:")
        if Path('export/dataset').exists():
            bias_files = list(Path('export/dataset').glob('*.csv'))
            logger.info(f"   ğŸ”— çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(bias_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        if enable_quality_check and Path('export/quality_reports').exists():
            logger.info("   ğŸ“ˆ å“è³ªãƒ¬ãƒãƒ¼ãƒˆ: export/quality_reports/")
        
        logger.info("\nğŸ“ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æ•´å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        logger.error("ğŸ”§ ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:", exc_info=True)
        return False

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(
        description='ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ï¼ˆè¨ˆç”»æ›¸Phase 0ï¼šãƒ‡ãƒ¼ã‚¿æ•´å‚™å¯¾å¿œç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ ä½¿ç”¨ä¾‹:
  python process_race_data.py                                    # åŸºæœ¬å‡¦ç†
  python process_race_data.py --turf-only                      # èŠã‚³ãƒ¼ã‚¹ã®ã¿ã§å‡¦ç†
  python process_race_data.py --no-missing-handling              # æ¬ æå€¤å‡¦ç†ã‚’ç„¡åŠ¹åŒ–
  python process_race_data.py --no-quality-check                 # å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–

ğŸ”§ ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å½¹å‰²:
  ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€è¤‡æ•°ã®å½¢å¼ã®ç”Ÿãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆBAC, SRB, SEDï¼‰ã‚’èª­ã¿è¾¼ã¿ã€
  ãã‚Œã‚‰ã‚’ä¸€ã¤ã®æ•´å½¢ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµ±åˆã—ã¾ã™ã€‚
  æœ€çµ‚çš„ãªæˆæœç‰©ã¯ `export/dataset/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‡ºåŠ›ã•ã‚Œã€
  ã“ã‚ŒãŒå¾Œç¶šã®åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆä¾‹: analyze_horse_REQI.pyï¼‰ã®å…¥åŠ›ã¨ãªã‚Šã¾ã™ã€‚

ğŸ”§ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®å“è³ªç®¡ç†:
  âœ… æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†
  âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒ¬ãƒãƒ¼ãƒˆ
  âœ… æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®è‡ªå‹•ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
  âœ… ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
  âœ… æ®µéšçš„å‡¦ç†ã¨ãƒ­ã‚°å‡ºåŠ›
  âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾©æ—§æ©Ÿèƒ½
        """
    )
    
    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    track_group = parser.add_mutually_exclusive_group()
    track_group.add_argument('--exclude-turf', '--èŠã‚³ãƒ¼ã‚¹é™¤å¤–', action='store_true', 
                           help='èŠã‚³ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ã™ã‚‹')
    track_group.add_argument('--turf-only', '--èŠã‚³ãƒ¼ã‚¹ã®ã¿', action='store_true', 
                           help='èŠã‚³ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‡¦ç†ã™ã‚‹')
    
    # æ©Ÿèƒ½ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--no-missing-handling', '--æ¬ æå€¤å‡¦ç†ç„¡åŠ¹', action='store_true',
                       help='æˆ¦ç•¥çš„æ¬ æå€¤å‡¦ç†ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹')
    
    parser.add_argument('--no-quality-check', '--å“è³ªãƒã‚§ãƒƒã‚¯ç„¡åŠ¹', action='store_true',
                       help='ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹')
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š')
    
    parser.add_argument('--log-file', help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ã¿ï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–
    log_file = args.log_file
    
    if log_file is None:
        # è‡ªå‹•ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚‚å«ã‚€ï¼‰
        log_dir = Path('export/logs')
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = f'export/logs/process_race_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    
    setup_logging(log_level=args.log_level, log_file=log_file)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¬ãƒ¼ã§ã®é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    main_logger = logging.getLogger(__name__)
    main_logger.info("ğŸš€ ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å®Ÿå‹™ãƒ¬ãƒ™ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    main_logger.info(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
    main_logger.info(f"ğŸ–¥ï¸ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {args.log_level}")
    if log_file:
        main_logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")

    try:
        success = process_race_data(
            exclude_turf=args.exclude_turf,
            turf_only=args.turf_only,
            enable_missing_value_handling=not args.no_missing_handling,
            enable_quality_check=not args.no_quality_check,
        )
    except Exception as e:
        main_logger.error(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
        main_logger.error("ğŸ”§ ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:", exc_info=True)
        success = False

    if success:
        main_logger.info("ğŸ‰ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        exit_code = 0
    else:
        main_logger.error("âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        exit_code = 1

    main_logger.info(f"ğŸ ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº† (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code})")
    exit(exit_code)