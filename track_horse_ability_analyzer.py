import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import japanize_matplotlib
from scipy import stats
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, classification_report
import warnings
warnings.filterwarnings('ignore')

class TrackHorseAbilityAnalyzer:
    """
    ç«¶é¦¬å ´ç‰¹å¾´Ã—é¦¬èƒ½åŠ›é©æ€§åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    1. ç«¶é¦¬å ´ã®ç‰©ç†çš„ç‰¹å¾´ã®æ•°å€¤åŒ–
    2. é¦¬ã®ç·åˆèƒ½åŠ›å€¤ã®ç®—å‡º
    3. ç«¶é¦¬å ´é©æ€§ã®å®šé‡åŒ–
    4. æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å‹ç‡äºˆæ¸¬
    """
    
    def __init__(self, data_folder="export/with_bias", output_folder="results/track_horse_ability_analysis"):
        """
        åˆæœŸåŒ–
        
        Args:
            data_folder (str): ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
            output_folder (str): çµæœå‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        """
        self.data_folder = data_folder
        self.output_folder = output_folder
        self.df = None
        self.track_features = None
        self.scaler = StandardScaler()
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        self._setup_japanese_font()
        
        # ç«¶é¦¬å ´ç‰¹å¾´ã®å®šç¾©
        self._define_track_characteristics()
        
    def _setup_japanese_font(self):
        """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆç¢ºå®Ÿç‰ˆï¼‰"""
        import matplotlib
        import matplotlib.font_manager as fm
        import platform
        
        try:
            print("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’é–‹å§‹ï¼ˆç¢ºå®Ÿç‰ˆï¼‰...")
            
            if platform.system() == 'Windows':
                # Windowsãƒ•ã‚©ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                windows_fonts_dir = r'C:\Windows\Fonts'
                
                # åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥æŒ‡å®š
                font_candidates = [
                    (os.path.join(windows_fonts_dir, 'YuGothM.ttc'), 'Yu Gothic Medium'),
                    (os.path.join(windows_fonts_dir, 'YuGothB.ttc'), 'Yu Gothic Bold'),
                    (os.path.join(windows_fonts_dir, 'msgothic.ttc'), 'MS Gothic'),
                    (os.path.join(windows_fonts_dir, 'meiryo.ttc'), 'Meiryo'),
                    (os.path.join(windows_fonts_dir, 'msmincho.ttc'), 'MS Mincho'),
                ]
                
                self.font_prop = None
                for font_path, font_name in font_candidates:
                    if os.path.exists(font_path):
                        try:
                            # FontPropertiesã‚’ç›´æ¥ä½œæˆ
                            self.font_prop = fm.FontProperties(fname=font_path)
                            
                            # matplotlibè¨­å®šã‚‚æ›´æ–°
                            matplotlib.rcParams['font.family'] = [font_name]
                            matplotlib.rcParams['axes.unicode_minus'] = False
                            
                            print(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šæˆåŠŸ: {font_path} ({font_name})")
                            break
                        except Exception as e:
                            print(f"ãƒ•ã‚©ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {font_path} - {e}")
                            continue
                
                if self.font_prop is None:
                    print("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    self.font_prop = fm.FontProperties()
            else:
                # Windowsä»¥å¤–ã®ç’°å¢ƒ
                self.font_prop = fm.FontProperties()
                
        except Exception as e:
            print(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            self.font_prop = fm.FontProperties()
            
        print("ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šå®Œäº†")
    
    def _define_track_characteristics(self):
        """
        ç«¶é¦¬å ´ã®ç‰©ç†çš„ç‰¹å¾´ã‚’å®šç¾©
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›æƒ…å ±:
        - ä¸­å±±ã€é˜ªç¥ï¼šå‚ãŒãã¤ã„
        - ä¸­äº¬ï¼šæ€¥ã‚«ãƒ¼ãƒ–ã‹ã¤ä¸‹ã‚Šå‚ï¼ˆå¤–ã‚’å›ã•ã‚Œã‚‹ã¨è†¨ã‚‰ã¿ã‚„ã™ãã‹ãªã‚Šãã¤ã„ï¼‰
        - æ±äº¬ã€é˜ªç¥ã€äº¬éƒ½ã§å®Ÿç¸¾ã‚ã‚Šã¯æœ­å¹Œã§ã‚‚å®Ÿç¸¾å‡ºã—ã‚„ã™ã„
        """
        self.track_characteristics = {
            'ä¸­å±±': {
                'slope_difficulty': 0.9,    # å‚ã®å³ã—ã• (0-1)
                'curve_tightness': 0.6,     # ã‚«ãƒ¼ãƒ–ã®æ€¥ã•
                'bias_impact': 0.7,         # ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿åº¦
                'stamina_demand': 0.8,      # ã‚¹ã‚¿ãƒŸãƒŠè¦æ±‚åº¦
                'speed_sustainability': 0.7, # ã‚¹ãƒ”ãƒ¼ãƒ‰æŒç¶šæ€§è¦æ±‚
                'outside_disadvantage': 0.6, # å¤–æ ä¸åˆ©åº¦
                'track_type': 'power',      # ãƒ‘ãƒ¯ãƒ¼å‹
                'similar_tracks': ['æœ­å¹Œ']   # é¡ä¼¼ã‚³ãƒ¼ã‚¹
            },
            'é˜ªç¥': {
                'slope_difficulty': 0.9,
                'curve_tightness': 0.7,
                'bias_impact': 0.8,
                'stamina_demand': 0.8,
                'speed_sustainability': 0.8,
                'outside_disadvantage': 0.7,
                'track_type': 'power',
                'similar_tracks': ['æœ­å¹Œ']
            },
            'ä¸­äº¬': {
                'slope_difficulty': 0.8,    # ä¸‹ã‚Šå‚
                'curve_tightness': 0.95,    # æ€¥ã‚«ãƒ¼ãƒ–ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
                'bias_impact': 0.9,         # å¤–ã«è†¨ã‚‰ã¿ã‚„ã™ã„
                'stamina_demand': 0.7,
                'speed_sustainability': 0.6,
                'outside_disadvantage': 0.9, # å¤–æ éå¸¸ã«ä¸åˆ©
                'track_type': 'technical',   # æŠ€è¡“å‹
                'similar_tracks': []
            },
            'æ±äº¬': {
                'slope_difficulty': 0.5,
                'curve_tightness': 0.4,
                'bias_impact': 0.6,
                'stamina_demand': 0.7,
                'speed_sustainability': 0.8,
                'outside_disadvantage': 0.5,
                'track_type': 'speed',      # ã‚¹ãƒ”ãƒ¼ãƒ‰å‹
                'similar_tracks': ['æœ­å¹Œ']
            },
            'äº¬éƒ½': {
                'slope_difficulty': 0.6,
                'curve_tightness': 0.5,
                'bias_impact': 0.7,
                'stamina_demand': 0.7,
                'speed_sustainability': 0.8,
                'outside_disadvantage': 0.6,
                'track_type': 'balanced',   # ãƒãƒ©ãƒ³ã‚¹å‹
                'similar_tracks': ['æœ­å¹Œ']
            },
            'æ–°æ½Ÿ': {
                'slope_difficulty': 0.3,
                'curve_tightness': 0.2,     # ç›´ç·šçš„
                'bias_impact': 0.4,
                'stamina_demand': 0.6,
                'speed_sustainability': 0.9,
                'outside_disadvantage': 0.2, # å¤–æ æœ‰åˆ©
                'track_type': 'speed',
                'similar_tracks': []
            },
            'ç¦å³¶': {
                'slope_difficulty': 0.4,
                'curve_tightness': 0.5,
                'bias_impact': 0.6,
                'stamina_demand': 0.6,
                'speed_sustainability': 0.7,
                'outside_disadvantage': 0.5,
                'track_type': 'balanced',
                'similar_tracks': []
            },
            'å‡½é¤¨': {
                'slope_difficulty': 0.3,
                'curve_tightness': 0.6,
                'bias_impact': 0.5,
                'stamina_demand': 0.5,
                'speed_sustainability': 0.7,
                'outside_disadvantage': 0.4,
                'track_type': 'speed',
                'similar_tracks': []
            },
            'å°å€‰': {
                'slope_difficulty': 0.4,
                'curve_tightness': 0.7,
                'bias_impact': 0.6,
                'stamina_demand': 0.6,
                'speed_sustainability': 0.7,
                'outside_disadvantage': 0.6,
                'track_type': 'balanced',
                'similar_tracks': []
            },
            'æœ­å¹Œ': {
                'slope_difficulty': 0.4,
                'curve_tightness': 0.5,
                'bias_impact': 0.5,
                'stamina_demand': 0.6,
                'speed_sustainability': 0.8,
                'outside_disadvantage': 0.4,
                'track_type': 'speed',
                'similar_tracks': ['æ±äº¬', 'é˜ªç¥', 'äº¬éƒ½'] # é¡ä¼¼å®Ÿç¸¾åæ˜ 
            }
        }
    
    def load_and_preprocess_data(self):
        """
        SEDãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œ
        """
        print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ã‚’é–‹å§‹...")
        
        # SEDãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        sed_files = glob.glob(os.path.join(self.data_folder, "SED*_formatted_with_bias.csv"))
        
        if not sed_files:
            print(f"ã‚¨ãƒ©ãƒ¼: {self.data_folder} ã«SEDãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return False
        
        print(f"è¦‹ã¤ã‹ã£ãŸSEDãƒ•ã‚¡ã‚¤ãƒ«: {len(sed_files)}å€‹")
        
        data_list = []
        for file_path in sed_files[:50]:  # å‡¦ç†æ™‚é–“çŸ­ç¸®ã®ãŸã‚æœ€åˆã®50ãƒ•ã‚¡ã‚¤ãƒ«
            try:
                for encoding in ['utf-8', 'shift-jis', 'cp932']:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding)
                        data_list.append(df)
                        break
                    except UnicodeDecodeError:
                        continue
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
        
        if not data_list:
            print("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return False
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        self.df = pd.concat(data_list, ignore_index=True)
        print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(self.df)}è¡Œ")
        
        # å‰å‡¦ç†å®Ÿè¡Œ
        return self._preprocess_data()
    
    def _preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        print("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
        required_columns = ['å ´å', 'å¹´', 'é¦¬ç•ª', 'ç€é †', 'IDM', 'ç´ ç‚¹']
        missing_columns = [col for col in required_columns if col not in self.df.columns]
        
        if missing_columns:
            print(f"ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_columns}")
            return False
        
        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        numeric_columns = ['å¹´', 'é¦¬ç•ª', 'ç€é †', 'IDM', 'ç´ ç‚¹', 'ãƒ†ãƒ³æŒ‡æ•°', 'ä¸ŠãŒã‚ŠæŒ‡æ•°', 'ãƒšãƒ¼ã‚¹æŒ‡æ•°', 'è·é›¢', 'é¦¬ä½“é‡']
        for col in numeric_columns:
            if col in self.df.columns:
                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
        
        # å‹åˆ©ãƒ•ãƒ©ã‚°ä½œæˆ
        self.df['å‹åˆ©'] = (self.df['ç€é †'] == 1).astype(int)
        
        # é¦¬ã®ç·åˆèƒ½åŠ›å€¤ç®—å‡º
        self._calculate_horse_ability_scores()
        
        # ç«¶é¦¬å ´ç‰¹å¾´é‡è¿½åŠ 
        self._add_track_features()
        
        # é©æ€§ã‚¹ã‚³ã‚¢ç®—å‡º
        self._calculate_track_aptitude()
        
        # ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿é™¤å»
        before_count = len(self.df)
        self.df = self.df.dropna(subset=['å¹´', 'é¦¬ç•ª', 'ç€é †', 'å ´å', 'ç·åˆèƒ½åŠ›å€¤'])
        after_count = len(self.df)
        
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: {before_count}è¡Œ â†’ {after_count}è¡Œ")
        print(f"åˆ†æå¯¾è±¡ç«¶é¦¬å ´: {list(self.df['å ´å'].unique())}")
        
        return True
    
    def _calculate_horse_ability_scores(self):
        """
        é¦¬ã®ç·åˆèƒ½åŠ›å€¤ã‚’ç®—å‡º
        è¤‡æ•°ã®æŒ‡æ•°ã‚’çµ±åˆã—ã¦é¦¬ã®ç·åˆåŠ›ã‚’æ•°å€¤åŒ–
        """
        print("é¦¬ã®ç·åˆèƒ½åŠ›å€¤ã‚’ç®—å‡ºä¸­...")
        
        # åŸºæœ¬èƒ½åŠ›å€¤ï¼ˆIDMã€ç´ ç‚¹ã‚’ãƒ™ãƒ¼ã‚¹ï¼‰
        self.df['åŸºæœ¬èƒ½åŠ›å€¤'] = (
            self.df['IDM'].fillna(self.df['IDM'].median()) * 0.4 +
            self.df['ç´ ç‚¹'].fillna(self.df['ç´ ç‚¹'].median()) * 0.6
        )
        
        # ã‚¹ãƒ”ãƒ¼ãƒ‰èƒ½åŠ›å€¤
        self.df['ã‚¹ãƒ”ãƒ¼ãƒ‰èƒ½åŠ›å€¤'] = (
            self.df['ãƒ†ãƒ³æŒ‡æ•°'].fillna(self.df['ãƒ†ãƒ³æŒ‡æ•°'].median()) * 0.6 +
            self.df['ä¸ŠãŒã‚ŠæŒ‡æ•°'].fillna(self.df['ä¸ŠãŒã‚ŠæŒ‡æ•°'].median()) * 0.4
        )
        
        # ã‚¹ã‚¿ãƒŸãƒŠèƒ½åŠ›å€¤ï¼ˆè·é›¢é©æ€§ã‚’è€ƒæ…®ï¼‰
        self.df['ã‚¹ã‚¿ãƒŸãƒŠèƒ½åŠ›å€¤'] = (
            self.df['åŸºæœ¬èƒ½åŠ›å€¤'] * 0.7 +
            (self.df['è·é›¢'] / 2000) * self.df['ãƒšãƒ¼ã‚¹æŒ‡æ•°'].fillna(self.df['ãƒšãƒ¼ã‚¹æŒ‡æ•°'].median()) * 0.3
        )
        
        # ç·åˆèƒ½åŠ›å€¤ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        self.df['ç·åˆèƒ½åŠ›å€¤'] = (
            self.df['åŸºæœ¬èƒ½åŠ›å€¤'] * 0.5 +
            self.df['ã‚¹ãƒ”ãƒ¼ãƒ‰èƒ½åŠ›å€¤'] * 0.3 +
            self.df['ã‚¹ã‚¿ãƒŸãƒŠèƒ½åŠ›å€¤'] * 0.2
        )
        
        print("èƒ½åŠ›å€¤ç®—å‡ºå®Œäº†")
    
    def _add_track_features(self):
        """ç«¶é¦¬å ´ã®ç‰¹å¾´é‡ã‚’ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ """
        print("ç«¶é¦¬å ´ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        
        # ç«¶é¦¬å ´ç‰¹å¾´é‡ã‚’è¿½åŠ 
        for feature in ['slope_difficulty', 'curve_tightness', 'bias_impact', 
                       'stamina_demand', 'speed_sustainability', 'outside_disadvantage']:
            self.df[f'track_{feature}'] = self.df['å ´å'].map(
                lambda x: self.track_characteristics.get(x, {}).get(feature, 0.5)
            )
        
        # ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        track_type_map = {'speed': 0, 'balanced': 1, 'power': 2, 'technical': 3}
        self.df['track_type_encoded'] = self.df['å ´å'].map(
            lambda x: track_type_map.get(
                self.track_characteristics.get(x, {}).get('track_type', 'balanced'), 1
            )
        )
        
        print("ç«¶é¦¬å ´ç‰¹å¾´é‡è¿½åŠ å®Œäº†")
    
    def _calculate_track_aptitude(self):
        """
        é¦¬ã®ç«¶é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º
        é¦¬ã®èƒ½åŠ›ç‰¹æ€§ã¨ç«¶é¦¬å ´è¦æ±‚èƒ½åŠ›ã®ãƒãƒƒãƒãƒ³ã‚°åº¦ã‚’æ•°å€¤åŒ–
        """
        print("ç«¶é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºä¸­...")
        
        # ã‚¹ãƒ”ãƒ¼ãƒ‰é©æ€§
        self.df['ã‚¹ãƒ”ãƒ¼ãƒ‰é©æ€§'] = (
            self.df['ã‚¹ãƒ”ãƒ¼ãƒ‰èƒ½åŠ›å€¤'] * (1 - self.df['track_slope_difficulty']) * 
            (1 - self.df['track_curve_tightness'])
        )
        
        # ãƒ‘ãƒ¯ãƒ¼é©æ€§
        self.df['ãƒ‘ãƒ¯ãƒ¼é©æ€§'] = (
            self.df['åŸºæœ¬èƒ½åŠ›å€¤'] * self.df['track_slope_difficulty'] * 
            self.df['track_stamina_demand']
        )
        
        # æŠ€è¡“é©æ€§
        self.df['æŠ€è¡“é©æ€§'] = (
            self.df['ç·åˆèƒ½åŠ›å€¤'] * self.df['track_curve_tightness'] * 
            (1 - self.df['track_bias_impact'])
        )
        
        # æ é †é©æ€§ï¼ˆé¦¬ç•ªã«ã‚ˆã‚‹å½±éŸ¿ï¼‰
        self.df['æ é †é©æ€§'] = (
            1 - (self.df['é¦¬ç•ª'] - 1) / 17 * self.df['track_outside_disadvantage']
        )
        
        # ç·åˆé©æ€§ã‚¹ã‚³ã‚¢
        self.df['ç·åˆé©æ€§ã‚¹ã‚³ã‚¢'] = (
            self.df['ã‚¹ãƒ”ãƒ¼ãƒ‰é©æ€§'] * 0.3 +
            self.df['ãƒ‘ãƒ¯ãƒ¼é©æ€§'] * 0.3 +
            self.df['æŠ€è¡“é©æ€§'] * 0.2 +
            self.df['æ é †é©æ€§'] * 0.2
        )
        
        # é¡ä¼¼ç«¶é¦¬å ´ã§ã®å®Ÿç¸¾ã‚’è€ƒæ…®
        self._add_similar_track_performance()
        
        print("é©æ€§ã‚¹ã‚³ã‚¢ç®—å‡ºå®Œäº†")
    
    def _add_similar_track_performance(self):
        """é¡ä¼¼ç«¶é¦¬å ´ã§ã®å®Ÿç¸¾ã‚’é©æ€§ã‚¹ã‚³ã‚¢ã«åæ˜ """
        print("é¡ä¼¼ç«¶é¦¬å ´å®Ÿç¸¾ã‚’åæ˜ ä¸­...")
        
        # é¦¬ã”ã¨ã®ç«¶é¦¬å ´åˆ¥å®Ÿç¸¾ã‚’é›†è¨ˆ
        horse_track_performance = self.df.groupby(['è¡€çµ±ç™»éŒ²ç•ªå·', 'å ´å']).agg({
            'å‹åˆ©': 'mean',
            'ç€é †': 'mean',
            'ç·åˆèƒ½åŠ›å€¤': 'mean'
        }).reset_index()
        
        # é¡ä¼¼ç«¶é¦¬å ´ã§ã®å®Ÿç¸¾ã‚’åŠ å‘³
        for track, features in self.track_characteristics.items():
            similar_tracks = features.get('similar_tracks', [])
            if similar_tracks:
                track_mask = self.df['å ´å'] == track
                
                for _, row in self.df[track_mask].iterrows():
                    horse_id = row['è¡€çµ±ç™»éŒ²ç•ªå·']
                    
                    # é¡ä¼¼ç«¶é¦¬å ´ã§ã®å¹³å‡å‹ç‡ã‚’å–å¾—
                    similar_performance = horse_track_performance[
                        (horse_track_performance['è¡€çµ±ç™»éŒ²ç•ªå·'] == horse_id) &
                        (horse_track_performance['å ´å'].isin(similar_tracks))
                    ]['å‹åˆ©'].mean()
                    
                    if not pd.isna(similar_performance):
                        # é©æ€§ã‚¹ã‚³ã‚¢ã«é¡ä¼¼ç«¶é¦¬å ´å®Ÿç¸¾ã‚’åæ˜ 
                        adjustment_factor = 1 + (similar_performance - 0.1) * 0.3
                        self.df.loc[row.name, 'ç·åˆé©æ€§ã‚¹ã‚³ã‚¢'] *= adjustment_factor
        
        print("é¡ä¼¼ç«¶é¦¬å ´å®Ÿç¸¾åæ˜ å®Œäº†")
    
    def analyze_track_aptitude_correlation(self):
        """
        ç«¶é¦¬å ´ã”ã¨ã®å‹ç‡ã¨é¦¬ã®é©æ€§ã«ã¤ã„ã¦ç›¸é–¢åˆ†æ
        ç›¸é–¢ä¿‚æ•°ã€æ•£å¸ƒå›³ã€på€¤ã€å›å¸°ç›´ç·šã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’å®Ÿæ–½
        """
        print("\n=== ç«¶é¦¬å ´åˆ¥é©æ€§ç›¸é–¢åˆ†æé–‹å§‹ ===")
        
        if not os.path.exists(self.output_folder):
            os.makedirs(self.output_folder)
        
        # 1. ç«¶é¦¬å ´åˆ¥çµ±è¨ˆåˆ†æ
        track_correlation_stats = self._calculate_track_correlation_statistics()
        
        # 2. æ•£å¸ƒå›³ã¨å›å¸°åˆ†æã®å¯è¦–åŒ–
        self._create_aptitude_correlation_visualizations(track_correlation_stats)
        
        # 3. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æ
        logistic_results = self._perform_logistic_regression_analysis()
        
        # 4. è©³ç´°çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_aptitude_correlation_report(track_correlation_stats, logistic_results)
        
        return {
            'correlation_stats': track_correlation_stats,
            'logistic_results': logistic_results
        }
    
    def _calculate_track_correlation_statistics(self):
        """ç«¶é¦¬å ´åˆ¥ã®é©æ€§ã¨å‹ç‡ã®ç›¸é–¢çµ±è¨ˆã‚’è¨ˆç®—"""
        print("ç«¶é¦¬å ´åˆ¥ç›¸é–¢çµ±è¨ˆè¨ˆç®—ä¸­...")
        
        track_stats = {}
        
        for track in self.df['å ´å'].unique():
            track_data = self.df[self.df['å ´å'] == track].copy()
            
            if len(track_data) < 50:  # æœ€ä½ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
                print(f"è­¦å‘Š: {track}ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°‘ãªã„({len(track_data)})")
                continue
            
            # é©æ€§ã‚¹ã‚³ã‚¢ã¨å‹ç‡ã®ãƒ‡ãƒ¼ã‚¿
            aptitude_scores = track_data['ç·åˆé©æ€§ã‚¹ã‚³ã‚¢'].values
            win_flags = track_data['å‹åˆ©'].values
            win_rates = track_data.groupby('ç·åˆé©æ€§ã‚¹ã‚³ã‚¢')['å‹åˆ©'].transform('mean').values
            
            # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã¨på€¤
            correlation, p_value = stats.pearsonr(aptitude_scores, win_flags)
            
            # ã‚¹ãƒ”ã‚¢ãƒãƒ³ã®é †ä½ç›¸é–¢ä¿‚æ•°
            spearman_corr, spearman_p = stats.spearmanr(aptitude_scores, win_flags)
            
            # ç·šå½¢å›å¸°
            X = aptitude_scores.reshape(-1, 1)
            y = win_flags
            
            reg_model = LinearRegression()
            reg_model.fit(X, y)
            
            y_pred = reg_model.predict(X)
            r2 = r2_score(y, y_pred)
            
            # å›å¸°ç›´ç·šã®ä¿‚æ•°
            slope = reg_model.coef_[0]
            intercept = reg_model.intercept_
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§åˆ¤å®š
            is_significant = p_value < 0.05
            significance_level = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "n.s."
            
            # é©æ€§ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒçµ±è¨ˆ
            aptitude_stats = {
                'mean': aptitude_scores.mean(),
                'std': aptitude_scores.std(),
                'min': aptitude_scores.min(),
                'max': aptitude_scores.max(),
                'q25': np.percentile(aptitude_scores, 25),
                'median': np.median(aptitude_scores),
                'q75': np.percentile(aptitude_scores, 75)
            }
            
            track_stats[track] = {
                'sample_size': len(track_data),
                'correlation': correlation,
                'p_value': p_value,
                'spearman_correlation': spearman_corr,
                'spearman_p_value': spearman_p,
                'r_squared': r2,
                'slope': slope,
                'intercept': intercept,
                'win_rate_mean': win_flags.mean(),
                'win_rate_std': win_flags.std(),
                'is_significant': is_significant,
                'significance_level': significance_level,
                'aptitude_data': aptitude_scores,
                'win_data': win_flags,
                'aptitude_stats': aptitude_stats
            }
        
        return track_stats
    
    def _create_aptitude_correlation_visualizations(self, track_stats):
        """é©æ€§ç›¸é–¢é–¢ä¿‚ã®å¯è¦–åŒ–"""
        print("é©æ€§ç›¸é–¢é–¢ä¿‚å¯è¦–åŒ–ä½œæˆä¸­...")
        
        # ç«¶é¦¬å ´æ•°ã«å¿œã˜ã¦ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆé…ç½®ã‚’æ±ºå®š
        n_tracks = len(track_stats)
        n_cols = 3
        n_rows = (n_tracks + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))
        fig.suptitle('ç«¶é¦¬å ´åˆ¥ é¦¬é©æ€§Ã—å‹ç‡ ç›¸é–¢åˆ†æ', fontproperties=self.font_prop, fontsize=16)
        
        # axesãŒ1æ¬¡å…ƒã®å ´åˆã«å¯¾å¿œ
        if n_tracks == 1:
            axes = axes.reshape(1, -1)
        elif n_rows == 1:
            axes = axes.reshape(1, -1)
        elif n_cols == 1:
            axes = axes.reshape(-1, 1)
        
        track_names = list(track_stats.keys())
        
        for i, track in enumerate(track_names):
            row = i // n_cols
            col = i % n_cols
            ax = axes[row, col] if n_rows > 1 else axes[col]
            
            stats_data = track_stats[track]
            
            # æ•£å¸ƒå›³ï¼ˆé©æ€§ã‚¹ã‚³ã‚¢ã§ãƒ“ãƒ‹ãƒ³ã‚°ï¼‰
            aptitude_bins = np.linspace(stats_data['aptitude_stats']['min'], 
                                      stats_data['aptitude_stats']['max'], 20)
            bin_indices = np.digitize(stats_data['aptitude_data'], aptitude_bins)
            
            # ãƒ“ãƒ³ã”ã¨ã®å‹ç‡ã‚’è¨ˆç®—
            bin_win_rates = []
            bin_centers = []
            for bin_idx in range(1, len(aptitude_bins)):
                mask = bin_indices == bin_idx
                if np.sum(mask) > 0:
                    bin_win_rates.append(stats_data['win_data'][mask].mean())
                    bin_centers.append((aptitude_bins[bin_idx-1] + aptitude_bins[bin_idx]) / 2)
            
            # æ•£å¸ƒå›³
            ax.scatter(stats_data['aptitude_data'], stats_data['win_data'], 
                      alpha=0.3, s=20, color='lightblue', label='å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿')
            
            # ãƒ“ãƒ³é›†è¨ˆã®å‹ç‡
            if bin_centers:
                ax.scatter(bin_centers, bin_win_rates, 
                          s=100, color='darkblue', marker='o', label='ãƒ“ãƒ³å¹³å‡å‹ç‡')
            
            # å›å¸°ç›´ç·š
            x_line = np.linspace(stats_data['aptitude_stats']['min'], 
                               stats_data['aptitude_stats']['max'], 100)
            y_line = stats_data['slope'] * x_line + stats_data['intercept']
            ax.plot(x_line, y_line, color='red', linewidth=2, 
                   label=f'å›å¸°ç›´ç·š (RÂ²={stats_data["r_squared"]:.3f})')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨çµ±è¨ˆæƒ…å ±
            title = (f'{track}\n'
                    f'ç›¸é–¢ä¿‚æ•°: {stats_data["correlation"]:.3f}{stats_data["significance_level"]} '
                    f'(p={stats_data["p_value"] if stats_data["p_value"] > 0 else 0:.4f})\n'
                    f'ã‚¹ãƒ”ã‚¢ãƒãƒ³: {stats_data["spearman_correlation"]:.3f}')
            ax.set_title(title, fontproperties=self.font_prop, fontsize=10)
            
            ax.set_xlabel('ç·åˆé©æ€§ã‚¹ã‚³ã‚¢', fontproperties=self.font_prop, fontsize=9)
            ax.set_ylabel('å‹åˆ©ãƒ•ãƒ©ã‚°', fontproperties=self.font_prop, fontsize=9)
            ax.legend(prop=self.font_prop, fontsize=8)
            ax.grid(True, alpha=0.3)
            ax.set_ylim(-0.1, 1.1)
        
        # ç©ºã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
        for i in range(len(track_names), n_rows * n_cols):
            row = i // n_cols
            col = i % n_cols
            ax = axes[row, col] if n_rows > 1 else axes[col]
            ax.set_visible(False)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_folder, 'ç«¶é¦¬å ´åˆ¥é©æ€§ç›¸é–¢åˆ†æ.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç›¸é–¢ä¿‚æ•°æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ
        self._create_aptitude_correlation_comparison_chart(track_stats)
    
    def _create_aptitude_correlation_comparison_chart(self, track_stats):
        """é©æ€§ç›¸é–¢ä¿‚æ•°æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ"""
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        tracks = list(track_stats.keys())
        correlations = [track_stats[track]['correlation'] for track in tracks]
        spearman_correlations = [track_stats[track]['spearman_correlation'] for track in tracks]
        p_values = [track_stats[track]['p_value'] for track in tracks]
        r_squared = [track_stats[track]['r_squared'] for track in tracks]
        
        # æœ‰æ„æ€§ã«ã‚ˆã‚‹è‰²åˆ†ã‘
        colors = ['red' if p < 0.001 else 'orange' if p < 0.01 else 'yellow' if p < 0.05 else 'gray' 
                 for p in p_values]
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ç«¶é¦¬å ´åˆ¥é©æ€§ç›¸é–¢åˆ†æ ç·åˆæ¯”è¼ƒ', fontproperties=self.font_prop, fontsize=16)
        
        # 1. ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°æ¯”è¼ƒ
        bars1 = ax1.barh(tracks, correlations, color=colors, alpha=0.7)
        ax1.set_xlabel('ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°', fontproperties=self.font_prop)
        ax1.set_title('ç«¶é¦¬å ´åˆ¥é©æ€§ç›¸é–¢ä¿‚æ•°ï¼ˆãƒ”ã‚¢ã‚½ãƒ³ï¼‰', fontproperties=self.font_prop, fontsize=12)
        ax1.axvline(0, color='black', linestyle='-', alpha=0.3)
        
        for i, (bar, corr) in enumerate(zip(bars1, correlations)):
            ax1.text(corr + 0.001, bar.get_y() + bar.get_height()/2, 
                    f'{corr:.3f}', va='center', fontproperties=self.font_prop, fontsize=9)
        
        for label in ax1.get_yticklabels():
            label.set_fontproperties(self.font_prop)
        
        # 2. ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ä¿‚æ•°æ¯”è¼ƒ
        bars2 = ax2.barh(tracks, spearman_correlations, color=colors, alpha=0.7)
        ax2.set_xlabel('ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ä¿‚æ•°', fontproperties=self.font_prop)
        ax2.set_title('ç«¶é¦¬å ´åˆ¥é©æ€§ç›¸é–¢ä¿‚æ•°ï¼ˆã‚¹ãƒ”ã‚¢ãƒãƒ³ï¼‰', fontproperties=self.font_prop, fontsize=12)
        ax2.axvline(0, color='black', linestyle='-', alpha=0.3)
        
        for i, (bar, corr) in enumerate(zip(bars2, spearman_correlations)):
            ax2.text(corr + 0.001, bar.get_y() + bar.get_height()/2, 
                    f'{corr:.3f}', va='center', fontproperties=self.font_prop, fontsize=9)
        
        for label in ax2.get_yticklabels():
            label.set_fontproperties(self.font_prop)
        
        # 3. på€¤æ¯”è¼ƒï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        ax3.barh(tracks, [-np.log10(p) if p > 0 else 10 for p in p_values], color=colors, alpha=0.7)
        ax3.set_xlabel('-log10(på€¤)', fontproperties=self.font_prop)
        ax3.set_title('çµ±è¨ˆçš„æœ‰æ„æ€§ (-log10(på€¤))', fontproperties=self.font_prop, fontsize=12)
        ax3.axvline(-np.log10(0.05), color='red', linestyle='--', alpha=0.7, label='p=0.05')
        ax3.axvline(-np.log10(0.01), color='orange', linestyle='--', alpha=0.7, label='p=0.01')
        ax3.axvline(-np.log10(0.001), color='darkred', linestyle='--', alpha=0.7, label='p=0.001')
        ax3.legend(prop=self.font_prop)
        
        for label in ax3.get_yticklabels():
            label.set_fontproperties(self.font_prop)
        
        # 4. RÂ²å€¤æ¯”è¼ƒ
        bars4 = ax4.barh(tracks, r_squared, color=colors, alpha=0.7)
        ax4.set_xlabel('æ±ºå®šä¿‚æ•° (RÂ²)', fontproperties=self.font_prop)
        ax4.set_title('å›å¸°ãƒ¢ãƒ‡ãƒ«èª¬æ˜åŠ› (RÂ²å€¤)', fontproperties=self.font_prop, fontsize=12)
        
        for i, (bar, r2) in enumerate(zip(bars4, r_squared)):
            ax4.text(r2 + 0.001, bar.get_y() + bar.get_height()/2, 
                    f'{r2:.3f}', va='center', fontproperties=self.font_prop, fontsize=9)
        
        for label in ax4.get_yticklabels():
            label.set_fontproperties(self.font_prop)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_folder, 'é©æ€§ç›¸é–¢ä¿‚æ•°æ¯”è¼ƒ.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _perform_logistic_regression_analysis(self):
        """ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æã®å®Ÿæ–½"""
        print("ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æå®Ÿè¡Œä¸­...")
        
        logistic_results = {}
        
        for track in self.df['å ´å'].unique():
            track_data = self.df[self.df['å ´å'] == track].copy()
            
            if len(track_data) < 50:
                continue
            
            # ç‰¹å¾´é‡æº–å‚™
            feature_cols = ['ç·åˆé©æ€§ã‚¹ã‚³ã‚¢', 'ã‚¹ãƒ”ãƒ¼ãƒ‰é©æ€§', 'ãƒ‘ãƒ¯ãƒ¼é©æ€§', 'æŠ€è¡“é©æ€§', 'æ é †é©æ€§']
            X = track_data[feature_cols].fillna(track_data[feature_cols].median())
            y = track_data['å‹åˆ©']
            
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y
            )
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
            log_reg = LogisticRegression(random_state=42, max_iter=1000)
            log_reg.fit(X_train_scaled, y_train)
            
            # äºˆæ¸¬ã¨è©•ä¾¡
            y_pred = log_reg.predict(X_test_scaled)
            y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]
            
            # è©•ä¾¡æŒ‡æ¨™
            accuracy = log_reg.score(X_test_scaled, y_test)
            
            # ä¿‚æ•°
            coefficients = dict(zip(feature_cols, log_reg.coef_[0]))
            
            logistic_results[track] = {
                'accuracy': accuracy,
                'coefficients': coefficients,
                'intercept': log_reg.intercept_[0],
                'sample_size': len(track_data),
                'predictions': y_pred,
                'probabilities': y_pred_proba,
                'actual': y_test,
                'feature_importance': abs(log_reg.coef_[0])
            }
        
        # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°çµæœã®å¯è¦–åŒ–
        self._visualize_logistic_regression_results(logistic_results)
        
        return logistic_results
    
    def _visualize_logistic_regression_results(self, logistic_results):
        """ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°çµæœã®å¯è¦–åŒ–"""
        print("ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°çµæœå¯è¦–åŒ–ä¸­...")
        
        # ä¿‚æ•°æ¯”è¼ƒã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        tracks = list(logistic_results.keys())
        feature_names = ['ç·åˆé©æ€§ã‚¹ã‚³ã‚¢', 'ã‚¹ãƒ”ãƒ¼ãƒ‰é©æ€§', 'ãƒ‘ãƒ¯ãƒ¼é©æ€§', 'æŠ€è¡“é©æ€§', 'æ é †é©æ€§']
        
        coef_matrix = []
        for track in tracks:
            coef_matrix.append([logistic_results[track]['coefficients'][feat] for feat in feature_names])
        
        coef_df = pd.DataFrame(coef_matrix, index=tracks, columns=feature_names)
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(coef_df, annot=True, cmap='RdBu_r', center=0, fmt='.3f')
        plt.title('ç«¶é¦¬å ´åˆ¥ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ä¿‚æ•°', fontproperties=self.font_prop, fontsize=14)
        plt.xlabel('ç‰¹å¾´é‡', fontproperties=self.font_prop, fontsize=12)
        plt.ylabel('ç«¶é¦¬å ´', fontproperties=self.font_prop, fontsize=12)
        
        # è»¸ãƒ©ãƒ™ãƒ«ã«ãƒ•ã‚©ãƒ³ãƒˆé©ç”¨
        ax = plt.gca()
        for label in ax.get_xticklabels():
            label.set_fontproperties(self.font_prop)
        for label in ax.get_yticklabels():
            label.set_fontproperties(self.font_prop)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_folder, 'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ä¿‚æ•°.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç²¾åº¦æ¯”è¼ƒ
        accuracies = [logistic_results[track]['accuracy'] for track in tracks]
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(tracks, accuracies, alpha=0.7, color='steelblue')
        plt.title('ç«¶é¦¬å ´åˆ¥ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ç²¾åº¦', fontproperties=self.font_prop, fontsize=14)
        plt.xlabel('ç«¶é¦¬å ´', fontproperties=self.font_prop, fontsize=12)
        plt.ylabel('ç²¾åº¦', fontproperties=self.font_prop, fontsize=12)
        plt.xticks(rotation=45)
        
        # ç²¾åº¦ã®å€¤ã‚’è¡¨ç¤º
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{acc:.3f}', ha='center', va='bottom', fontproperties=self.font_prop, fontsize=9)
        
        # xè»¸ãƒ©ãƒ™ãƒ«ã«ãƒ•ã‚©ãƒ³ãƒˆé©ç”¨
        ax = plt.gca()
        for label in ax.get_xticklabels():
            label.set_fontproperties(self.font_prop)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_folder, 'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ç²¾åº¦.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_aptitude_correlation_report(self, track_stats, logistic_results):
        """é©æ€§ç›¸é–¢åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report_path = os.path.join(self.output_folder, 'ç«¶é¦¬å ´åˆ¥é©æ€§ç›¸é–¢åˆ†æãƒ¬ãƒãƒ¼ãƒˆ.md')
        
        with open(report_path, 'w', encoding='utf-8-sig') as f:
            f.write("# ç«¶é¦¬å ´åˆ¥ é¦¬é©æ€§Ã—å‹ç‡ ç›¸é–¢åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ¯ åˆ†æç›®çš„\n\n")
            f.write("å„ç«¶é¦¬å ´ã«ãŠã„ã¦ã€é¦¬ã®ç·åˆé©æ€§ã‚¹ã‚³ã‚¢ã¨å‹ç‡ã®é–“ã®ç›¸é–¢é–¢ä¿‚ã‚’\n")
            f.write("å¤šè§’çš„ã«åˆ†æã—ã€ç«¶é¦¬å ´ã”ã¨ã®ç‰¹æ€§ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸã€‚\n\n")
            
            f.write("## ğŸ“Š ç›¸é–¢åˆ†æçµæœ\n\n")
            f.write("| ç«¶é¦¬å ´ | ã‚µãƒ³ãƒ—ãƒ«æ•° | ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ | ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ | på€¤ | RÂ² | æœ‰æ„æ€§ |\n")
            f.write("|--------|------------|--------------|----------------|-----|----|---------|\n")
            
            # ç›¸é–¢ä¿‚æ•°ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_tracks = sorted(track_stats.items(), 
                                 key=lambda x: x[1]['correlation'], reverse=True)
            
            for track, stats in sorted_tracks:
                f.write(f"| {track} | {stats['sample_size']} | "
                       f"{stats['correlation']:.3f} | {stats['spearman_correlation']:.3f} | "
                       f"{stats['p_value']:.4f} | {stats['r_squared']:.3f} | "
                       f"{stats['significance_level']} |\n")
            
            f.write("\n**æœ‰æ„æ€§ã®è¨˜å·**\n")
            f.write("- *** : p < 0.001 (æ¥µã‚ã¦é«˜ã„æœ‰æ„æ€§)\n")
            f.write("- ** : p < 0.01 (é«˜ã„æœ‰æ„æ€§)\n") 
            f.write("- * : p < 0.05 (æœ‰æ„)\n")
            f.write("- n.s. : p â‰¥ 0.05 (æœ‰æ„å·®ãªã—)\n\n")
            
            f.write("## ğŸ¤– ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æçµæœ\n\n")
            f.write("| ç«¶é¦¬å ´ | ç²¾åº¦ | ç·åˆé©æ€§ä¿‚æ•° | ã‚¹ãƒ”ãƒ¼ãƒ‰ä¿‚æ•° | ãƒ‘ãƒ¯ãƒ¼ä¿‚æ•° | æŠ€è¡“ä¿‚æ•° | æ é †ä¿‚æ•° |\n")
            f.write("|--------|------|--------------|--------------|------------|----------|----------|\n")
            
            for track, results in logistic_results.items():
                coef = results['coefficients']
                f.write(f"| {track} | {results['accuracy']:.3f} | "
                       f"{coef['ç·åˆé©æ€§ã‚¹ã‚³ã‚¢']:.3f} | {coef['ã‚¹ãƒ”ãƒ¼ãƒ‰é©æ€§']:.3f} | "
                       f"{coef['ãƒ‘ãƒ¯ãƒ¼é©æ€§']:.3f} | {coef['æŠ€è¡“é©æ€§']:.3f} | "
                       f"{coef['æ é †é©æ€§']:.3f} |\n")
            
            f.write("\n## ğŸ† ä¸»è¦ãªç™ºè¦‹\n\n")
            
            # æœ€é«˜ãƒ»æœ€ä½ç›¸é–¢
            highest_track, highest_stats = max(track_stats.items(), 
                                             key=lambda x: x[1]['correlation'])
            lowest_track, lowest_stats = min(track_stats.items(), 
                                           key=lambda x: x[1]['correlation'])
            
            f.write(f"### é©æ€§ã¨å‹ç‡ã®ç›¸é–¢ãŒæœ€ã‚‚å¼·ã„ç«¶é¦¬å ´\n")
            f.write(f"- **{highest_track}**: ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•° {highest_stats['correlation']:.3f}\n")
            f.write(f"  - ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ä¿‚æ•°: {highest_stats['spearman_correlation']:.3f}\n")
            f.write(f"  - på€¤: {highest_stats['p_value']:.4f}\n")
            f.write(f"  - RÂ²: {highest_stats['r_squared']:.3f}\n\n")
            
            f.write(f"### é©æ€§ã¨å‹ç‡ã®ç›¸é–¢ãŒæœ€ã‚‚å¼±ã„ç«¶é¦¬å ´\n")
            f.write(f"- **{lowest_track}**: ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•° {lowest_stats['correlation']:.3f}\n")
            f.write(f"  - ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ä¿‚æ•°: {lowest_stats['spearman_correlation']:.3f}\n")
            f.write(f"  - på€¤: {lowest_stats['p_value']:.4f}\n")
            f.write(f"  - RÂ²: {lowest_stats['r_squared']:.3f}\n\n")
            
            # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®æœ€é«˜ç²¾åº¦
            best_logistic = max(logistic_results.items(), key=lambda x: x[1]['accuracy'])
            f.write(f"### ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§æœ€ã‚‚äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„ç«¶é¦¬å ´\n")
            f.write(f"- **{best_logistic[0]}**: ç²¾åº¦ {best_logistic[1]['accuracy']:.3f}\n\n")
            
            # æœ‰æ„æ€§ã«ã‚ˆã‚‹åˆ†é¡
            significant_tracks = [track for track, stats in track_stats.items() 
                                if stats['is_significant']]
            non_significant_tracks = [track for track, stats in track_stats.items() 
                                    if not stats['is_significant']]
            
            f.write(f"### çµ±è¨ˆçš„æœ‰æ„æ€§ã®çµæœ\n")
            f.write(f"- **æœ‰æ„ãªç›¸é–¢ãŒã‚ã‚‹ç«¶é¦¬å ´** ({len(significant_tracks)}å ´): {', '.join(significant_tracks)}\n")
            f.write(f"- **æœ‰æ„ãªç›¸é–¢ãŒãªã„ç«¶é¦¬å ´** ({len(non_significant_tracks)}å ´): {', '.join(non_significant_tracks) if non_significant_tracks else 'ãªã—'}\n\n")
            
            f.write("## ğŸ’¡ å®Ÿç”¨çš„è§£é‡ˆ\n\n")
            
            f.write("### é©æ€§ã‚¹ã‚³ã‚¢ã®æœ‰åŠ¹æ€§\n")
            avg_correlation = np.mean([stats['correlation'] for stats in track_stats.values()])
            f.write(f"- **å…¨ç«¶é¦¬å ´å¹³å‡ç›¸é–¢ä¿‚æ•°**: {avg_correlation:.3f}\n")
            
            if avg_correlation > 0.3:
                f.write("- é¦¬ã®é©æ€§ã‚¹ã‚³ã‚¢ã¯å‹ç‡äºˆæ¸¬ã«**éå¸¸ã«æœ‰åŠ¹**\n")
            elif avg_correlation > 0.2:
                f.write("- é¦¬ã®é©æ€§ã‚¹ã‚³ã‚¢ã¯å‹ç‡äºˆæ¸¬ã«**æœ‰åŠ¹**\n")
            elif avg_correlation > 0.1:
                f.write("- é¦¬ã®é©æ€§ã‚¹ã‚³ã‚¢ã¯å‹ç‡äºˆæ¸¬ã«**ã‚„ã‚„æœ‰åŠ¹**\n")
            else:
                f.write("- é¦¬ã®é©æ€§ã‚¹ã‚³ã‚¢ã¯å‹ç‡äºˆæ¸¬ã¸ã®å¯„ä¸ãŒ**é™å®šçš„**\n")
            
            f.write("\n### ç«¶é¦¬å ´åˆ¥ã®äºˆæ¸¬æˆ¦ç•¥\n")
            
            # ç›¸é–¢ã®å¼·ã„ç«¶é¦¬å ´
            strong_corr_tracks = [track for track, stats in track_stats.items() 
                                if stats['correlation'] > avg_correlation + 0.1]
            weak_corr_tracks = [track for track, stats in track_stats.items() 
                              if stats['correlation'] < avg_correlation - 0.1]
            
            f.write(f"- **é©æ€§é‡è¦–å‹ç«¶é¦¬å ´** (ç›¸é–¢ãŒå¼·ã„): {', '.join(strong_corr_tracks)}\n")
            f.write(f"  â†’ é¦¬ã®é©æ€§ã‚¹ã‚³ã‚¢ã‚’é‡è¦–ã—ãŸäºˆæƒ³ãŒæœ‰åŠ¹\n")
            f.write(f"- **å¤šè¦å› å‹ç«¶é¦¬å ´** (ç›¸é–¢ãŒå¼±ã„): {', '.join(weak_corr_tracks)}\n")
            f.write(f"  â†’ é©æ€§ä»¥å¤–ã®è¦å› ï¼ˆå±•é–‹ã€é¦¬å ´çŠ¶æ…‹ç­‰ï¼‰ã‚‚è€ƒæ…®ãŒå¿…è¦\n\n")
            
            f.write("### å®Ÿè·µçš„ã‚¢ãƒ‰ãƒã‚¤ã‚¹\n")
            f.write("1. **é«˜ç›¸é–¢ç«¶é¦¬å ´**ã§ã¯é©æ€§ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹äºˆæƒ³ã®ä¿¡é ¼æ€§ãŒé«˜ã„\n")
            f.write("2. **ä½ç›¸é–¢ç«¶é¦¬å ´**ã§ã¯ä»–ã®è¦å› ã¨ã®è¤‡åˆçš„ãªåˆ†æãŒå¿…è¦\n")
            f.write("3. **ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ä¿‚æ•°**ã‹ã‚‰å„é©æ€§è¦ç´ ã®é‡è¦åº¦ã‚’åˆ¤æ–­\n")
            f.write("4. **ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢**ãŒé«˜ã„å ´åˆã¯é †ä½é–¢ä¿‚ã®äºˆæ¸¬ã«æœ‰åŠ¹\n\n")
        
        print(f"é©æ€§ç›¸é–¢åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    
    def analyze_track_horse_compatibility(self):
        """
        ç«¶é¦¬å ´ã¨é¦¬ã®ç›¸æ€§ã‚’è©³ç´°åˆ†æ
        """
        print("ç«¶é¦¬å ´Ã—é¦¬ç›¸æ€§åˆ†æã‚’å®Ÿè¡Œä¸­...")
        
        # ç«¶é¦¬å ´åˆ¥ã®å¹³å‡èƒ½åŠ›å€¤ã¨å‹ç‡
        track_analysis = self.df.groupby('å ´å').agg({
            'ç·åˆèƒ½åŠ›å€¤': ['mean', 'std'],
            'ç·åˆé©æ€§ã‚¹ã‚³ã‚¢': ['mean', 'std'],
            'å‹åˆ©': 'mean',
            'ç€é †': 'mean'
        }).round(4)
        
        # èƒ½åŠ›å€¤ãƒ¬ãƒ³ã‚¸åˆ¥ã®å‹ç‡åˆ†æ
        self.df['èƒ½åŠ›å€¤ãƒ©ãƒ³ã‚¯'] = pd.qcut(self.df['ç·åˆèƒ½åŠ›å€¤'], q=5, labels=['D', 'C', 'B', 'A', 'S'])
        
        ability_track_analysis = self.df.groupby(['å ´å', 'èƒ½åŠ›å€¤ãƒ©ãƒ³ã‚¯']).agg({
            'å‹åˆ©': 'mean',
            'ç€é †': 'mean',
            'ç·åˆé©æ€§ã‚¹ã‚³ã‚¢': 'mean'
        }).reset_index()
        
        # å¯è¦–åŒ–
        self._create_compatibility_visualizations(track_analysis, ability_track_analysis)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_compatibility_report(track_analysis, ability_track_analysis)
        
        print("ç«¶é¦¬å ´Ã—é¦¬ç›¸æ€§åˆ†æå®Œäº†")
    
    def _create_compatibility_visualizations(self, track_analysis, ability_analysis):
        """ç›¸æ€§åˆ†æã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ç«¶é¦¬å ´Ã—é¦¬èƒ½åŠ›é©æ€§åˆ†æ', fontproperties=self.font_prop, fontsize=16)
        
        # 1. ç«¶é¦¬å ´åˆ¥å¹³å‡é©æ€§ã‚¹ã‚³ã‚¢
        ax1 = axes[0, 0]
        tracks = track_analysis.index
        scores = track_analysis[('ç·åˆé©æ€§ã‚¹ã‚³ã‚¢', 'mean')]
        bars = ax1.bar(tracks, scores, alpha=0.7)
        ax1.set_title('ç«¶é¦¬å ´åˆ¥å¹³å‡é©æ€§ã‚¹ã‚³ã‚¢', fontproperties=self.font_prop, fontsize=12)
        ax1.set_ylabel('é©æ€§ã‚¹ã‚³ã‚¢', fontproperties=self.font_prop, fontsize=10)
        ax1.tick_params(axis='x', rotation=45, labelsize=9)
        for label in ax1.get_xticklabels():
            label.set_fontproperties(self.font_prop)
        
        # 2. ç«¶é¦¬å ´åˆ¥å‹ç‡vsé©æ€§ã‚¹ã‚³ã‚¢
        ax2 = axes[0, 1]
        win_rates = track_analysis[('å‹åˆ©', 'mean')]
        ax2.scatter(scores, win_rates, s=100, alpha=0.7)
        for i, track in enumerate(tracks):
            ax2.annotate(track, (scores.iloc[i], win_rates.iloc[i]), 
                        xytext=(5, 5), textcoords='offset points',
                        fontproperties=self.font_prop, fontsize=9)
        ax2.set_xlabel('å¹³å‡é©æ€§ã‚¹ã‚³ã‚¢', fontproperties=self.font_prop, fontsize=10)
        ax2.set_ylabel('å¹³å‡å‹ç‡', fontproperties=self.font_prop, fontsize=10)
        ax2.set_title('é©æ€§ã‚¹ã‚³ã‚¢ vs å‹ç‡', fontproperties=self.font_prop, fontsize=12)
        
        # 3. èƒ½åŠ›ãƒ©ãƒ³ã‚¯åˆ¥å‹ç‡ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰
        ax3 = axes[1, 0]
        heatmap_data = ability_analysis.pivot(index='å ´å', columns='èƒ½åŠ›å€¤ãƒ©ãƒ³ã‚¯', values='å‹åˆ©')
        sns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', ax=ax3, fmt='.3f')
        ax3.set_title('èƒ½åŠ›ãƒ©ãƒ³ã‚¯åˆ¥å‹ç‡', fontproperties=self.font_prop, fontsize=12)
        ax3.set_xlabel('èƒ½åŠ›å€¤ãƒ©ãƒ³ã‚¯', fontproperties=self.font_prop, fontsize=10)
        ax3.set_ylabel('ç«¶é¦¬å ´', fontproperties=self.font_prop, fontsize=10)
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è»¸ãƒ©ãƒ™ãƒ«ã«ãƒ•ã‚©ãƒ³ãƒˆé©ç”¨
        for label in ax3.get_xticklabels():
            label.set_fontproperties(self.font_prop)
        for label in ax3.get_yticklabels():
            label.set_fontproperties(self.font_prop)
        
        # 4. ç«¶é¦¬å ´ç‰¹å¾´æ¯”è¼ƒï¼ˆãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰
        ax4 = axes[1, 1]
        self._create_track_comparison_chart(ax4, ['ä¸­å±±', 'ä¸­äº¬', 'æ±äº¬', 'æ–°æ½Ÿ'])
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_folder, 'ç«¶é¦¬å ´é©æ€§åˆ†æ.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # åˆ¥é€”ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ
        self._create_radar_chart_separately(['ä¸­å±±', 'ä¸­äº¬', 'æ±äº¬', 'æ–°æ½Ÿ'])
    
    def _create_track_comparison_chart(self, ax, tracks):
        """ç«¶é¦¬å ´ç‰¹å¾´ã®æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆä½œæˆï¼ˆãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆç‰ˆï¼‰"""
        categories = ['å‚é›£æ˜“åº¦', 'ã‚«ãƒ¼ãƒ–æ€¥åº¦', 'ãƒã‚¤ã‚¢ã‚¹å½±éŸ¿', 'ã‚¹ã‚¿ãƒŸãƒŠè¦æ±‚', 'ã‚¹ãƒ”ãƒ¼ãƒ‰æŒç¶š', 'å¤–æ ä¸åˆ©']
        
        x = np.arange(len(categories))
        width = 0.2
        
        for i, track in enumerate(tracks):
            if track in self.track_characteristics:
                values = [
                    self.track_characteristics[track]['slope_difficulty'],
                    self.track_characteristics[track]['curve_tightness'],
                    self.track_characteristics[track]['bias_impact'],
                    self.track_characteristics[track]['stamina_demand'],
                    self.track_characteristics[track]['speed_sustainability'],
                    self.track_characteristics[track]['outside_disadvantage']
                ]
                
                ax.bar(x + i * width, values, width, label=track, alpha=0.8)
        
        ax.set_xlabel('ç«¶é¦¬å ´ç‰¹å¾´', fontproperties=self.font_prop, fontsize=10)
        ax.set_ylabel('æ•°å€¤', fontproperties=self.font_prop, fontsize=10)
        ax.set_title('ç«¶é¦¬å ´ç‰¹å¾´æ¯”è¼ƒ', fontproperties=self.font_prop, fontsize=12)
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(categories, rotation=45, fontproperties=self.font_prop, fontsize=9)
        
        # å‡¡ä¾‹ã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        legend = ax.legend()
        for text in legend.get_texts():
            text.set_fontproperties(self.font_prop)
            
        ax.set_ylim(0, 1)
    
    def _create_radar_chart_separately(self, tracks):
        """ç«¶é¦¬å ´ç‰¹å¾´ã®ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’åˆ¥é€”ä½œæˆ"""
        categories = ['å‚é›£æ˜“åº¦', 'ã‚«ãƒ¼ãƒ–æ€¥åº¦', 'ãƒã‚¤ã‚¢ã‚¹å½±éŸ¿', 'ã‚¹ã‚¿ãƒŸãƒŠè¦æ±‚', 'ã‚¹ãƒ”ãƒ¼ãƒ‰æŒç¶š', 'å¤–æ ä¸åˆ©']
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        ax.set_theta_offset(np.pi / 2)
        ax.set_theta_direction(-1)
        ax.set_thetagrids(np.degrees(angles[:-1]), categories)
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ã«ãƒ•ã‚©ãƒ³ãƒˆé©ç”¨ï¼ˆæ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
        try:
            for label in ax.get_xticklabels():
                if hasattr(label, 'set_fontproperties'):
                    label.set_fontproperties(self.font_prop)
                    label.set_fontsize(10)
        except Exception as e:
            print(f"ãƒ©ãƒ™ãƒ«ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        for track in tracks:
            if track in self.track_characteristics:
                values = [
                    self.track_characteristics[track]['slope_difficulty'],
                    self.track_characteristics[track]['curve_tightness'],
                    self.track_characteristics[track]['bias_impact'],
                    self.track_characteristics[track]['stamina_demand'],
                    self.track_characteristics[track]['speed_sustainability'],
                    self.track_characteristics[track]['outside_disadvantage']
                ]
                values += values[:1]
                
                ax.plot(angles, values, 'o-', linewidth=2, label=track)
                ax.fill(angles, values, alpha=0.25)
        
        ax.set_ylim(0, 1)
        ax.set_title('ç«¶é¦¬å ´ç‰¹å¾´ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ', fontproperties=self.font_prop, fontsize=14, pad=20)
        
        # å‡¡ä¾‹ã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        legend = ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        for text in legend.get_texts():
            text.set_fontproperties(self.font_prop)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_folder, 'ç«¶é¦¬å ´ç‰¹å¾´ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ.png'), dpi=300, bbox_inches='tight')
        plt.close()
        print("ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆä½œæˆå®Œäº†")
    
    def _generate_compatibility_report(self, track_analysis, ability_analysis):
        """ç›¸æ€§åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report_path = os.path.join(self.output_folder, 'ç«¶é¦¬å ´é©æ€§åˆ†æãƒ¬ãƒãƒ¼ãƒˆ.md')
        
        with open(report_path, 'w', encoding='utf-8-sig') as f:
            f.write("# ç«¶é¦¬å ´Ã—é¦¬èƒ½åŠ›é©æ€§åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## åˆ†ææ¦‚è¦\n\n")
            f.write("ç«¶é¦¬å ´ã®ç‰©ç†çš„ç‰¹å¾´ã¨é¦¬ã®èƒ½åŠ›ç‰¹æ€§ã‚’æ•°å€¤åŒ–ã—ã€ç›¸æ€§ã‚’å®šé‡çš„ã«åˆ†æã—ã¾ã—ãŸã€‚\n\n")
            
            f.write("## ç«¶é¦¬å ´ç‰¹å¾´å®šç¾©\n\n")
            f.write("| ç«¶é¦¬å ´ | å‚é›£æ˜“åº¦ | ã‚«ãƒ¼ãƒ–æ€¥åº¦ | ãƒã‚¤ã‚¢ã‚¹å½±éŸ¿ | ã‚¹ã‚¿ãƒŸãƒŠè¦æ±‚ | ã‚¹ãƒ”ãƒ¼ãƒ‰æŒç¶š | å¤–æ ä¸åˆ© | ã‚¿ã‚¤ãƒ— |\n")
            f.write("|--------|----------|------------|-------------|-------------|-------------|----------|--------|\n")
            
            for track, features in self.track_characteristics.items():
                f.write(f"| {track} | {features['slope_difficulty']:.2f} | "
                       f"{features['curve_tightness']:.2f} | {features['bias_impact']:.2f} | "
                       f"{features['stamina_demand']:.2f} | {features['speed_sustainability']:.2f} | "
                       f"{features['outside_disadvantage']:.2f} | {features['track_type']} |\n")
            
            f.write("\n## ç«¶é¦¬å ´åˆ¥åˆ†æçµæœ\n\n")
            f.write("| ç«¶é¦¬å ´ | å¹³å‡é©æ€§ã‚¹ã‚³ã‚¢ | å¹³å‡å‹ç‡ | å¹³å‡ç€é † |\n")
            f.write("|--------|----------------|----------|----------|\n")
            
            for track in track_analysis.index:
                aptitude = track_analysis.loc[track, ('ç·åˆé©æ€§ã‚¹ã‚³ã‚¢', 'mean')]
                win_rate = track_analysis.loc[track, ('å‹åˆ©', 'mean')]
                avg_rank = track_analysis.loc[track, ('ç€é †', 'mean')]
                f.write(f"| {track} | {aptitude:.4f} | {win_rate:.4f} | {avg_rank:.2f} |\n")
            
            # é‡è¦ãªç™ºè¦‹
            f.write("\n## é‡è¦ãªç™ºè¦‹\n\n")
            
            best_aptitude_track = track_analysis[('ç·åˆé©æ€§ã‚¹ã‚³ã‚¢', 'mean')].idxmax()
            worst_aptitude_track = track_analysis[('ç·åˆé©æ€§ã‚¹ã‚³ã‚¢', 'mean')].idxmin()
            
            f.write(f"- **æœ€é«˜é©æ€§ç«¶é¦¬å ´**: {best_aptitude_track}\n")
            f.write(f"- **æœ€ä½é©æ€§ç«¶é¦¬å ´**: {worst_aptitude_track}\n")
            
            # èƒ½åŠ›ãƒ©ãƒ³ã‚¯åˆ¥ã®ç‰¹å¾´
            high_ability_best = ability_analysis[ability_analysis['èƒ½åŠ›å€¤ãƒ©ãƒ³ã‚¯'] == 'S'].nlargest(3, 'å‹åˆ©')
            f.write(f"\n### é«˜èƒ½åŠ›é¦¬ï¼ˆSãƒ©ãƒ³ã‚¯ï¼‰ãŒæœ€ã‚‚æ´»èºã™ã‚‹ç«¶é¦¬å ´TOP3\n")
            for i, (_, row) in enumerate(high_ability_best.iterrows(), 1):
                f.write(f"{i}. {row['å ´å']}: å‹ç‡{row['å‹åˆ©']:.3f}\n")
        
        print(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç«¶é¦¬å ´ç‰¹å¾´Ã—é¦¬èƒ½åŠ›é©æ€§åˆ†æ')
    parser.add_argument('--data-folder', type=str, default="export/with_bias",
                       help='ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹')
    parser.add_argument('--output-folder', type=str, default="results/track_horse_ability_analysis",
                       help='çµæœå‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹')
    
    args = parser.parse_args()
    
    # åˆ†æå™¨ã‚’åˆæœŸåŒ–
    analyzer = TrackHorseAbilityAnalyzer(
        data_folder=args.data_folder,
        output_folder=args.output_folder
    )
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
    if not analyzer.load_and_preprocess_data():
        print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return
    
    # é©æ€§ç›¸é–¢åˆ†æå®Ÿè¡Œ
    correlation_results = analyzer.analyze_track_aptitude_correlation()
    
    # ç«¶é¦¬å ´Ã—é¦¬ç›¸æ€§åˆ†æ
    analyzer.analyze_track_horse_compatibility()
    
    print(f"\n=== åˆ†æå®Œäº† ===")
    print(f"çµæœä¿å­˜å…ˆ: {args.output_folder}")

if __name__ == "__main__":
    main() 