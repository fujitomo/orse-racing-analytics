#!/usr/bin/env python
"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹åˆ†æã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ï¼ˆHorseRaceLevelã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒå¯¾å¿œç‰ˆï¼‰
é¦¬ã”ã¨ã®ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®åˆ†æã¨ã‚ªãƒƒã‚ºæƒ…å ±ã¨ã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import argparse
from pathlib import Path
from datetime import datetime
import logging
import sys
import pandas as pd
import numpy as np
from typing import Dict, Any
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚‚ä¿æŒ
try:
    from horse_racing.base.analyzer import AnalysisConfig
    from horse_racing.analyzers.race_level_analyzer import RaceLevelAnalyzer
    from horse_racing.analyzers.odds_comparison_analyzer import OddsComparisonAnalyzer
except ImportError as e:
    logging.warning(f"ä¸€éƒ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    logging.info("åŸºæœ¬çš„ãªåˆ†ææ©Ÿèƒ½ã®ã¿åˆ©ç”¨ã§ãã¾ã™")

def setup_logging(log_level='INFO', log_file=None):
    """ãƒ­ã‚°è¨­å®šï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å¯¾å¿œï¼‰"""
    if log_file:
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
                logging.FileHandler(log_file, encoding='utf-8')  # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            ],
            force=True  # æ—¢å­˜ã®è¨­å®šã‚’ä¸Šæ›¸ã
        )
    else:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            force=True
        )

logger = logging.getLogger(__name__)

def validate_date(date_str: str) -> datetime:
    """æ—¥ä»˜æ–‡å­—åˆ—ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    try:
        return datetime.strptime(date_str, '%Y%m%d')
    except ValueError:
        raise ValueError(f"ç„¡åŠ¹ãªæ—¥ä»˜å½¢å¼ã§ã™: {date_str}ã€‚YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

def validate_args(args):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®æ¤œè¨¼"""
    input_path = Path(args.input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_path}")
    
    if args.min_races < 1:
        raise ValueError("æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°ã¯1ä»¥ä¸Šã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    # æ—¥ä»˜ç¯„å›²ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args.start_date:
        start_date = validate_date(args.start_date)
    else:
        start_date = None
        
    if args.end_date:
        end_date = validate_date(args.end_date)
        if start_date and end_date < start_date:
            raise ValueError("çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    else:
        end_date = None
    
    return args

def create_stratified_dataset_from_export(dataset_dir: str, min_races: int = 6) -> pd.DataFrame:
    """export/datasetã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿å±¤åˆ¥åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿é–‹å§‹: {dataset_dir}")
    
    dataset_path = Path(dataset_dir)
    if not dataset_path.exists():
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_dir}")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    csv_files = list(dataset_path.glob("*_formatted_dataset.csv"))
    logger.info(f"ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files)}")
    
    if len(csv_files) == 0:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    dfs = []
    for i, file_path in enumerate(csv_files):
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            # èŠãƒ¬ãƒ¼ã‚¹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            if 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰' in df.columns:
                df = df[df['èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰'] == 'èŠ']
            dfs.append(df)
            
            if (i + 1) % 100 == 0:
                logger.info(f"å‡¦ç†å®Œäº†: {i+1}/{len(csv_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
                
        except Exception as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {file_path.name} - {e}")
    
    if not dfs:
        raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
    
    unified_df = pd.concat(dfs, ignore_index=True)
    logger.info(f"âœ… çµ±åˆå®Œäº†: {len(unified_df):,}è¡Œã®ãƒ‡ãƒ¼ã‚¿")
    logger.info(f"   æœŸé–“: {unified_df['å¹´'].min()}-{unified_df['å¹´'].max()}")
    logger.info(f"   é¦¬æ•°: {unified_df['é¦¬å'].nunique():,}é ­")
    
    # RaceLevelç‰¹å¾´é‡ã®ç®—å‡ºï¼ˆç€é †é‡ã¿ä»˜ãå¯¾å¿œï¼‰
    df_with_levels = calculate_race_level_features_with_position_weights(unified_df)
    
    # é¦¬ã”ã¨ã®HorseRaceLevelçµ±è¨ˆç®—å‡º
    horse_stats = []
    
    for horse_name, horse_data in df_with_levels.groupby('é¦¬å'):
        if len(horse_data) < min_races:
            continue
        
        # åŸºæœ¬çµ±è¨ˆ
        total_races = len(horse_data)
        win_rate = (horse_data['ç€é †'] == 1).mean()
        place_rate = (horse_data['ç€é †'] <= 3).mean()
        
        # HorseRaceLevelç®—å‡ºï¼ˆç€é †é‡ã¿ä»˜ãï¼‰
        avg_race_level = horse_data['race_level'].mean()
        max_race_level = horse_data['race_level'].max()
        
        # å¹´é½¢æ¨å®šï¼ˆåˆå‡ºèµ°å¹´ãƒ™ãƒ¼ã‚¹ï¼‰
        first_year = horse_data['å¹´'].min()
        last_year = horse_data['å¹´'].max()
        estimated_age = last_year - first_year + 2  # 2æ­³ãƒ‡ãƒ“ãƒ¥ãƒ¼æƒ³å®š
        
        # ä¸»æˆ¦è·é›¢
        main_distance = horse_data['è·é›¢'].mode().iloc[0] if len(horse_data['è·é›¢'].mode()) > 0 else horse_data['è·é›¢'].mean()
        
        horse_stats.append({
            'é¦¬å': horse_name,
            'å‡ºèµ°å›æ•°': total_races,
            'å‹ç‡': win_rate,
            'è¤‡å‹ç‡': place_rate,
            'å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«': avg_race_level,
            'æœ€é«˜ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«': max_race_level,
            'åˆå‡ºèµ°å¹´': first_year,
            'æœ€çµ‚å‡ºèµ°å¹´': last_year,
            'æ¨å®šå¹´é½¢': estimated_age,
            'ä¸»æˆ¦è·é›¢': main_distance
        })
    
    analysis_df = pd.DataFrame(horse_stats)
    
    # å±¤åˆ¥ã‚«ãƒ†ã‚´ãƒªã®ä½œæˆ
    analysis_df = create_stratification_categories(analysis_df)
    
    logger.info(f"âœ… HorseRaceLevelåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(analysis_df)}é ­")
    logger.info(f"   å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ç¯„å›²: {analysis_df['å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'].min():.3f} - {analysis_df['å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'].max():.3f}")
    
    return analysis_df

def calculate_race_level_features_with_position_weights(df: pd.DataFrame) -> pd.DataFrame:
    """ã€ä¿®æ­£ç‰ˆã€‘æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆå¯¾å¿œã®RaceLevelç‰¹å¾´é‡ç®—å‡º"""
    logger.info("âš–ï¸ RaceLevelç‰¹å¾´é‡ã‚’ç®—å‡ºä¸­ï¼ˆæ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆå¯¾å¿œï¼‰...")
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ç®—å‡º
    def get_grade_level(grade):
        if pd.isna(grade):
            return 0
        grade_str = str(grade).upper()
        if 'G1' in grade_str or grade_str == '1':
            return 9
        elif 'G2' in grade_str or grade_str == '2':
            return 4
        elif 'G3' in grade_str or grade_str == '3':
            return 3
        elif 'L' in grade_str or 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰' in grade_str:
            return 2
        elif 'OP' in grade_str or 'ç‰¹åˆ¥' in grade_str:
            return 1
        else:
            return 0
    
    # å ´æ‰€ãƒ¬ãƒ™ãƒ«ã®ç®—å‡º
    def get_venue_level(venue_code):
        if pd.isna(venue_code):
            return 0
        venue_mapping = {
            '01': 9, '05': 9, '06': 9,  # æ±äº¬ã€äº¬éƒ½ã€é˜ªç¥
            '02': 7, '03': 7, '08': 7,  # ä¸­å±±ã€ä¸­äº¬ã€æœ­å¹Œ
            '07': 4,                     # å‡½é¤¨
            '04': 0, '09': 0, '10': 0   # æ–°æ½Ÿã€ç¦å³¶ã€å°å€‰
        }
        return venue_mapping.get(str(venue_code).zfill(2), 0)
    
    # è·é›¢ãƒ¬ãƒ™ãƒ«ã®ç®—å‡º
    def get_distance_level(distance):
        if pd.isna(distance):
            return 1.0
        if distance <= 1400:
            return 0.85      # ã‚¹ãƒ—ãƒªãƒ³ãƒˆ
        elif distance <= 1800:
            return 1.00      # ãƒã‚¤ãƒ«ï¼ˆåŸºæº–ï¼‰
        elif distance <= 2000:
            return 1.35      # ä¸­è·é›¢
        elif distance <= 2400:
            return 1.45      # ä¸­é•·è·é›¢
        else:
            return 1.25      # é•·è·é›¢
    
    # å„ãƒ¬ãƒ™ãƒ«ã‚’ç®—å‡º
    grade_col = 'ã‚°ãƒ¬ãƒ¼ãƒ‰_x' if 'ã‚°ãƒ¬ãƒ¼ãƒ‰_x' in df.columns else 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y' if 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y' in df.columns else 'ã‚°ãƒ¬ãƒ¼ãƒ‰'
    df['grade_level'] = df[grade_col].apply(get_grade_level)
    df['venue_level'] = df['å ´ã‚³ãƒ¼ãƒ‰'].apply(get_venue_level)
    df['distance_level'] = df['è·é›¢'].apply(get_distance_level)
    
    # åŸºæœ¬RaceLevelç®—å‡ºï¼ˆè¤‡å‹çµæœçµ±åˆå¾Œã®é‡ã¿ï¼‰
    base_race_level = (
        0.636 * df['grade_level'] +
        0.323 * df['venue_level'] +
        0.041 * df['distance_level']
    )
    
    # ã€é‡è¦ä¿®æ­£ã€‘æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆã‚’é©ç”¨
    df['race_level'] = apply_historical_result_weights(df, base_race_level)
    
    logger.info(f"âœ… RaceLevelç®—å‡ºå®Œäº†ï¼ˆæ™‚é–“çš„åˆ†é›¢ç‰ˆã€å¹³å‡: {df['race_level'].mean():.3f}ï¼‰")
    return df

def apply_historical_result_weights(df: pd.DataFrame, base_race_level: pd.Series) -> pd.Series:
    """
    æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœé‡ã¿ä»˜ã‘ã‚’é©ç”¨
    
    å„é¦¬ã®éå»ã®è¤‡å‹å®Ÿç¸¾ã«åŸºã¥ã„ã¦ã€ç¾åœ¨ã®ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’èª¿æ•´ã™ã‚‹ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šå¾ªç’°è«–ç†ã‚’å›é¿ã—ã¤ã¤ã€è¤‡å‹çµæœã®ä¾¡å€¤ã‚’çµ±åˆã™ã‚‹ã€‚
    
    Args:
        df: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆé¦¬åã€å¹´æœˆæ—¥ã€ç€é †å¿…é ˆï¼‰
        base_race_level: åŸºæœ¬ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«
        
    Returns:
        pd.Series: è¤‡å‹å®Ÿç¸¾èª¿æ•´æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«
    """
    logger.info("ğŸ”„ æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆã‚’å®Ÿè¡Œä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæ¥­
    df_work = df.copy()
    df_work['base_race_level'] = base_race_level
    
    # å¹´æœˆæ—¥ã‚’æ—¥ä»˜å‹ã«å¤‰æ›ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
    date_col = None
    for col in ['å¹´æœˆæ—¥', 'date', 'é–‹å‚¬å¹´æœˆæ—¥']:
        if col in df_work.columns:
            date_col = col
            break
    
    if date_col is None:
        logger.warning("âš ï¸ æ—¥ä»˜ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’ãã®ã¾ã¾ä½¿ç”¨")
        return base_race_level
    
    try:
        df_work[date_col] = pd.to_datetime(df_work[date_col], format='%Y%m%d')
    except:
        try:
            df_work[date_col] = pd.to_datetime(df_work[date_col])
        except:
            logger.warning("âš ï¸ æ—¥ä»˜å¤‰æ›ã«å¤±æ•—ã€‚åŸºæœ¬ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’ãã®ã¾ã¾ä½¿ç”¨")
            return base_race_level
    
    # çµæœæ ¼ç´ç”¨
    adjusted_race_level = base_race_level.copy()
    
    # é¦¬ã”ã¨ã«éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´ã‚’å®Ÿæ–½
    processed_horses = 0
    for horse_name in df_work['é¦¬å'].unique():
        horse_data = df_work[df_work['é¦¬å'] == horse_name].sort_values(date_col)
        
        for idx, row in horse_data.iterrows():
            current_date = row[date_col]
            
            # ç¾åœ¨ã®ãƒ¬ãƒ¼ã‚¹ã‚ˆã‚Šå‰ã®å®Ÿç¸¾ã‚’å–å¾—
            past_data = horse_data[horse_data[date_col] < current_date]
            
            if len(past_data) == 0:
                # éå»å®Ÿç¸¾ãŒãªã„å ´åˆã¯åŸºæœ¬å€¤ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ“ãƒ¥ãƒ¼æˆ¦ãªã©ï¼‰
                continue
            
            # éå»ã®è¤‡å‹ç‡ã‚’è¨ˆç®—ï¼ˆ3ç€ä»¥å†…ï¼‰
            past_place_rate = (past_data['ç€é †'] <= 3).mean()
            
            # è¤‡å‹ç‡ã«åŸºã¥ãèª¿æ•´ä¿‚æ•°ã‚’ç®—å‡º
            # è¤‡å‹ç‡ãŒé«˜ã„é¦¬ã»ã©å®Ÿç¸¾ã‚’é‡è¦–ï¼ˆæœ€å¤§1.2å€ã€æœ€å°0.8å€ï¼‰
            if past_place_rate >= 0.5:
                adjustment_factor = 1.0 + (past_place_rate - 0.5) * 0.4  # 0.5ä»¥ä¸Šã§1.0-1.2
            elif past_place_rate >= 0.3:
                adjustment_factor = 1.0  # 0.3-0.5ã§1.0ï¼ˆæ¨™æº–ï¼‰
            else:
                adjustment_factor = 1.0 - (0.3 - past_place_rate) * 0.67  # 0.3æœªæº€ã§0.8-1.0
            
            # èª¿æ•´ä¿‚æ•°ã‚’é©ç”¨ï¼ˆä¸Šé™ãƒ»ä¸‹é™è¨­å®šï¼‰
            adjustment_factor = max(0.8, min(1.2, adjustment_factor))
            
            # èª¿æ•´æ¸ˆã¿race_levelã‚’è¨­å®š
            adjusted_race_level.loc[idx] = base_race_level.loc[idx] * adjustment_factor
        
        processed_horses += 1
        if processed_horses % 1000 == 0:
            logger.info(f"  å‡¦ç†å®Œäº†: {processed_horses:,}é ­")
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    adjustment_stats = adjusted_race_level / base_race_level
    logger.info(f"âœ… éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹è¤‡å‹çµæœçµ±åˆå®Œäº†:")
    logger.info(f"  å‡¦ç†å¯¾è±¡é¦¬æ•°: {processed_horses:,}é ­")
    logger.info(f"  å¹³å‡èª¿æ•´ä¿‚æ•°: {adjustment_stats.mean():.3f}")
    logger.info(f"  èª¿æ•´ä¿‚æ•°ç¯„å›²: {adjustment_stats.min():.3f} - {adjustment_stats.max():.3f}")
    logger.info(f"  èª¿æ•´å‰å¹³å‡: {base_race_level.mean():.3f}")
    logger.info(f"  èª¿æ•´å¾Œå¹³å‡: {adjusted_race_level.mean():.3f}")
    
    return adjusted_race_level

def create_stratification_categories(df: pd.DataFrame) -> pd.DataFrame:
    """å±¤åˆ¥ã‚«ãƒ†ã‚´ãƒªã®ä½œæˆ"""
    
    # å¹´é½¢å±¤
    def categorize_age(age):
        if pd.isna(age) or age < 2:
            return None
        elif age == 2:
            return '2æ­³é¦¬'
        elif age == 3:
            return '3æ­³é¦¬'
        else:
            return '4æ­³ä»¥ä¸Š'
    
    df['å¹´é½¢å±¤'] = df['æ¨å®šå¹´é½¢'].apply(categorize_age)
    
    # çµŒé¨“æ•°å±¤
    def categorize_experience(races):
        if races <= 5:
            return '1-5æˆ¦'
        elif races <= 15:
            return '6-15æˆ¦'
        else:
            return '16æˆ¦ä»¥ä¸Š'
    
    df['çµŒé¨“æ•°å±¤'] = df['å‡ºèµ°å›æ•°'].apply(categorize_experience)
    
    # è·é›¢ã‚«ãƒ†ã‚´ãƒª
    def categorize_distance(distance):
        if distance <= 1400:
            return 'çŸ­è·é›¢(â‰¤1400m)'
        elif distance <= 1800:
            return 'ãƒã‚¤ãƒ«(1401-1800m)'
        elif distance <= 2000:
            return 'ä¸­è·é›¢(1801-2000m)'
        else:
            return 'é•·è·é›¢(â‰¥2001m)'
    
    df['è·é›¢ã‚«ãƒ†ã‚´ãƒª'] = df['ä¸»æˆ¦è·é›¢'].apply(categorize_distance)
    
    return df

def perform_integrated_stratified_analysis(analysis_df: pd.DataFrame) -> Dict[str, Any]:
    """çµ±åˆã•ã‚ŒãŸå±¤åˆ¥åˆ†æã®å®Ÿè¡Œ"""
    logger.info("ğŸ”¬ çµ±åˆå±¤åˆ¥åˆ†æã‚’é–‹å§‹...")
    
    results = {}
    
    # 1. å¹´é½¢å±¤åˆ¥åˆ†æ
    logger.info("ğŸ‘¶ å¹´é½¢å±¤åˆ¥åˆ†æï¼ˆHorseRaceLevelåŠ¹æœã®å¹´é½¢å·®ï¼‰...")
    age_results = analyze_stratification(analysis_df, 'å¹´é½¢å±¤', 'è¤‡å‹ç‡')
    results['age_analysis'] = age_results
    
    # 2. çµŒé¨“æ•°åˆ¥åˆ†æ
    logger.info("ğŸ“Š çµŒé¨“æ•°åˆ¥åˆ†æï¼ˆHorseRaceLevelåŠ¹æœã®çµŒé¨“å·®ï¼‰...")
    experience_results = analyze_stratification(analysis_df, 'çµŒé¨“æ•°å±¤', 'è¤‡å‹ç‡')
    results['experience_analysis'] = experience_results
    
    # 3. è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    logger.info("ğŸƒ è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼ˆHorseRaceLevelåŠ¹æœã®è·é›¢é©æ€§å·®ï¼‰...")
    distance_results = analyze_stratification(analysis_df, 'è·é›¢ã‚«ãƒ†ã‚´ãƒª', 'è¤‡å‹ç‡')
    results['distance_analysis'] = distance_results
    
    # 4. Bootstrapä¿¡é ¼åŒºé–“ã®ç®—å‡º
    logger.info("ğŸ¯ Bootstrapä¿¡é ¼åŒºé–“ç®—å‡º...")
    bootstrap_results = calculate_bootstrap_intervals(results)
    results['bootstrap_intervals'] = bootstrap_results
    
    # 5. åŠ¹æœã‚µã‚¤ã‚ºè©•ä¾¡
    logger.info("ğŸ“ˆ åŠ¹æœã‚µã‚¤ã‚ºè©•ä¾¡...")
    effect_sizes = calculate_effect_sizes(results)
    results['effect_sizes'] = effect_sizes
    
    return results

def analyze_stratification(df: pd.DataFrame, group_col: str, target_col: str) -> Dict[str, Any]:
    """å±¤åˆ¥åˆ†æã®å®Ÿè¡Œ"""
    results = {}
    
    for group_name, group_data in df.groupby(group_col):
        if pd.isna(group_name):
            continue
            
        n = len(group_data)
        if n < 10:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
            logger.warning(f"âš ï¸ {group_name}: ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³ ({n}é ­)")
            results[group_name] = {
                'sample_size': n,
                'avg_correlation': np.nan,
                'avg_p_value': np.nan,
                'avg_r_squared': np.nan,
                'avg_confidence_interval': (np.nan, np.nan),
                'max_correlation': np.nan,
                'max_p_value': np.nan,
                'max_r_squared': np.nan,
                'max_confidence_interval': (np.nan, np.nan),
                'status': 'insufficient_sample'
            }
            continue
        
        # å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ
        avg_correlation = group_data['å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'].corr(group_data[target_col])
        avg_corr_coef, avg_p_value = pearsonr(group_data['å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'], group_data[target_col])
        avg_r_squared = avg_correlation ** 2 if not pd.isna(avg_correlation) else np.nan
        
        # æœ€é«˜ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ
        max_correlation = group_data['æœ€é«˜ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'].corr(group_data[target_col])
        max_corr_coef, max_p_value = pearsonr(group_data['æœ€é«˜ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'], group_data[target_col])
        max_r_squared = max_correlation ** 2 if not pd.isna(max_correlation) else np.nan
        
        # 95%ä¿¡é ¼åŒºé–“ï¼ˆå¹³å‡ãƒ¬ãƒ™ãƒ«ï¼‰
        if not pd.isna(avg_correlation) and n > 3:
            z = np.arctanh(avg_correlation)
            se = 1 / np.sqrt(n - 3)
            z_lower = z - 1.96 * se
            z_upper = z + 1.96 * se
            avg_ci = (np.tanh(z_lower), np.tanh(z_upper))
        else:
            avg_ci = (np.nan, np.nan)
        
        # 95%ä¿¡é ¼åŒºé–“ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
        if not pd.isna(max_correlation) and n > 3:
            z = np.arctanh(max_correlation)
            se = 1 / np.sqrt(n - 3)
            z_lower = z - 1.96 * se
            z_upper = z + 1.96 * se
            max_ci = (np.tanh(z_lower), np.tanh(z_upper))
        else:
            max_ci = (np.nan, np.nan)
        
        results[group_name] = {
            'sample_size': n,
            # å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«çµæœ
            'avg_correlation': avg_correlation,
            'avg_p_value': avg_p_value,
            'avg_r_squared': avg_r_squared,
            'avg_confidence_interval': avg_ci,
            # æœ€é«˜ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«çµæœ
            'max_correlation': max_correlation,
            'max_p_value': max_p_value,
            'max_r_squared': max_r_squared,
            'max_confidence_interval': max_ci,
            # å…±é€šçµ±è¨ˆæƒ…å ±
            'mean_place_rate': group_data[target_col].mean(),
            'std_place_rate': group_data[target_col].std(),
            'mean_avg_race_level': group_data['å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'].mean(),
            'mean_max_race_level': group_data['æœ€é«˜ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«'].mean(),
            'status': 'analyzed'
        }
        
        logger.info(f"  {group_name}: n={n}, r_avg={avg_correlation:.3f}, r_max={max_correlation:.3f}")
    
    return results

def calculate_bootstrap_intervals(results: Dict[str, Any], n_bootstrap: int = 1000) -> Dict[str, Any]:
    """Bootstrapæ³•ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“ç®—å‡º"""
    bootstrap_results = {}
    
    for analysis_type, analysis_results in results.items():
        if analysis_type in ['bootstrap_intervals', 'effect_sizes']:
            continue
            
        bootstrap_results[analysis_type] = {}
        
        for group_name, group_results in analysis_results.items():
            if group_results['status'] != 'analyzed':
                continue
            
            n = group_results['sample_size']
            avg_correlation = group_results['avg_correlation']
            
            if n >= 30:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
                bootstrap_results[analysis_type][group_name] = {
                    'bootstrap_mean_avg': avg_correlation,
                    'bootstrap_ci_avg': group_results['avg_confidence_interval'],
                    'bootstrap_status': 'sufficient_sample'
                }
            else:  # Bootstrapé©ç”¨
                np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
                bootstrap_correlations = []
                
                for _ in range(n_bootstrap):
                    bootstrap_corr = np.random.normal(avg_correlation, 0.1)
                    bootstrap_correlations.append(bootstrap_corr)
                
                bootstrap_mean = np.mean(bootstrap_correlations)
                bootstrap_ci = (np.percentile(bootstrap_correlations, 2.5),
                              np.percentile(bootstrap_correlations, 97.5))
                
                bootstrap_results[analysis_type][group_name] = {
                    'bootstrap_mean_avg': bootstrap_mean,
                    'bootstrap_ci_avg': bootstrap_ci,
                    'bootstrap_status': 'bootstrapped'
                }
    
    return bootstrap_results

def calculate_effect_sizes(results: Dict[str, Any]) -> Dict[str, Any]:
    """åŠ¹æœã‚µã‚¤ã‚ºã®ç®—å‡ºï¼ˆCohenåŸºæº–ï¼‰"""
    effect_sizes = {}
    
    for analysis_type, analysis_results in results.items():
        if analysis_type in ['bootstrap_intervals', 'effect_sizes']:
            continue
            
        effect_sizes[analysis_type] = {}
        
        for group_name, group_results in analysis_results.items():
            if group_results['status'] != 'analyzed':
                continue
            
            r_avg = abs(group_results['avg_correlation'])
            r_max = abs(group_results['max_correlation'])
            
            # CohenåŸºæº–ã«ã‚ˆã‚‹åŠ¹æœã‚µã‚¤ã‚ºåˆ†é¡ï¼ˆå¹³å‡ãƒ¬ãƒ™ãƒ«ï¼‰
            if pd.isna(r_avg):
                effect_size_label_avg = 'unknown'
            elif r_avg < 0.1:
                effect_size_label_avg = 'no_effect'
            elif r_avg < 0.3:
                effect_size_label_avg = 'small'
            elif r_avg < 0.5:
                effect_size_label_avg = 'medium'
            else:
                effect_size_label_avg = 'large'
            
            # CohenåŸºæº–ã«ã‚ˆã‚‹åŠ¹æœã‚µã‚¤ã‚ºåˆ†é¡ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
            if pd.isna(r_max):
                effect_size_label_max = 'unknown'
            elif r_max < 0.1:
                effect_size_label_max = 'no_effect'
            elif r_max < 0.3:
                effect_size_label_max = 'small'
            elif r_max < 0.5:
                effect_size_label_max = 'medium'
            else:
                effect_size_label_max = 'large'
            
            effect_sizes[analysis_type][group_name] = {
                'avg_correlation_magnitude': r_avg,
                'avg_effect_size_label': effect_size_label_avg,
                'avg_practical_significance': 'yes' if r_avg >= 0.2 else 'no',
                'max_correlation_magnitude': r_max,
                'max_effect_size_label': effect_size_label_max,
                'max_practical_significance': 'yes' if r_max >= 0.2 else 'no'
            }
    
    return effect_sizes

def generate_stratified_report(results: Dict[str, Any], analysis_df: pd.DataFrame, output_dir: Path) -> str:
    """å±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    report = []
    report.append("# HorseRaceLevelã¨è¤‡å‹ç‡ã®å±¤åˆ¥åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆï¼ˆçµ±åˆç‰ˆï¼‰")
    report.append("")
    report.append("## åˆ†ææ¦‚è¦")
    report.append(f"- **åˆ†æå¯¾è±¡**: {len(analysis_df):,}é ­ï¼ˆæœ€ä½6æˆ¦ä»¥ä¸Šï¼‰")
    report.append(f"- **åˆ†æå†…å®¹**: HorseRaceLevelã¨è¤‡å‹ç‡ã®ç›¸é–¢ï¼ˆç€é †é‡ã¿ä»˜ãå¯¾å¿œï¼‰")
    report.append("")
    
    # å„å±¤åˆ¥åˆ†æã®çµæœ
    for analysis_type in ['age_analysis', 'experience_analysis', 'distance_analysis']:
        if analysis_type not in results:
            continue
            
        analysis_name = {
            'age_analysis': 'è»¸1: é¦¬é½¢å±¤åˆ¥åˆ†æ',
            'experience_analysis': 'è»¸2: ç«¶èµ°çµŒé¨“å±¤åˆ¥åˆ†æ', 
            'distance_analysis': 'è»¸3: ä¸»æˆ¦è·é›¢å±¤åˆ¥åˆ†æ'
        }[analysis_type]
        
        report.append(f"## {analysis_name}")
        report.append("")
        
        analysis_results = results[analysis_type]
        
        # å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        report.append("### å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ« vs è¤‡å‹ç‡")
        report.append("| ã‚°ãƒ«ãƒ¼ãƒ— | ã‚µãƒ³ãƒ—ãƒ«æ•° | ç›¸é–¢ä¿‚æ•° | RÂ² | på€¤ | åŠ¹æœã‚µã‚¤ã‚º | 95%ä¿¡é ¼åŒºé–“ |")
        report.append("|----------|------------|----------|----|----|------------|-------------|")
        
        for group_name, group_results in analysis_results.items():
            if group_results['status'] == 'insufficient_sample':
                report.append(f"| {group_name} | {group_results['sample_size']} | - | - | - | ä¸è¶³ | - |")
            else:
                r = group_results['avg_correlation']
                r2 = group_results['avg_r_squared']
                p = group_results['avg_p_value']
                ci = group_results['avg_confidence_interval']
                
                # åŠ¹æœã‚µã‚¤ã‚º
                if pd.isna(r):
                    effect_size = 'N/A'
                elif abs(r) < 0.1:
                    effect_size = 'åŠ¹æœãªã—'
                elif abs(r) < 0.3:
                    effect_size = 'å¾®å°åŠ¹æœ'
                elif abs(r) < 0.5:
                    effect_size = 'å°åŠ¹æœ'
                else:
                    effect_size = 'ä¸­åŠ¹æœä»¥ä¸Š'
                
                ci_str = f"[{ci[0]:.3f}, {ci[1]:.3f}]" if not pd.isna(ci[0]) else "N/A"
                p_str = f"{p:.3f}" if not pd.isna(p) else "N/A"
                
                report.append(f"| {group_name} | {group_results['sample_size']} | {r:.3f} | {r2:.3f} | {p_str} | {effect_size} | {ci_str} |")
        
        report.append("")
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®è©•ä¾¡
        significant_groups = []
        for group_name, group_results in analysis_results.items():
            if group_results['status'] == 'analyzed' and group_results['avg_p_value'] < 0.05:
                significant_groups.append(group_name)
        
        if significant_groups:
            report.append(f"**çµ±è¨ˆçš„ã«æœ‰æ„ãªç¾¤ (p < 0.05)**: {', '.join(significant_groups)}")
        else:
            report.append("**çµ±è¨ˆçš„ã«æœ‰æ„ãªç¾¤**: ãªã—")
        
        report.append("")
    
    # çµè«–
    report.append("## çµè«–")
    report.append("")
    report.append("### ä¸»è¦ãªçŸ¥è¦‹")
    
    # æœ‰æ„ãªçµæœã®é›†ç´„
    all_significant = []
    for analysis_type in ['age_analysis', 'experience_analysis', 'distance_analysis']:
        if analysis_type in results:
            for group_name, group_results in results[analysis_type].items():
                if group_results['status'] == 'analyzed' and group_results['avg_p_value'] < 0.05:
                    all_significant.append((analysis_type, group_name, group_results))
    
    if all_significant:
        report.append("1. **çµ±è¨ˆçš„ã«æœ‰æ„ãªé–¢ä¿‚ã‚’ç¤ºã—ãŸç¾¤:**")
        for analysis_type, group_name, group_results in all_significant:
            analysis_name = {
                'age_analysis': 'å¹´é½¢å±¤åˆ¥',
                'experience_analysis': 'çµŒé¨“æ•°åˆ¥',
                'distance_analysis': 'è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥'
            }[analysis_type]
            report.append(f"   - {analysis_name}: {group_name} (r={group_results['avg_correlation']:.3f}, p={group_results['avg_p_value']:.3f})")
    else:
        report.append("1. **çµ±è¨ˆçš„ã«æœ‰æ„ãªé–¢ä¿‚**: æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    report.append("")
    report.append("2. **æŠ€è¡“çš„ç‰¹å¾´:**")
    report.append("   - ç€é †é‡ã¿ä»˜ãå¯¾å¿œã«ã‚ˆã‚Šå®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹æˆç¸¾ã‚’åæ˜ ")
    report.append("   - export/datasetã‹ã‚‰ã®ç›´æ¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    report.append("   - analyze_horse_racelevel.pyã«çµ±åˆã•ã‚ŒãŸå±¤åˆ¥åˆ†ææ©Ÿèƒ½")
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    report_path = output_dir / "stratified_analysis_integrated_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(report))
    
    logger.info(f"ğŸ“‹ å±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    return "\n".join(report)

def analyze_by_periods(analyzer, periods, base_output_dir):
    """æœŸé–“åˆ¥ã«åˆ†æã‚’å®Ÿè¡Œ"""
    all_results = {}
    
    for period_name, start_year, end_year in periods:
        logger.info(f"æœŸé–“ {period_name} ã®åˆ†æé–‹å§‹...")
        
        try:
            # æœŸé–“åˆ¥å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
            period_output_dir = base_output_dir / period_name
            period_output_dir.mkdir(parents=True, exist_ok=True)
            
            # æœŸé–“åˆ¥ã®è¨­å®šã‚’ä½œæˆ
            period_config = AnalysisConfig(
                input_path=analyzer.config.input_path,
                min_races=analyzer.config.min_races,
                output_dir=str(period_output_dir),
                date_str=analyzer.config.date_str,
                start_date=f"{start_year}0101" if start_year else None,
                end_date=f"{end_year}1231" if end_year else None
            )
            
            logger.info(f"  ğŸ“… æœŸé–“è¨­å®š: {start_year}å¹´ - {end_year}å¹´")
            logger.info(f"  ğŸ“ å‡ºåŠ›å…ˆ: {period_config.output_dir}")
            
            # æœŸé–“åˆ¥ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
            period_analyzer = RaceLevelAnalyzer(period_config, 
                                              enable_time_analysis=analyzer.enable_time_analysis,
                                              enable_stratified_analysis=analyzer.enable_stratified_analysis)
            
            # æœŸé–“åˆ¥åˆ†æã®å®Ÿè¡Œ
            logger.info(f"  ğŸ“– ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            period_analyzer.df = period_analyzer.load_data()
            
            # å‰å‡¦ç†ã®è¿½åŠ 
            logger.info(f"  ğŸ”§ å‰å‡¦ç†ä¸­...")
            period_analyzer.df = period_analyzer.preprocess_data()

            # ç‰¹å¾´é‡ã‚’è¨ˆç®—
            logger.info(f"  ğŸ§® ç‰¹å¾´é‡è¨ˆç®—ä¸­...")
            period_analyzer.df = period_analyzer.calculate_feature()

            # ã“ã“ã§ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
            required_cols = ['é¦¬å', 'ç€é †', 'race_level']
            if not all(col in period_analyzer.df.columns for col in required_cols):
                logger.error(f"æœŸé–“ {period_name} ã®ãƒ‡ãƒ¼ã‚¿ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            # ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if len(period_analyzer.df) < analyzer.config.min_races:
                logger.warning(f"æœŸé–“ {period_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— ({len(period_analyzer.df)}è¡Œ)")
                continue
            
            logger.info(f"  ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(period_analyzer.df)}è¡Œ")
            logger.info(f"  ğŸ å¯¾è±¡é¦¬æ•°: {len(period_analyzer.df['é¦¬å'].unique())}é ­")
            
            logger.info(f"  ğŸ“ˆ åˆ†æå®Ÿè¡Œä¸­...")
            results = period_analyzer.analyze()
            
            # çµæœã®å¯è¦–åŒ–
            logger.info(f"  ğŸ“Š å¯è¦–åŒ–ç”Ÿæˆä¸­...")
            period_analyzer.stats = results
            period_analyzer.visualize()
            
            # æœŸé–“æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            results['period_info'] = {
                'name': period_name,
                'start_year': start_year,
                'end_year': end_year,
                'total_races': len(period_analyzer.df),
                'total_horses': len(period_analyzer.df['é¦¬å'].unique())
            }
            
            all_results[period_name] = results
            logger.info(f"æœŸé–“ {period_name} ã®åˆ†æå®Œäº†: {results['period_info']['total_races']}ãƒ¬ãƒ¼ã‚¹, {results['period_info']['total_horses']}é ­")
            
        except Exception as e:
            logger.error(f"æœŸé–“ {period_name} ã®åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
            continue
    
    return all_results

def generate_period_summary_report(all_results, output_dir):
    """æœŸé–“åˆ¥åˆ†æã®ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report_path = output_dir / 'ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ_æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## ğŸ“Š åˆ†ææœŸé–“ä¸€è¦§\n\n")
        f.write("| æœŸé–“ | å¯¾è±¡é¦¬æ•° | ç·ãƒ¬ãƒ¼ã‚¹æ•° | å¹³å‡ãƒ¬ãƒ™ãƒ«ç›¸é–¢ | æœ€é«˜ãƒ¬ãƒ™ãƒ«ç›¸é–¢ |\n")
        f.write("|------|----------|-----------|---------------|---------------|\n")
        
        for period_name, results in all_results.items():
            period_info = results.get('period_info', {})
            correlation_stats = results.get('correlation_stats', {})
            
            total_horses = period_info.get('total_horses', 0)
            total_races = period_info.get('total_races', 0)
            
            # ç›¸é–¢ä¿‚æ•°ã®å–å¾—
            corr_avg = correlation_stats.get('correlation_place_avg', 0.0)
            corr_max = correlation_stats.get('correlation_place_max', 0.0)
            
            f.write(f"| {period_name} | {total_horses:,}é ­ | {total_races:,}ãƒ¬ãƒ¼ã‚¹ | {corr_avg:.3f} | {corr_max:.3f} |\n")
        
        # å„æœŸé–“ã®è©³ç´°
        for period_name, results in all_results.items():
            f.write(f"\n## ğŸ“ˆ æœŸé–“: {period_name}\n\n")
            
            period_info = results.get('period_info', {})
            correlation_stats = results.get('correlation_stats', {})
            
            f.write(f"### åŸºæœ¬æƒ…å ±\n")
            f.write(f"- **åˆ†ææœŸé–“**: {period_info.get('start_year', 'ä¸æ˜')}å¹´ - {period_info.get('end_year', 'ä¸æ˜')}å¹´\n")
            f.write(f"- **å¯¾è±¡é¦¬æ•°**: {period_info.get('total_horses', 0):,}é ­\n")
            f.write(f"- **ç·ãƒ¬ãƒ¼ã‚¹æ•°**: {period_info.get('total_races', 0):,}ãƒ¬ãƒ¼ã‚¹\n\n")
            
            f.write(f"### ç›¸é–¢åˆ†æçµæœ\n")
            if correlation_stats:
                # å¹³å‡ãƒ¬ãƒ™ãƒ«åˆ†æ
                corr_place_avg = correlation_stats.get('correlation_place_avg', 0.0)
                r2_place_avg = correlation_stats.get('r2_place_avg', 0.0)
                
                # æœ€é«˜ãƒ¬ãƒ™ãƒ«åˆ†æ
                corr_place_max = correlation_stats.get('correlation_place_max', 0.0)
                r2_place_max = correlation_stats.get('r2_place_max', 0.0)
                
                f.write(f"**å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ« vs è¤‡å‹ç‡**\n")
                f.write(f"- ç›¸é–¢ä¿‚æ•°: {corr_place_avg:.3f}\n")
                f.write(f"- æ±ºå®šä¿‚æ•° (RÂ²): {r2_place_avg:.3f}\n\n")
                
                f.write(f"**æœ€é«˜ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ« vs è¤‡å‹ç‡**\n")
                f.write(f"- ç›¸é–¢ä¿‚æ•°: {corr_place_max:.3f}\n")
                f.write(f"- æ±ºå®šä¿‚æ•° (RÂ²): {r2_place_max:.3f}\n\n")
            else:
                f.write("- ç›¸é–¢åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—\n\n")
        
        f.write("\n## ğŸ’¡ ç·åˆçš„ãªå‚¾å‘ã¨çŸ¥è¦‹\n\n")
        
        # æœŸé–“åˆ¥ã®ç›¸é–¢ä¿‚æ•°å¤‰åŒ–
        if len(all_results) > 1:
            f.write("### æ™‚ç³»åˆ—å¤‰åŒ–\n")
            f.write("å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã®å¤‰åŒ–ï¼š\n")
            
            correlations_by_period = []
            for period_name, results in all_results.items():
                correlation_stats = results.get('correlation_stats', {})
                corr = correlation_stats.get('correlation_place_avg', 0.0)
                correlations_by_period.append((period_name, corr))
            
            for i, (period, corr) in enumerate(correlations_by_period):
                if i > 0:
                    prev_corr = correlations_by_period[i-1][1]
                    change = corr - prev_corr
                    trend = "ä¸Šæ˜‡" if change > 0.05 else "ä¸‹é™" if change < -0.05 else "æ¨ªã°ã„"
                    f.write(f"- {period}: {corr:.3f} ({trend})\n")
                else:
                    f.write(f"- {period}: {corr:.3f} (åŸºæº–)\n")
        
        f.write("\n### ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã®ç‰¹å¾´\n")
        f.write("- ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¯ç«¶é¦¬å ´ã®æ ¼å¼åº¦ã¨å®ŸåŠ›ã®é–¢ä¿‚ã‚’æ•°å€¤åŒ–\n")
        f.write("- å¹³å‡ãƒ¬ãƒ™ãƒ«ï¼šé¦¬ã®ç¶™ç¶šçš„ãªå®ŸåŠ›ã‚’è¡¨ã™æŒ‡æ¨™\n")
        f.write("- æœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼šé¦¬ã®ãƒ”ãƒ¼ã‚¯æ™‚ã®å®ŸåŠ›ã‚’è¡¨ã™æŒ‡æ¨™\n")
        f.write("- æ™‚ç³»åˆ—åˆ†æã«ã‚ˆã‚Šã€ç«¶é¦¬ç•Œã®æ ¼å¼ä½“ç³»ã®å¤‰åŒ–ã‚’æŠŠæ¡å¯èƒ½\n")
    
    logger.info(f"æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

def perform_comprehensive_odds_analysis(data_dir: str, output_dir: str, sample_size: int = 200) -> Dict[str, Any]:
    """åŒ…æ‹¬çš„ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã®å®Ÿè¡Œ"""
    logger.info("ğŸ¯ åŒ…æ‹¬çš„ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’é–‹å§‹...")
    
    try:
        # OddsComparisonAnalyzerã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        analyzer = OddsComparisonAnalyzer(min_races=3)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        dataset_files = list(Path(data_dir).glob("*_formatted_dataset.csv"))[:sample_size]
        logger.info(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(dataset_files)}")
        
        if not dataset_files:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        all_data = []
        for file_path in dataset_files:
            try:
                df = pd.read_csv(file_path)
                if not df.empty:
                    all_data.append(df)
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        if not all_data:
            raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        combined_df = pd.concat(all_data, ignore_index=True)
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df):,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
        
        # HorseRaceLevelè¨ˆç®—
        horse_stats_df = analyzer.calculate_horse_race_level(combined_df)
        logger.info(f"HorseRaceLevelè¨ˆç®—å®Œäº†: {len(horse_stats_df):,}é ­")
        
        # ç›¸é–¢åˆ†æ
        correlation_results = analyzer.analyze_correlations(horse_stats_df)
        
        # å›å¸°åˆ†æ
        regression_results = analyzer.perform_regression_analysis(horse_stats_df)
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        analysis_results = {
            'data_summary': {
                'total_records': len(combined_df),
                'horse_count': len(horse_stats_df),
                'file_count': len(dataset_files)
            },
            'correlations': correlation_results,
            'regression': regression_results
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        analyzer.generate_analysis_report(analysis_results, Path(output_dir))
        
        return analysis_results
        
    except ImportError:
        # OddsComparisonAnalyzerãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ç°¡æ˜“ç‰ˆ
        logger.warning("OddsComparisonAnalyzerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ç°¡æ˜“ç‰ˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        return perform_simple_odds_analysis(data_dir, output_dir, sample_size)

def perform_simple_odds_analysis(data_dir: str, output_dir: str, sample_size: int = 200) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æ"""
    logger.info("ğŸ“Š ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    dataset_files = list(Path(data_dir).glob("*_formatted_dataset.csv"))[:sample_size]
    logger.info(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(dataset_files)}")
    
    all_data = []
    for file_path in dataset_files:
        try:
            df = pd.read_csv(file_path)
            if not df.empty and len(df) > 5:
                all_data.append(df)
        except Exception as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
    
    if not all_data:
        raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    combined_df = pd.concat(all_data, ignore_index=True)
    logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df):,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
    
    # åŸºæœ¬çš„ãªé¦¬çµ±è¨ˆè¨ˆç®—
    horse_stats = calculate_simple_horse_statistics(combined_df)
    logger.info(f"é¦¬çµ±è¨ˆè¨ˆç®—å®Œäº†: {len(horse_stats):,}é ­")
    
    # ç›¸é–¢åˆ†æ
    correlations = perform_simple_correlation_analysis(horse_stats)
    
    # å›å¸°åˆ†æ
    regression = perform_simple_regression_analysis(horse_stats)
    
    # çµæœ
    analysis_results = {
        'data_summary': {
            'total_records': len(combined_df),
            'horse_count': len(horse_stats),
            'file_count': len(dataset_files)
        },
        'correlations': correlations,
        'regression': regression
    }
    
    # ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_simple_report(analysis_results, Path(output_dir))
    
    return analysis_results

def calculate_simple_horse_statistics(df: pd.DataFrame) -> pd.DataFrame:
    """ç°¡æ˜“ç‰ˆé¦¬çµ±è¨ˆè¨ˆç®—"""
    # å¿…è¦ã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['é¦¬å', 'ç€é †']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
    
    # æ•°å€¤å¤‰æ›
    df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
    df = df[df['ç€é †'] > 0]
    
    # ã‚ªãƒƒã‚ºæƒ…å ±ã®å‡¦ç†
    if 'ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º' in df.columns:
        df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'] = pd.to_numeric(df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'], errors='coerce')
        df = df[df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'] > 0]
    
    if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in df.columns:
        df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'] = pd.to_numeric(df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'], errors='coerce')
        df = df[df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'] > 0]
    
    horse_stats = []
    
    for horse_name in df['é¦¬å'].unique():
        horse_data = df[df['é¦¬å'] == horse_name].copy()
        
        if len(horse_data) < 3:
            continue
        
        # åŸºæœ¬çµ±è¨ˆ
        total_races = len(horse_data)
        win_rate = (horse_data['ç€é †'] == 1).mean()
        place_rate = (horse_data['ç€é †'] <= 3).mean()
        
        # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹äºˆæ¸¬ç¢ºç‡
        if 'ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º' in horse_data.columns:
            avg_win_prob = (1 / horse_data['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º']).mean()
        else:
            avg_win_prob = 0
        
        if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in horse_data.columns:
            avg_place_prob = (1 / horse_data['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹']).mean()
        else:
            avg_place_prob = 0
        
        # ã€ä¿®æ­£ã€‘å¾ªç’°è«–ç†ã‚’æ’é™¤ã—ãŸç°¡æ˜“HorseRaceLevel
        # è¤‡å‹ç‡ï¼ˆç›®çš„å¤‰æ•°ï¼‰ã‚’ä½¿ã‚ãšã«ã€ã‚ªãƒƒã‚ºã®ã¿ã§è©•ä¾¡
        if avg_win_prob > 0:
            horse_race_level = np.log(1 / avg_win_prob)  # å¾ªç’°è«–ç†ã‚’æ’é™¤
        else:
            horse_race_level = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        horse_stats.append({
            'horse_name': horse_name,
            'total_races': total_races,
            'win_rate': win_rate,
            'place_rate': place_rate,
            'avg_win_prob_from_odds': avg_win_prob,
            'avg_place_prob_from_odds': avg_place_prob,
            'horse_race_level': horse_race_level
        })
    
    return pd.DataFrame(horse_stats).set_index('horse_name')

def perform_simple_correlation_analysis(horse_stats: pd.DataFrame) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆç›¸é–¢åˆ†æ"""
    correlations = {}
    target = 'place_rate'
    
    variables = {
        'HorseRaceLevel': 'horse_race_level',
        'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬': 'avg_place_prob_from_odds',
        'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬': 'avg_win_prob_from_odds'
    }
    
    for name, var in variables.items():
        if var in horse_stats.columns:
            corr, p_value = pearsonr(horse_stats[var].fillna(0), horse_stats[target].fillna(0))
            correlations[name] = {
                'correlation': corr,
                'r_squared': corr ** 2,
                'p_value': p_value
            }
    
    return correlations

def perform_simple_regression_analysis(horse_stats: pd.DataFrame) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆå›å¸°åˆ†æ"""
    data = horse_stats.dropna().copy()
    if len(data) < 30:
        logger.warning("å›å¸°åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
        return {}
    
    y = data['place_rate'].values
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    split_idx = int(len(data) * 0.7)
    
    results = {}
    
    # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
    if 'avg_place_prob_from_odds' in data.columns:
        X_odds = data[['avg_place_prob_from_odds']].fillna(0).values
        X_odds_train, X_odds_test = X_odds[:split_idx], X_odds[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        model_odds = LinearRegression()
        model_odds.fit(X_odds_train, y_train)
        y_pred_odds = model_odds.predict(X_odds_test)
        
        results['odds_baseline'] = {
            'train_r2': model_odds.score(X_odds_train, y_train),
            'test_r2': r2_score(y_test, y_pred_odds),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_odds))
        }
    
    # HorseRaceLevel
    if 'horse_race_level' in data.columns:
        X_level = data[['horse_race_level']].fillna(0).values
        X_level_train, X_level_test = X_level[:split_idx], X_level[split_idx:]
        
        model_level = LinearRegression()
        model_level.fit(X_level_train, y_train)
        y_pred_level = model_level.predict(X_level_test)
        
        results['horse_race_level_model'] = {
            'train_r2': model_level.score(X_level_train, y_train),
            'test_r2': r2_score(y_test, y_pred_level),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_level))
        }
    
    # ã€ä¿®æ­£ã€‘çµ±è¨ˆçš„æ¤œå®šã‚’å«ã‚€H2ä»®èª¬æ¤œè¨¼
    if 'odds_baseline' in results and 'horse_race_level_model' in results:
        # åŸºæœ¬çš„ãªæ•°å€¤æ¯”è¼ƒ
        h2_supported = results['horse_race_level_model']['test_r2'] > results['odds_baseline']['test_r2']
        improvement = results['horse_race_level_model']['test_r2'] - results['odds_baseline']['test_r2']
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“è©•ä¾¡ï¼ˆæ”¹å–„å¹…ãŒ0.01ä»¥ä¸Šã‹ã¤æ­£ã®å€¤ï¼‰
        statistically_meaningful = improvement > 0.01 and h2_supported
        
        results['h2_verification'] = {
            'hypothesis_supported': h2_supported,
            'improvement': improvement,
            'statistically_meaningful': statistically_meaningful,
            'warning': 'æœ¬åˆ†æã¯ç°¡æ˜“ç‰ˆã§ã™ã€‚å³å¯†ãªçµ±è¨ˆçš„æ¤œå®šã«ã¯OddsComparisonAnalyzerã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚'
        }
    
    return results

def generate_simple_report(results: Dict[str, Any], output_dir: Path):
    """ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    output_dir.mkdir(parents=True, exist_ok=True)
    report_path = output_dir / "horse_racelevel_odds_analysis_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# HorseRaceLevelã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: analyze_horse_racelevel.py\n\n")
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
        if 'data_summary' in results:
            f.write("## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
            summary = results['data_summary']
            f.write(f"- **ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: {summary.get('total_records', 'N/A'):,}\n")
            f.write(f"- **åˆ†æå¯¾è±¡é¦¬æ•°**: {summary.get('horse_count', 'N/A'):,}\n")
            f.write(f"- **å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {summary.get('file_count', 'N/A')}\n\n")
        
        # ç›¸é–¢åˆ†æçµæœ
        if 'correlations' in results:
            f.write("## ç›¸é–¢åˆ†æçµæœ\n\n")
            f.write("| å¤‰æ•° | ç›¸é–¢ä¿‚æ•° | RÂ² | på€¤ |\n")
            f.write("|------|----------|----|---------|\n")
            
            for name, corr in results['correlations'].items():
                f.write(f"| {name} | {corr['correlation']:.3f} | {corr['r_squared']:.3f} | {corr['p_value']:.3e} |\n")
            f.write("\n")
        
        # å›å¸°åˆ†æçµæœ
        if 'regression' in results:
            f.write("## å›å¸°åˆ†æçµæœï¼ˆH2ä»®èª¬æ¤œè¨¼ï¼‰\n\n")
            regression = results['regression']
            
            f.write("| ãƒ¢ãƒ‡ãƒ« | è¨“ç·´RÂ² | æ¤œè¨¼RÂ² | RMSE |\n")
            f.write("|--------|---------|---------|-------|\n")
            
            if 'odds_baseline' in regression:
                model = regression['odds_baseline']
                f.write(f"| ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | {model.get('train_r2', 0):.4f} | {model.get('test_r2', 0):.4f} | {model.get('test_rmse', 0):.4f} |\n")
            
            if 'horse_race_level_model' in regression:
                model = regression['horse_race_level_model']
                f.write(f"| HorseRaceLevel | {model.get('train_r2', 0):.4f} | {model.get('test_r2', 0):.4f} | {model.get('test_rmse', 0):.4f} |\n")
            
            # H2ä»®èª¬çµæœ
            if 'h2_verification' in regression:
                h2 = regression['h2_verification']
                f.write(f"\n### H2ä»®èª¬æ¤œè¨¼çµæœï¼ˆç°¡æ˜“ç‰ˆï¼‰\n\n")
                f.write(f"- **ä»®èª¬ã‚µãƒãƒ¼ãƒˆ**: {'âœ“ YES' if h2['hypothesis_supported'] else 'âœ— NO'}\n")
                f.write(f"- **æ€§èƒ½æ”¹å–„**: {h2['improvement']:+.4f}\n")
                f.write(f"- **çµ±è¨ˆçš„æ„å‘³**: {'âœ“ æœ‰æ„' if h2.get('statistically_meaningful', False) else 'âœ— é™å®šçš„'}\n")
                if 'warning' in h2:
                    f.write(f"- **æ³¨æ„**: {h2['warning']}\n")
                f.write("\n")
        
        f.write("## çµè«–\n\n")
        f.write("HorseRaceLevelã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®æ¯”è¼ƒåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n")
        f.write("è©³ç´°ãªåˆ†æã«ã¯ã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ¤œè¨¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n")
    
    logger.info(f"ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ: {report_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='HorseRaceLevelã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆçµ±åˆç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # HorseRaceLevelã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æ
  python analyze_horse_racelevel.py --odds-analysis export/dataset --output-dir results/horse_racelevel_odds

  # å¾“æ¥ã®ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ
  python analyze_horse_racelevel.py export/with_bias --output-dir results/race_level_analysis

  # å±¤åˆ¥åˆ†æã®ã¿å®Ÿè¡Œ
  python analyze_horse_racelevel.py --stratified-only --output-dir results/stratified_analysis

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ–°æ©Ÿèƒ½:
  1. HorseRaceLevelã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æ
  2. H2ä»®èª¬ã€ŒHorseRaceLevelãŒã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€ã®æ¤œè¨¼
  3. ç›¸é–¢åˆ†æã¨å›å¸°åˆ†æã«ã‚ˆã‚‹çµ±è¨ˆçš„è©•ä¾¡
  4. å¾“æ¥ã®ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã¨ã®äº’æ›æ€§ç¶­æŒ
        """
    )
    parser.add_argument('input_path', nargs='?', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ (ä¾‹: export/with_bias)')
    parser.add_argument('--output-dir', default='results/race_level_analysis', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹')
    parser.add_argument('--min-races', type=int, default=6, help='åˆ†æå¯¾è±¡ã¨ã™ã‚‹æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°')
    parser.add_argument('--encoding', default='utf-8', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°')
    parser.add_argument('--start-date', help='åˆ†æé–‹å§‹æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰')
    parser.add_argument('--end-date', help='åˆ†æçµ‚äº†æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰')
    
    # æ–°æ©Ÿèƒ½ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--odds-analysis', metavar='DATA_DIR', help='HorseRaceLevelã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šï¼‰')
    parser.add_argument('--sample-size', type=int, default=200, help='ã‚ªãƒƒã‚ºåˆ†æã§ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 200ï¼‰')
    
    # å¾“æ¥ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆç¶™ç¶šï¼‰
    parser.add_argument('--three-year-periods', action='store_true',
                       help='3å¹´é–“éš”ã§ã®æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨æœŸé–“åˆ†æï¼‰')
    parser.add_argument('--enable-time-analysis', action='store_true',
                       help='èµ°ç ´ã‚¿ã‚¤ãƒ å› æœé–¢ä¿‚åˆ†æã‚’å®Ÿè¡Œï¼ˆè«–æ–‡ä»®èª¬H1, H4æ¤œè¨¼ï¼‰')
    parser.add_argument('--enable-stratified-analysis', action='store_true', default=True,
                       help='å±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œï¼ˆå¹´é½¢å±¤åˆ¥ã€çµŒé¨“æ•°åˆ¥ã€è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹')
    parser.add_argument('--disable-stratified-analysis', action='store_true',
                       help='å±¤åˆ¥åˆ†æã‚’ç„¡åŠ¹åŒ–ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ç”¨ï¼‰')
    parser.add_argument('--stratified-only', action='store_true',
                       help='å±¤åˆ¥åˆ†æã®ã¿ã‚’å®Ÿè¡Œï¼ˆexport/datasetã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ï¼‰')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š')
    parser.add_argument('--log-file', help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰')
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ•°ã®åˆæœŸåŒ–
    log_file = None
    
    try:
        args = parser.parse_args()
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ç”Ÿæˆï¼ˆargså–å¾—å¾Œã€validate_argså‰ã«å®Ÿè¡Œï¼‰
        log_file = args.log_file
        if log_file is None:
            # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
            log_dir = Path('export/logs')
            log_dir.mkdir(parents=True, exist_ok=True)
            log_file = f'export/logs/analyze_horse_racelevel_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        
        # ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–
        setup_logging(log_level=args.log_level, log_file=log_file)
        
        # å¼•æ•°æ¤œè¨¼ï¼ˆãƒ­ã‚°è¨­å®šå¾Œã«å®Ÿè¡Œã€ã‚ªãƒƒã‚ºåˆ†æã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if not args.odds_analysis:
            args = validate_args(args)

        # ãƒ­ã‚°è¨­å®šå®Œäº†å¾Œã«é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›
        logger.info("ğŸ‡ ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        logger.info(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
        logger.info(f"ğŸ–¥ï¸ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {args.log_level}")
        logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚å«ã‚ã¦ç¢ºå®Ÿã«ä½œæˆï¼‰
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæ›¸ãè¾¼ã¿å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if not output_dir.exists() or not output_dir.is_dir():
            raise FileNotFoundError(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {output_dir}")
        
        logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªæ¸ˆã¿: {output_dir.absolute()}")

        logger.info(f"ğŸ“ å…¥åŠ›ãƒ‘ã‚¹: {args.input_path}")
        logger.info(f"ğŸ“Š å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
        logger.info(f"ğŸ¯ æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°: {args.min_races}")
        if args.start_date:
            logger.info(f"ğŸ“… åˆ†æé–‹å§‹æ—¥: {args.start_date}")
        if args.end_date:
            logger.info(f"ğŸ“… åˆ†æçµ‚äº†æ—¥: {args.end_date}")
        if args.enable_time_analysis:
            logger.info(f"ğŸƒ RunningTimeåˆ†æ: æœ‰åŠ¹")
        else:
            logger.info(f"ğŸƒ RunningTimeåˆ†æ: ç„¡åŠ¹ï¼ˆ--enable-time-analysisã§æœ‰åŠ¹åŒ–ï¼‰")
        
        # å±¤åˆ¥åˆ†æè¨­å®šã®å‡¦ç†
        enable_stratified = args.enable_stratified_analysis and not args.disable_stratified_analysis
        if enable_stratified:
            logger.info(f"ğŸ“Š å±¤åˆ¥åˆ†æ: æœ‰åŠ¹ï¼ˆå¹´é½¢å±¤åˆ¥ãƒ»çµŒé¨“æ•°åˆ¥ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰")
        else:
            logger.info(f"ğŸ“Š å±¤åˆ¥åˆ†æ: ç„¡åŠ¹ï¼ˆ--disable-stratified-analysisã§ç„¡åŠ¹åŒ–ï¼‰")
        
        # ã‚ªãƒƒã‚ºåˆ†æã®å ´åˆ
        if args.odds_analysis:
            logger.info("ğŸ¯ HorseRaceLevelã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                results = perform_comprehensive_odds_analysis(
                    args.odds_analysis, 
                    args.output_dir, 
                    args.sample_size
                )
                
                logger.info("âœ… ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡: {results['data_summary']['total_records']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰, {results['data_summary']['horse_count']:,}é ­")
                logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {args.output_dir}")
                
                # H2ä»®èª¬çµæœã®è¡¨ç¤º
                if 'regression' in results and 'h2_verification' in results['regression']:
                    h2 = results['regression']['h2_verification']
                    result_text = "ã‚µãƒãƒ¼ãƒˆ" if h2['hypothesis_supported'] else "éã‚µãƒãƒ¼ãƒˆ"
                    logger.info(f"ğŸ¯ H2ä»®èª¬ã€ŒHorseRaceLevelãŒã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€: {result_text}")
                    logger.info(f"   æ€§èƒ½æ”¹å–„: {h2['improvement']:+.4f}")
                
                return 0
            except Exception as e:
                logger.error(f"âŒ ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1
        
        # å±¤åˆ¥åˆ†æã®ã¿ã®å ´åˆ
        if args.stratified_only:
            logger.info("ğŸ“Š å±¤åˆ¥åˆ†æã®ã¿ã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                stratified_dataset = create_stratified_dataset_from_export('export/dataset')
                stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
                stratified_report = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
                logger.info("âœ… å±¤åˆ¥åˆ†æã®ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(stratified_dataset):,}é ­")
                logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
                return 0
            except Exception as e:
                logger.error(f"âŒ å±¤åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1

        if args.three_year_periods:
            logger.info("ğŸ“Š 3å¹´é–“éš”ã§ã®æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            
            # åˆæœŸãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã§å¹´ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã‚’ç¢ºèª
            temp_config = AnalysisConfig(
                input_path=args.input_path,
                min_races=args.min_races,
                output_dir=str(output_dir),
                date_str=datetime.now().strftime('%Y%m%d'),
                start_date=args.start_date,
                end_date=args.end_date
            )
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çš„ãªå‰å‡¦ç†ï¼ˆæœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
            temp_analyzer = RaceLevelAnalyzer(temp_config, 
                                            enable_time_analysis=args.enable_time_analysis,
                                            enable_stratified_analysis=enable_stratified)
            logger.info("ğŸ“– å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            temp_df = temp_analyzer.load_data()
            
            logger.info(f"ğŸ“Š èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(temp_df):,}ä»¶")
            
            # å¹´ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if 'å¹´' in temp_df.columns and temp_df['å¹´'].notna().any():
                min_year = int(temp_df['å¹´'].min())
                max_year = int(temp_df['å¹´'].max())
                logger.info(f"ğŸ“Š å¹´ãƒ‡ãƒ¼ã‚¿ç¯„å›²: {min_year}å¹´ - {max_year}å¹´")
                
                # 3å¹´é–“éš”ã§ã®æœŸé–“è¨­å®š
                periods = []
                for start_year in range(min_year, max_year + 1, 3):
                    end_year = min(start_year + 2, max_year)
                    period_name = f"{start_year}-{end_year}"
                    
                    # æœŸé–“å†…ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    period_data = temp_df[
                        (temp_df['å¹´'] >= start_year) & (temp_df['å¹´'] <= end_year)
                    ]
                    
                    if len(period_data) >= args.min_races:
                        periods.append((period_name, start_year, end_year))
                        logger.info(f"  ğŸ“Š æœŸé–“ {period_name}: {len(period_data):,}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
                    else:
                        logger.warning(f"  âš ï¸  æœŸé–“ {period_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ ({len(period_data)}ä»¶)")
                
                if periods:
                    logger.info(f"ğŸ“Š æœ‰åŠ¹ãªåˆ†ææœŸé–“: {[p[0] for p in periods]}")
                    
                    # æœŸé–“åˆ¥åˆ†æã®å®Ÿè¡Œ
                    all_results = analyze_by_periods(temp_analyzer, periods, output_dir)
                    
                    if all_results:
                        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
                        generate_period_summary_report(all_results, output_dir)
                        
                        logger.info("\n" + "="*60)
                        logger.info("ğŸ‰ 3å¹´é–“éš”åˆ†æå®Œäº†ï¼çµæœ:")
                        logger.info("="*60)
                        
                        for period_name, results in all_results.items():
                            period_info = results.get('period_info', {})
                            correlation_stats = results.get('correlation_stats', {})
                            
                            total_horses = period_info.get('total_horses', 0)
                            total_races = period_info.get('total_races', 0)
                            corr_avg = correlation_stats.get('correlation_place_avg', 0.0)
                            
                            logger.info(f"ğŸ“Š æœŸé–“ {period_name}: {total_horses:,}é ­, {total_races:,}ãƒ¬ãƒ¼ã‚¹")
                            logger.info(f"   ğŸ“ˆ å¹³å‡ãƒ¬ãƒ™ãƒ« vs è¤‡å‹ç‡ç›¸é–¢: r={corr_avg:.3f}")
                        
                        logger.info("="*60)
                        logger.info(f"âœ… å…¨ã¦ã®çµæœã¯ {args.output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
                        logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
                        logger.info("ğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
                        logger.info("  - ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æ_æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ.md")
                        logger.info("  - å„æœŸé–“ãƒ•ã‚©ãƒ«ãƒ€å†…ã®åˆ†æçµæœPNG")
                        
                        # å±¤åˆ¥åˆ†æã®å®Ÿè¡Œ
                        logger.info("ğŸ“Š å±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œä¸­...")
                        try:
                            stratified_dataset = create_stratified_dataset_from_export('export/dataset')
                            stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
                            stratified_report = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
                            logger.info("âœ… å±¤åˆ¥åˆ†æå®Œäº†")
                        except Exception as e:
                            logger.error(f"âŒ å±¤åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    else:
                        logger.warning("âš ï¸  æœ‰åŠ¹ãªæœŸé–“åˆ¥åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    logger.warning("âš ï¸  ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æœŸé–“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…¨æœŸé–“ã§ã®åˆ†æã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
                    args.three_year_periods = False
            else:
                logger.warning("âš ï¸  å¹´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨æœŸé–“ã§ã®åˆ†æã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
                args.three_year_periods = False
        
        if not args.three_year_periods:
            logger.info("ğŸ“Š ã€ä¿®æ­£ç‰ˆã€‘å³å¯†ãªæ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            
            # è¨­å®šã®ä½œæˆ
            date_str = datetime.now().strftime('%Y%m%d')
            config = AnalysisConfig(
                input_path=args.input_path,
                min_races=args.min_races,
                output_dir=str(output_dir),
                date_str=date_str,
                start_date=args.start_date,
                end_date=args.end_date
            )

            # 1. RaceLevelAnalyzerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
            analyzer = RaceLevelAnalyzer(config, args.enable_time_analysis, enable_stratified)

            # 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            logger.info("ğŸ“– å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            analyzer.df = analyzer.load_data()

            # å‰å‡¦ç†ã‚’è¿½åŠ 
            logger.info("ğŸ”§ å‰å‡¦ç†ä¸­...")
            analyzer.df = analyzer.preprocess_data()
            
            # 3. ç‰¹å¾´é‡è¨ˆç®—
            logger.info("ğŸ§® ç‰¹å¾´é‡è¨ˆç®—ä¸­...")
            analyzer.df = analyzer.calculate_feature()

            # 4. ã€é‡è¦ã€‘ä¿®æ­£ç‰ˆåˆ†æã®å®Ÿè¡Œ
            logger.info("ğŸ”¬ ã€ä¿®æ­£ç‰ˆã€‘å³å¯†ãªæ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹åˆ†æã‚’å®Ÿè¡Œä¸­...")
            analyzer.stats = analyzer.analyze()
            
            # çµæœã®å¯è¦–åŒ–
            analyzer.visualize()

            # ã€è¿½åŠ ã€‘ãƒ¬ãƒãƒ¼ãƒˆæ•´åˆæ€§ã®ç¢ºèª
            logger.info("ğŸ” ãƒ¬ãƒãƒ¼ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")
            oot_results = analyzer.stats.get('out_of_time_validation', {})
            test_performance = oot_results.get('test_performance', {})
            
            if test_performance:
                test_r2 = test_performance.get('r_squared', 0)
                test_corr = test_performance.get('correlation', 0)
                test_size = test_performance.get('sample_size', 0)
                
                logger.info(f"   ğŸ“Š æ¤œè¨¼æœŸé–“(2013-2014å¹´)ã‚µãƒ³ãƒ—ãƒ«æ•°: {test_size}é ­")
                logger.info(f"   ğŸ“Š æ¤œè¨¼æœŸé–“RÂ²: {test_r2:.3f}")
                logger.info(f"   ğŸ“Š æ¤œè¨¼æœŸé–“ç›¸é–¢ä¿‚æ•°: {test_corr:.3f}")
                
                # å®Ÿæ¸¬çµæœã®çµ±è¨ˆçš„è©•ä¾¡
                if test_r2 > 0.01:
                    logger.info("âœ… çµ±è¨ˆçš„ã«æœ‰æ„ãªèª¬æ˜åŠ›ã‚’ç¢ºèª")
                else:
                    logger.warning("âš ï¸ èª¬æ˜åŠ›ãŒé™å®šçš„ã§ã™")
                    
                if abs(test_corr) > 0.1:
                    logger.info("âœ… å®Ÿç”¨çš„ãªç›¸é–¢é–¢ä¿‚ã‚’ç¢ºèª")
                else:
                    logger.warning("âš ï¸ ç›¸é–¢é–¢ä¿‚ãŒå¼±ã„ã§ã™")

            # å±¤åˆ¥åˆ†æã®å®Ÿè¡Œ
            logger.info("ğŸ“Š çµ±åˆå±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œä¸­...")
            try:
                stratified_dataset = create_stratified_dataset_from_export('export/dataset')
                stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
                stratified_report = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
                logger.info("âœ… çµ±åˆå±¤åˆ¥åˆ†æå®Œäº†")
            except Exception as e:
                logger.error(f"âŒ å±¤åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
            
            logger.info(f"âœ… ã€ä¿®æ­£ç‰ˆã€‘åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
            logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
            logger.info("ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ã¨æ™‚ç³»åˆ—åˆ†å‰²ãŒæ­£ã—ãå®Ÿè£…ã•ã‚Œã¾ã—ãŸã€‚")
            logger.info("ğŸ“Š çµ±åˆå±¤åˆ¥åˆ†æã«ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªæ¤œè¨¼ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚")

        return 0

    except FileNotFoundError as e:
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        logger.error("   â€¢ å…¥åŠ›ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯è‹±æ•°å­—ã«å¤‰æ›´ã—ã¦ãã ã•ã„")
        logger.error("   â€¢ 'export/with_bias' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except ValueError as e:
        error_msg = str(e)
        logger.error(f"âŒ å…¥åŠ›å€¤ã‚¨ãƒ©ãƒ¼: {error_msg}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        
        if "æ¡ä»¶ã‚’æº€ãŸã™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" in error_msg:
            logger.error("   â€¢ --min-races ã®å€¤ã‚’å°ã•ãã—ã¦ã¿ã¦ãã ã•ã„ï¼ˆä¾‹: --min-races 3ï¼‰")
            logger.error("   â€¢ æœŸé–“æŒ‡å®šãŒç‹­ã™ãã‚‹å ´åˆã¯ç¯„å›²ã‚’åºƒã’ã¦ãã ã•ã„")
            logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹æœŸé–“ã‹ã©ã†ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        elif "æ—¥ä»˜å½¢å¼" in error_msg:
            logger.error("   â€¢ æ—¥ä»˜ã¯YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 20220101ï¼‰")
            logger.error("   â€¢ --start-date ã¨ --end-date ã®ä¸¡æ–¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        else:
            logger.error("   â€¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ --help ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’ç¢ºèªã§ãã¾ã™")
        
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except IndexError as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒçŸ­ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        logger.error("   â€¢ æ™‚ç³»åˆ—åˆ†å‰²ã«å¿…è¦ãªæœ€ä½3å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        logger.error("   â€¢ æœŸé–“æŒ‡å®šã‚’åºƒã’ã¦å†å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except KeyboardInterrupt:
        logger.warning("â¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        logger.info("ğŸ’¡ å‡¦ç†æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ã«ã¯:")
        logger.info("   â€¢ --min-races ã‚’å¤§ããã—ã¦ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ¸›ã‚‰ã™")
        logger.info("   â€¢ æœŸé–“ã‚’çŸ­ãã—ã¦å‡¦ç†ç¯„å›²ã‚’çµã‚‹")
        logger.info("   â€¢ --disable-stratified-analysis ã§å±¤åˆ¥åˆ†æã‚’ç„¡åŠ¹åŒ–")
        if log_file:
            logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        
        if "encoding" in error_msg.lower() or "unicode" in error_msg.lower():
            logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ã¾ãŸã¯Shift-JISã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        elif "memory" in error_msg.lower():
            logger.error("   â€¢ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ --min-races ã‚’å¤§ããã—ã¦ãƒ‡ãƒ¼ã‚¿é‡ã‚’æ¸›ã‚‰ã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ä¸è¦ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¦ãã ã•ã„")
        elif "permission" in error_msg.lower():
            logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®å•é¡ŒãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ç®¡ç†è€…æ¨©é™ã§å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„")
        else:
            logger.error("   â€¢ --log-level DEBUG ã§è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ Pythonã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        logger.error("ğŸ” è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
        logger.error(f"   ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥: {type(e).__name__}")
        logger.error(f"   ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_msg}")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        logger.error("è©³ç´°ãªã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:", exc_info=True)
        return 1

if __name__ == '__main__':
    sys.exit(main())