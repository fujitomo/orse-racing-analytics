#!/usr/bin/env python
"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹åˆ†æã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«
é¦¬ã”ã¨ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã®åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import argparse
from pathlib import Path
from datetime import datetime
import logging
import sys
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import time
import psutil
import os
from functools import wraps

# ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from horse_racing.data.data_loader import DataLoader, GLOBAL_DATA_CACHE
from horse_racing.analyzers.feature_calculator import FeatureCalculator
from horse_racing.analyzers.stratified_analyzer import StratifiedAnalyzer
from horse_racing.output.report_generator import ReportGenerator
from horse_racing.core.weight_manager import WeightManager, get_global_weights
from horse_racing.analyzers.odds_comparison_analyzer import OddsComparisonAnalyzer
from horse_racing.base.unified_analyzer import create_unified_analyzer
from horse_racing.services.reqi_initializer import REQIInitializer
from horse_racing.base.analyzer import AnalysisConfig as _AnalysisConfig
from horse_racing.analyzers.race_level_analyzer import REQIAnalyzer as _REQIAnalyzer
from horse_racing.data.utils import filter_by_date_range
from horse_racing.data.processors.grade_estimator import GradeEstimator
from horse_racing.services.data_loader_service import DataLoaderService
from horse_racing.utils.font_config import setup_japanese_fonts, apply_plot_style

def setup_logging(log_level='INFO', log_file=None):
    """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚

    Args:
        log_level (str): ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åï¼ˆä¾‹: ``INFO``ã€``DEBUG``ï¼‰ã€‚
        log_file (str | None): ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚``None`` ã®å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ã¿
            ï¼ˆãŸã ã— ``main`` å´ã§æ—¢å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼‰ã€‚
    """
    level = getattr(logging, str(log_level).upper(), logging.INFO)
    config = {
        'level': level,
        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        'force': True,
    }

    if log_file:
        # æŒ‡å®šãŒã‚ã‚‹å ´åˆã¯ãã®ãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚‚è¡Œã†ï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä½µç”¨ï¼‰
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        config['handlers'] = [
            logging.StreamHandler(),
            logging.FileHandler(log_file, encoding='utf-8'),
        ]

    logging.basicConfig(**config)

logger = logging.getLogger(__name__)

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆunified_analyzer ãŒå‚ç…§ï¼‰
AnalysisConfig = _AnalysisConfig
REQIAnalyzer = _REQIAnalyzer

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ç¾¤ï¼ˆGLOBAL_DATA_CACHEã‚’ç›´æ¥ä½¿ç”¨ï¼‰
def cache_raw_data(df: pd.DataFrame, copy: bool = True) -> None:
    """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã™ï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰ã€‚
    
    Args:
        df (pd.DataFrame): ä¿å­˜å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        copy (bool): ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã™ã‚‹ã‹ã©ã†ã‹ã€‚
    """
    GLOBAL_DATA_CACHE.set_raw_data(df, copy=copy)


def cache_combined_data(df: pd.DataFrame, copy: bool = True) -> None:
    """çµ±åˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã™ï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰ã€‚
    
    Args:
        df (pd.DataFrame): ä¿å­˜å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        copy (bool): ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã™ã‚‹ã‹ã©ã†ã‹ã€‚
    """
    GLOBAL_DATA_CACHE.set_combined_data(df, copy=copy)


def cache_feature_levels(df: pd.DataFrame, copy: bool = True) -> None:
    """ç‰¹å¾´é‡è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã™ï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰ã€‚
    
    Args:
        df (pd.DataFrame): ä¿å­˜å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        copy (bool): ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã™ã‚‹ã‹ã©ã†ã‹ã€‚
    """
    GLOBAL_DATA_CACHE.set_feature_levels(df, copy=copy)


def get_cached_raw_data(copy: bool = True) -> Optional[pd.DataFrame]:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ã„ã‚‹ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰ã€‚
    
    Args:
        copy (bool): ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™ã‹ã©ã†ã‹ã€‚
        
    Returns:
        Optional[pd.DataFrame]: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸç”Ÿãƒ‡ãƒ¼ã‚¿ã€‚
    """
    return GLOBAL_DATA_CACHE.get_raw_data(copy=copy)


def get_cached_combined_data(copy: bool = True) -> Optional[pd.DataFrame]:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ã„ã‚‹çµ±åˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰ã€‚
    
    Args:
        copy (bool): ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™ã‹ã©ã†ã‹ã€‚
        
    Returns:
        Optional[pd.DataFrame]: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµ±åˆãƒ‡ãƒ¼ã‚¿ã€‚
    """
    return GLOBAL_DATA_CACHE.get_combined_data(copy=copy)


def get_cached_feature_levels(copy: bool = True) -> Optional[pd.DataFrame]:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ã„ã‚‹ç‰¹å¾´é‡è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰ã€‚
    
    Args:
        copy (bool): ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™ã‹ã©ã†ã‹ã€‚
        
    Returns:
        Optional[pd.DataFrame]: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã€‚
    """
    return GLOBAL_DATA_CACHE.get_feature_levels(copy=copy)

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
def log_performance(func_name=None):
    """æŒ‡å®šã—ãŸé–¢æ•°ã®å®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã€‚

    Args:
        func_name (str | None): ãƒ­ã‚°ã«è¡¨ç¤ºã™ã‚‹åå‰ã€‚``None`` ã®å ´åˆã¯å¯¾è±¡é–¢æ•°ã®
            ``__name__`` ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

    Returns:
        Callable: å®Ÿè¡Œæ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ»CPU ä½¿ç”¨ç‡ã‚’è¨˜éŒ²ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ã€‚
    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            name = func_name or func.__name__
            process = psutil.Process(os.getpid())
            start_time = time.time()
            start_memory_mb = process.memory_info().rss / 1024 / 1024
            start_cpu_percent = process.cpu_percent()

            logger.info(
                f"ğŸš€ [{name}] é–‹å§‹ - é–‹å§‹æ™‚ãƒ¡ãƒ¢ãƒª: {start_memory_mb:.1f}MB, CPU: {start_cpu_percent:.1f}%"
            )

            error_occurred = False
            try:
                result = func(*args, **kwargs)
                return result
            except Exception:
                error_occurred = True
                logger.error(
                    f"âŒ [{name}] ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - å®Ÿè¡Œæ™‚é–“: {time.time() - start_time:.2f}ç§’"
                )
                raise
            finally:
                end_memory_mb = process.memory_info().rss / 1024 / 1024
                end_cpu_percent = process.cpu_percent()
                execution_time = time.time() - start_time
                memory_diff = end_memory_mb - start_memory_mb

                if not error_occurred:
                    if memory_diff > 500:
                        logger.warning(
                            f"âš ï¸ [{name}] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ500MBå¢—åŠ ã—ã¾ã—ãŸ: {memory_diff:+.1f}MB"
                        )
                    elif memory_diff > 200:
                        logger.warning(
                            f"âš ï¸ [{name}] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ200MBå¢—åŠ ã—ã¾ã—ãŸ: {memory_diff:+.1f}MB"
                        )

                    logger.info(f"âœ… [{name}] å®Œäº† - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
                    logger.info(
                        f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {end_memory_mb:.1f}MB (å·®åˆ†: {memory_diff:+.1f}MB)"
                    )
                    logger.info(f"   ğŸ–¥ï¸  CPUä½¿ç”¨ç‡: {end_cpu_percent:.1f}%")

                    if execution_time > 60:
                        logger.warning(
                            f"âš ï¸ [{name}] å®Ÿè¡Œæ™‚é–“ãŒ1åˆ†ã‚’è¶…ãˆã¾ã—ãŸ: {execution_time:.2f}ç§’"
                        )
                else:
                    logger.info(
                        f"   ğŸ’¾ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ãƒ¡ãƒ¢ãƒª: {end_memory_mb:.1f}MB (å·®åˆ†: {memory_diff:+.1f}MB)"
                    )
                    logger.info(f"   ğŸ–¥ï¸  ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚CPUä½¿ç”¨ç‡: {end_cpu_percent:.1f}%")

        return wrapper

    return decorator

def log_dataframe_info(df: pd.DataFrame, description: str) -> None:
    """DataFrame ã®åŸºæœ¬çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã¾ã™ã€‚

    Args:
        df (pd.DataFrame): å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        description (str): ãƒ­ã‚°ã«ä½µè¨˜ã™ã‚‹æ¦‚è¦èª¬æ˜ã€‚
    """
    memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
    logger.info(f"ğŸ“Š [{description}] ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±:")
    logger.info(f"   ğŸ“ å½¢çŠ¶: {df.shape[0]:,}è¡Œ Ã— {df.shape[1]}åˆ—")
    logger.info(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage:.1f}MB")
    logger.info(f"   ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å‹åˆ†å¸ƒ: {dict(df.dtypes.value_counts())}")
    
    # æ¬ æå€¤æƒ…å ±
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        logger.info(f"   âš ï¸ æ¬ æå€¤: {null_counts.sum():,}å€‹ ({null_counts.sum()/df.size*100:.1f}%)")
        try:
            # åˆ—åˆ¥ãƒˆãƒƒãƒ—Nã®æ¬ æå†…è¨³
            missing_counts_sorted = null_counts.sort_values(ascending=False)
            missing_pct_sorted = (missing_counts_sorted / len(df) * 100).round(1)
            top_n = 15
            top_missing = (
                pd.concat([
                    missing_counts_sorted.rename('count'),
                    missing_pct_sorted.rename('%')
                ], axis=1)
                .head(top_n)
            )
            if len(top_missing) > 0:
                logger.info("   ğŸ” æ¬ æãƒˆãƒƒãƒ—15(åˆ—):\n" + top_missing.to_string())
            
            # å¹´åˆ¥Ã—ä¸»è¦åˆ—ã®æ¬ æç‡
            key_cols = ['ã‚°ãƒ¬ãƒ¼ãƒ‰', '10æ™‚å˜å‹ã‚ªãƒƒã‚º', '10æ™‚è¤‡å‹ã‚ªãƒƒã‚º', 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹', 'é¨æ‰‹ã‚³ãƒ¼ãƒ‰']
            available_key_cols = [c for c in key_cols if c in df.columns]
            if 'å¹´' in df.columns and len(available_key_cols) > 0:
                year_missing = (
                    df.groupby('å¹´')[available_key_cols]
                      .apply(lambda x: x.isnull().mean().mul(100).round(1))
                )
                logger.info("   ğŸ” å¹´åˆ¥Ã—ä¸»è¦åˆ— æ¬ æç‡(%):\n" + year_missing.to_string())
        except Exception as e:
            logger.warning(f"   âš ï¸ æ¬ æè©³ç´°ãƒ­ã‚°ã®ç”Ÿæˆä¸­ã«ä¾‹å¤–: {str(e)}")
    
def log_processing_step(step_name: str, start_time: float, current_idx: int, total_count: int) -> None:
    """å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®é€²æ—çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã¾ã™ã€‚

    Args:
        step_name (str): ã‚¹ãƒ†ãƒƒãƒ—åã€‚
        start_time (float): ã‚¹ãƒ†ãƒƒãƒ—é–‹å§‹æ™‚åˆ»ï¼ˆ``time.time()``ï¼‰ã€‚
        current_idx (int): ç¾åœ¨ã®å‡¦ç†ä»¶æ•°ã€‚
        total_count (int): ç·ä»¶æ•°ã€‚
    """
    elapsed = time.time() - start_time
    if current_idx > 0:
        avg_time_per_item = elapsed / current_idx
        remaining_items = total_count - current_idx
        eta = remaining_items * avg_time_per_item

        logger.info(f"â³ [{step_name}] é€²æ—: {current_idx:,}/{total_count:,} "
                   f"({current_idx/total_count*100:.1f}%) - "
                   f"çµŒéæ™‚é–“: {elapsed:.1f}ç§’, æ®‹ã‚Šäºˆæƒ³: {eta:.1f}ç§’")

def log_system_resources() -> None:
    """ãƒ—ãƒ­ã‚»ã‚¹ãŠã‚ˆã³ã‚·ã‚¹ãƒ†ãƒ ã®ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã¾ã™ã€‚"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    cpu_percent = process.cpu_percent()
    
    # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æƒ…å ±
    system_memory = psutil.virtual_memory()
    system_cpu = psutil.cpu_percent()
    
    logger.info("ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³:")
    logger.info(f"   ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª: {memory_info.rss/1024/1024:.1f}MB")
    logger.info(f"   ãƒ—ãƒ­ã‚»ã‚¹CPU: {cpu_percent:.1f}%")
    logger.info(f"   ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {system_memory.percent:.1f}% "
               f"({system_memory.used/1024/1024/1024:.1f}GB/{system_memory.total/1024/1024/1024:.1f}GB)")
    logger.info(f"   ã‚·ã‚¹ãƒ†ãƒ CPUä½¿ç”¨ç‡: {system_cpu:.1f}%")

def get_all_dataset_files(data_dir: str) -> List[Path]:
    """æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ CSV ã®ä¸€è¦§ã‚’å–å¾—ã™ã‚‹ã€‚

    Args:
        data_dir (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ ¼ç´ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚

    Returns:
        List[pathlib.Path]: ãƒãƒƒãƒã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã€‚
    """
    data_path = Path(data_dir)
    if not data_path.exists():
        return []
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
    csv_files = list(data_path.glob('SED*_formatted_dataset.csv'))
    return sorted(csv_files)

def load_all_data_once(input_path: str, encoding: str = 'utf-8') -> pd.DataFrame:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚

    Args:
        input_path (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã€ã¾ãŸã¯ãã‚Œã‚‰ã‚’å«ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        encoding (str): èª­ã¿è¾¼ã¿æ™‚ã«ä½¿ç”¨ã™ã‚‹æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€‚

    Returns:
        pd.DataFrame: å…¥åŠ›ã‚½ãƒ¼ã‚¹ã‚’çµåˆã—ãŸç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """
    loader = DataLoader(cache=GLOBAL_DATA_CACHE)
    return loader.load_csv_files(input_path, encoding, use_cache=True)

def initialize_global_weights(args) -> bool:
    """REQI ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’åˆæœŸåŒ–ã—ã€é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚

    Args:
        args (argparse.Namespace): å…¥åŠ›ãƒ‘ã‚¹ã‚„åˆ†æãƒ¢ãƒ¼ãƒ‰ã€ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’å«ã‚€
            ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã€‚

    Returns:
        bool: åˆæœŸåŒ–ã«æˆåŠŸã—ãŸå ´åˆã¯ Trueã€‚å¤±æ•—ã—ãŸå ´åˆã¯ Falseã€‚
    """
    try:
        logger.info("ğŸ¯ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–é–‹å§‹...")
        initializer = REQIInitializer()
        success = initializer.initialize_from_args(
            args,
            feature_calc_func=calculate_accurate_feature_levels,
            reqi_calc_func=calculate_race_level_features_with_position_weights,
        )
        if success:
            logger.info("âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–å®Œäº†")
        else:
            logger.warning("âš ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return success
    except Exception as e:
        logger.error(f"âŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("è©³ç´°:", exc_info=True)
        return False



def validate_date(date_str: str) -> datetime:
    """``YYYYMMDD`` å½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’æ¤œè¨¼ã—ã¦å¤‰æ›ã™ã‚‹ã€‚

    Args:
        date_str (str): æ¤œè¨¼å¯¾è±¡ã®æ—¥ä»˜æ–‡å­—åˆ—ã€‚

    Returns:
        datetime: å¤‰æ›å¾Œã® ``datetime`` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚

    Raises:
        ValueError: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒä¸æ­£ãªå ´åˆã€‚
    """
    try:
        return datetime.strptime(date_str, '%Y%m%d')
    except ValueError:
        raise ValueError(f"ç„¡åŠ¹ãªæ—¥ä»˜å½¢å¼ã§ã™: {date_str}ã€‚YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

def validate_args(args):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’æ¤œè¨¼ã—ã€å¿…è¦ã«å¿œã˜ã¦è£œå®Œã™ã‚‹ã€‚

    Args:
        args (argparse.Namespace): ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿å¼•æ•°ã€‚

    Returns:
        argparse.Namespace: æ¤œè¨¼æ¸ˆã¿ï¼ˆè£œå®Œå¾Œï¼‰ã®å¼•æ•°ã€‚

    Raises:
        FileNotFoundError: æŒ‡å®šã•ã‚ŒãŸå…¥åŠ›ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã€‚
        ValueError: æœŸé–“æŒ‡å®šãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸æ­£ãªå ´åˆã€‚
    """
    input_path = Path(args.input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_path}")
    
    if args.min_races < 1:
        raise ValueError("æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°ã¯1ä»¥ä¸Šã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    # æ—¥ä»˜ç¯„å›²ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args.start_date:
        start_date = validate_date(args.start_date)
    else:
        start_date = None
        
    if args.end_date:
        end_date = validate_date(args.end_date)
        if start_date and end_date < start_date:
            raise ValueError("çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    else:
        end_date = None
    
    return args



@log_performance("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
def create_stratified_dataset_from_export(dataset_dir: str, min_races: int = 6, start_date: Optional[str] = None, end_date: Optional[str] = None) -> pd.DataFrame:
    """å±¤åˆ¥åˆ†æå‘ã‘ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

    Args:
        dataset_dir (str): ``*_formatted_dataset.csv`` ã‚’æ ¼ç´ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
        min_races (int): é¦¬ã‚’æ®‹ã™ãŸã‚ã®æœ€ä½å‡ºèµ°å›æ•°ã€‚
        start_date (str | None): ``YYYYMMDD`` å½¢å¼ã®ä¸‹é™æ—¥ä»˜ã€‚
        end_date (str | None): ``YYYYMMDD`` å½¢å¼ã®ä¸Šé™æ—¥ä»˜ã€‚

    Returns:
        pd.DataFrame: ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ãƒ¬ãƒ¼ã‚¹ã« REQI æŒ‡æ¨™ã‚’ä»˜ä¸ã—ãŸãƒ‡ãƒ¼ã‚¿ã€‚
    """
    logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿é–‹å§‹: {dataset_dir}")
    
    dataset_path = Path(dataset_dir)
    if not dataset_path.exists():
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_dir}")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    csv_files = list(dataset_path.glob("*_formatted_dataset.csv"))
    logger.info(f"ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files)}")
    
    if len(csv_files) == 0:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    dfs = []
    file_read_start = time.time()
    for i, file_path in enumerate(csv_files):
        try:
            file_start = time.time()
            df = pd.read_csv(file_path, encoding='utf-8')
            file_time = time.time() - file_start
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
            file_size = file_path.stat().st_size / 1024 / 1024  # MB
            read_speed = file_size / file_time if file_time > 0 else 0
            
            # èŠãƒ¬ãƒ¼ã‚¹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            if 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰' in df.columns:
                df = df[df['èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰'] == 'èŠ']
            dfs.append(df)
            
            if (i + 1) % 100 == 0:
                log_processing_step("ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿", file_read_start, i + 1, len(csv_files))
            
            # è©³ç´°ãƒ­ã‚°ï¼ˆæœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            if i < 10:
                logger.debug(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ« {i+1}: {file_path.name} - "
                           f"ã‚µã‚¤ã‚º: {file_size:.1f}MB, èª­ã¿è¾¼ã¿æ™‚é–“: {file_time:.2f}ç§’, "
                           f"é€Ÿåº¦: {read_speed:.1f}MB/s, è¡Œæ•°: {len(df):,}")
                
        except Exception as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {file_path.name} - {e}")
    
    if not dfs:
        raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
    
    logger.info("ğŸ”— ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ çµ±åˆä¸­...")
    concat_start = time.time()
    unified_df = pd.concat(dfs, ignore_index=True)
    # æŒ‡å®šãŒã‚ã‚Œã°æ—¥ä»˜ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿
    unified_df = filter_by_date_range(unified_df, start_date, end_date)
    concat_time = time.time() - concat_start
    
    logger.info(f"âœ… çµ±åˆå®Œäº†: {len(unified_df):,}è¡Œã®ãƒ‡ãƒ¼ã‚¿ (çµ±åˆæ™‚é–“: {concat_time:.2f}ç§’)")
    logger.info(f"   æœŸé–“: {unified_df['å¹´'].min()}-{unified_df['å¹´'].max()}")
    logger.info(f"   é¦¬æ•°: {unified_df['é¦¬å'].nunique():,}é ­")
    log_dataframe_info(unified_df, "çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    
    # REQIç‰¹å¾´é‡ã®ç®—å‡ºï¼ˆç€é †é‡ã¿ä»˜ãå¯¾å¿œï¼‰
    df_with_levels = calculate_race_level_features_with_position_weights(unified_df)
    
    # é¦¬ã”ã¨ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰çµ±è¨ˆç®—å‡º
    logger.info("ğŸ é¦¬ã”ã¨ã®çµ±è¨ˆè¨ˆç®—é–‹å§‹...")
    
    # ã€æœ€é©åŒ–ã€‘å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯é«˜é€Ÿç‰ˆã‚’ä½¿ç”¨
    if len(df_with_levels) > 50000:  # 5ä¸‡ãƒ¬ãƒ¼ã‚¹ä»¥ä¸Šã®å ´åˆ
        logger.info("ğŸ“Š å¤§é‡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º - é«˜é€Ÿçµ±è¨ˆè¨ˆç®—ã‚’ä½¿ç”¨")
        analysis_df = calculate_horse_stats_vectorized_stratified(df_with_levels, min_races)
    else:
        # å¾“æ¥ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†ï¼ˆå°‘é‡ãƒ‡ãƒ¼ã‚¿å‘ã‘ï¼‰
        horse_stats = []
        unique_horses = df_with_levels['é¦¬å'].unique()
        horse_calc_start = time.time()
        
        for i, horse_name in enumerate(unique_horses):
            horse_data = df_with_levels[df_with_levels['é¦¬å'] == horse_name]
            
            if len(horse_data) < min_races:
                continue
            
            # åŸºæœ¬çµ±è¨ˆ
            total_races = len(horse_data)
            # å‹ç‡ã‚’ wins/starts ã§å³å¯†å®šç¾©ï¼ˆå–æ¶ˆãƒ»é™¤å¤–ãƒ»ä¸­æ­¢ãªã©ã®éæ•°å€¤ã¯åˆ†æ¯ã«å«ã‚ãªã„ï¼‰
            s = pd.to_numeric(horse_data['ç€é †'], errors='coerce')
            wins = (s == 1).sum()
            starts = s.notna().sum()
            win_rate = (wins / starts) if starts > 0 else np.nan
            place_rate = (horse_data['ç€é †'] <= 3).mean()
            
            # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç®—å‡ºï¼ˆç€é †é‡ã¿ä»˜ãï¼‰
            avg_race_level = horse_data['race_level'].mean()
            max_race_level = horse_data['race_level'].max()
            
            # å¹´é½¢æ¨å®šï¼ˆåˆå‡ºèµ°å¹´ãƒ™ãƒ¼ã‚¹ï¼‰
            first_year = horse_data['å¹´'].min()
            last_year = horse_data['å¹´'].max()
            estimated_age = last_year - first_year + 2  # 2æ­³ãƒ‡ãƒ“ãƒ¥ãƒ¼æƒ³å®š
            
            # ä¸»æˆ¦è·é›¢
            main_distance = horse_data['è·é›¢'].mode().iloc[0] if len(horse_data['è·é›¢'].mode()) > 0 else horse_data['è·é›¢'].mean()
            
            horse_stats.append({
                'é¦¬å': horse_name,
                'å‡ºèµ°å›æ•°': total_races,
                'å‹ç‡': win_rate,
                'è¤‡å‹ç‡': place_rate,
                'å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰': avg_race_level,
                'æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰': max_race_level,
                'åˆå‡ºèµ°å¹´': first_year,
                'æœ€çµ‚å‡ºèµ°å¹´': last_year,
                'æ¨å®šå¹´é½¢': estimated_age,
                'ä¸»æˆ¦è·é›¢': main_distance
            })
                
            # é€²æ—ãƒ­ã‚°ï¼ˆ1000é ­ã”ã¨ï¼‰
            if (i + 1) % 1000 == 0:
                log_processing_step("é¦¬çµ±è¨ˆè¨ˆç®—", horse_calc_start, i + 1, len(unique_horses))
        
        analysis_df = pd.DataFrame(horse_stats)
    
    # å±¤åˆ¥ã‚«ãƒ†ã‚´ãƒªã®ä½œæˆ
    analysis_df = create_stratification_categories(analysis_df)
    
    logger.info(f"âœ… ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(analysis_df)}é ­")
    logger.info(f"   å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç¯„å›²: {analysis_df['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].min():.3f} - {analysis_df['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].max():.3f}")
    
    return analysis_df

def calculate_horse_stats_vectorized_stratified(df: pd.DataFrame, min_races: int) -> pd.DataFrame:
    """å±¤åˆ¥åˆ†æç”¨é¦¬çµ±è¨ˆã‚’é«˜é€Ÿè¨ˆç®—ã—ã¾ã™ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ï¼‰ã€‚
    
    Args:
        df (pd.DataFrame): ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã€‚
        min_races (int): æœ€ä½å‡ºèµ°å›æ•°ã€‚
        
    Returns:
        pd.DataFrame: é¦¬ã”ã¨ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã€‚
    """
    logger.info("ğŸš€ é«˜é€Ÿé¦¬çµ±è¨ˆè¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
    
    # è¤‡å‹ãƒ•ãƒ©ã‚°ä½œæˆ
    df['place_flag'] = (df['ç€é †'] <= 3).astype(int)
    df['win_flag'] = (df['ç€é †'] == 1).astype(int)
    
    # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’groupbyã§ä¸€æ‹¬è¨ˆç®—
    horse_stats = df.groupby('é¦¬å').agg({
        'race_level': ['mean', 'max'],
        'place_flag': 'mean',
        'win_flag': 'mean',
        'é¦¬å': 'count',  # total_races
        'å¹´': ['min', 'max'],
        'è·é›¢': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.mean()
    }).round(6)
    
    # ã‚«ãƒ©ãƒ åã‚’å¹³å¦åŒ–
    horse_stats.columns = ['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰', 'æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰', 'è¤‡å‹ç‡', 'å‹ç‡', 
                          'å‡ºèµ°å›æ•°', 'åˆå‡ºèµ°å¹´', 'æœ€çµ‚å‡ºèµ°å¹´', 'ä¸»æˆ¦è·é›¢']
    
    # æ¨å®šå¹´é½¢è¨ˆç®—
    horse_stats['æ¨å®šå¹´é½¢'] = horse_stats['æœ€çµ‚å‡ºèµ°å¹´'] - horse_stats['åˆå‡ºèµ°å¹´'] + 2
    
    # æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿
    horse_stats = horse_stats[horse_stats['å‡ºèµ°å›æ•°'] >= min_races]
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¦¬åã‚«ãƒ©ãƒ ã«å¤‰æ›
    horse_stats = horse_stats.reset_index()
    
    logger.info(f"âœ… é«˜é€Ÿçµ±è¨ˆè¨ˆç®—å®Œäº†: {len(horse_stats)}é ­")
    
    return horse_stats

def calculate_race_level_features_fast(df: pd.DataFrame) -> pd.DataFrame:
    """REQIç‰¹å¾´é‡ã‚’é«˜é€Ÿç®—å‡ºã—ã¾ã™ï¼ˆç°¡æ˜“é‡ã¿ä»˜ã‘å‡¦ç†ï¼‰ã€‚
    
    Args:
        df (pd.DataFrame): ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã€‚
        
    Returns:
        pd.DataFrame: REQIç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ã€‚
    """
    logger.info("ğŸš€ é«˜é€ŸREQIç®—å‡ºã‚’å®Ÿè¡Œä¸­...")
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ç®—å‡ºï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    def get_grade_level_vectorized(df):
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã‚’ç®—å‡ºï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
        
        ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿ã®ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ã¯ã€Œå°ã•ã„ã»ã©é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã¨ã„ã†é–¢ä¿‚
        - 1 = G1ï¼ˆæœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ â†’ 3.0ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
        - 2 = G2 â†’ 2.5
        - 3 = G3 â†’ 2.0
        - 4 = é‡è³ â†’ 1.5
        - 5 = ç‰¹åˆ¥ï¼ˆä½ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ â†’ 1.0ï¼ˆä½ãƒ¬ãƒ™ãƒ«ï¼‰
        - 6 = ãƒªã‚¹ãƒ†ãƒƒãƒ‰ â†’ 1.2
        """
        # ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        grade_col = None
        for col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade']:
            if col in df.columns:
                grade_col = col
                break
        
        if grade_col is None:
            # è³é‡‘ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            prize_col = None
            for col in ['1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', '1ç€è³é‡‘', 'æœ¬è³é‡‘']:
                if col in df.columns:
                    prize_col = col
                    break
            
            if prize_col is None:
                logger.warning("âš ï¸ ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ»è³é‡‘ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
                return np.ones(len(df)) * 1.0
            
            # è³é‡‘ãƒ™ãƒ¼ã‚¹ã®å‡¦ç†ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜ã«åŸºã¥ãæ­£ã—ã„ã—ãã„å€¤ï¼‰
            prizes = pd.to_numeric(df[prize_col], errors='coerce').fillna(0)
            result = np.ones(len(prizes)) * 0.5
            result[prizes >= 1650] = 3.0  # G1: 1,650ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 855) & (prizes < 1650)] = 2.5  # G2: 855ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 570) & (prizes < 855)] = 2.0  # G3: 570ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 300) & (prizes < 570)] = 1.5  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰: 300ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 120) & (prizes < 300)] = 1.0  # ç‰¹åˆ¥: 120ä¸‡å††ä»¥ä¸Š
            return result
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ã‚’å¤‰æ›
        # ãƒ‡ãƒ¼ã‚¿ã¯ã€Œ1=æœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ãªã®ã§ã€ãã®ã¾ã¾ãƒãƒƒãƒ”ãƒ³ã‚°
        grades = pd.to_numeric(df[grade_col], errors='coerce').fillna(5)
        result = np.ones(len(grades)) * 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        result[grades == 1] = 3.0  # G1ï¼ˆæœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ â†’ æœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
        result[grades == 2] = 2.5  # G2
        result[grades == 3] = 2.0  # G3
        result[grades == 4] = 1.5  # é‡è³
        result[grades == 5] = 1.0  # ç‰¹åˆ¥ï¼ˆä½ã‚°ãƒ¬ãƒ¼ãƒ‰ â†’ ä½ãƒ¬ãƒ™ãƒ«ï¼‰
        result[grades == 6] = 1.2  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰
        
        return result
    
    # è·é›¢ãƒ¬ãƒ™ãƒ«ã®ç®—å‡ºï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    def get_distance_level_vectorized(df):
        # è·é›¢ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        distance_col = None
        for col in ['è·é›¢', 'distance', 'ãƒ¬ãƒ¼ã‚¹è·é›¢']:
            if col in df.columns:
                distance_col = col
                break
        
        if distance_col is None:
            logger.warning("âš ï¸ è·é›¢ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            return np.ones(len(df)) * 1.0
        
        distances = pd.to_numeric(df[distance_col], errors='coerce').fillna(1600)
        result = np.ones(len(distances))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1.0
        
        result[(distances >= 2400)] = 1.3  # é•·è·é›¢
        result[(distances >= 2000) & (distances < 2400)] = 1.2  # ä¸­é•·è·é›¢
        result[(distances >= 1800) & (distances < 2000)] = 1.1  # ä¸­è·é›¢
        result[(distances < 1200)] = 0.9  # çŸ­è·é›¢
        
        return result
    
    # å‡ºèµ°é ­æ•°ãƒ¬ãƒ™ãƒ«ã®ç®—å‡ºï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    def get_field_size_level_vectorized(df):
        # å‡ºèµ°é ­æ•°ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        field_size_col = None
        for col in ['é ­æ•°_x', 'å‡ºèµ°é ­æ•°', 'field_size', 'é ­æ•°', 'å‡ºèµ°æ•°']:
            if col in df.columns:
                field_size_col = col
                break
        
        if field_size_col is None:
            logger.warning("âš ï¸ å‡ºèµ°é ­æ•°ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            return np.ones(len(df)) * 1.0
        
        field_sizes = pd.to_numeric(df[field_size_col], errors='coerce').fillna(12)
        result = np.ones(len(field_sizes))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1.0
        
        result[field_sizes >= 16] = 1.2  # å¤§è¦æ¨¡
        result[(field_sizes >= 12) & (field_sizes < 16)] = 1.1  # ä¸­è¦æ¨¡
        result[field_sizes < 8] = 0.9  # å°è¦æ¨¡
        
        return result
    
    # venue_levelã®ç®—å‡ºï¼ˆé€šå¸¸ç‰ˆã¨çµ±ä¸€ï¼‰
    def get_venue_level_vectorized(df):
        """venue_levelã‚’ç®—å‡ºï¼ˆé€šå¸¸ç‰ˆã¨çµ±ä¸€ã—ãŸæ–¹æ³•ï¼‰"""
        # é€šå¸¸ç‰ˆã¨åŒã˜venue_levelç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
        if 'å ´ã‚³ãƒ¼ãƒ‰' in df.columns:
            # å ´ã‚³ãƒ¼ãƒ‰ã‹ã‚‰åˆ¤å®šï¼ˆæ›¸ç±å¼•ç”¨æº–æ‹ ã€Œæ±äº¬ã€ä¸­å±±ã€é˜ªç¥ã€äº¬éƒ½ã€æœ­å¹Œ > ä¸­äº¬ã€å‡½é¤¨ã€æ–°æ½Ÿ > ç¦å³¶ã€å°å€‰ã€ï¼‰
            venue_codes = pd.to_numeric(df['å ´ã‚³ãƒ¼ãƒ‰'], errors='coerce').fillna(0).astype(int)
            result = np.ones(len(venue_codes)) * 0.0
            result[venue_codes.isin([1, 2, 6, 5, 8])] = 9.0  # æ±äº¬ã€ä¸­å±±ã€é˜ªç¥ã€äº¬éƒ½ã€æœ­å¹Œï¼ˆç¬¬1ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
            result[venue_codes.isin([3, 7, 4])] = 7.0  # ä¸­äº¬ã€å‡½é¤¨ã€æ–°æ½Ÿï¼ˆç¬¬2ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
            result[venue_codes.isin([9, 10])] = 4.0  # ç¦å³¶ã€å°å€‰ï¼ˆç¬¬3ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
            return result
        elif 'å ´å' in df.columns:
            # å ´åã‹ã‚‰åˆ¤å®šï¼ˆæ›¸ç±å¼•ç”¨æº–æ‹ ï¼‰
            venue_names = df['å ´å'].astype(str)
            result = np.ones(len(venue_names)) * 0.0
            result[venue_names.isin(['æ±äº¬', 'ä¸­å±±', 'é˜ªç¥', 'äº¬éƒ½', 'æœ­å¹Œ'])] = 9.0  # ç¬¬1ã‚°ãƒ«ãƒ¼ãƒ—
            result[venue_names.isin(['ä¸­äº¬', 'å‡½é¤¨', 'æ–°æ½Ÿ'])] = 7.0  # ç¬¬2ã‚°ãƒ«ãƒ¼ãƒ—
            result[venue_names.isin(['ç¦å³¶', 'å°å€‰'])] = 4.0  # ç¬¬3ã‚°ãƒ«ãƒ¼ãƒ—
            return result
        else:
            logger.warning("âš ï¸ å ´ã‚³ãƒ¼ãƒ‰ãƒ»å ´åã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            return np.ones(len(df)) * 0.0
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†
    df['grade_level'] = get_grade_level_vectorized(df)
    df['venue_level'] = get_venue_level_vectorized(df)
    df['distance_level'] = get_distance_level_vectorized(df)
    df['field_size_level'] = get_field_size_level_vectorized(df)
    
    # åŸºæœ¬REQIç®—å‡º
    df['base_race_level'] = (
        df['grade_level'] * 0.5 +
        df['distance_level'] * 0.3 +
        df['field_size_level'] * 0.2
    )
    
    # ç°¡æ˜“é‡ã¿ä»˜ã‘å‡¦ç†ï¼ˆæ™‚ç³»åˆ—é †åºã‚’è€ƒæ…®ã—ãŸé«˜é€Ÿç‰ˆï¼‰
    logger.info("ğŸ”„ ç°¡æ˜“é‡ã¿ä»˜ã‘å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
    
    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ï¼‰
    sort_cols = ['é¦¬å']
    if 'å¹´æœˆæ—¥' in df.columns:
        sort_cols.append('å¹´æœˆæ—¥')
    elif 'å¹´' in df.columns:
        sort_cols.append('å¹´')
        if 'æœˆ' in df.columns:
            sort_cols.append('æœˆ')
        if 'æ—¥' in df.columns:
            sort_cols.append('æ—¥')
    
    df = df.sort_values(sort_cols).copy()
    
    # é¦¬ã”ã¨ã«é€£ç•ªã‚’ä»˜ä¸
    df['race_sequence'] = df.groupby('é¦¬å').cumcount() + 1
    
    # è¤‡å‹çµæœã«ã‚ˆã‚‹ç°¡æ˜“èª¿æ•´ä¿‚æ•°
    df['place_result'] = (df['ç€é †'] <= 3).astype(int)
    
    # éå»ã®è¤‡å‹ç‡ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆç§»å‹•å¹³å‡ï¼‰
    df['historical_place_rate'] = df.groupby('é¦¬å')['place_result'].expanding().mean().values
    
    # èª¿æ•´ä¿‚æ•°ã®ç®—å‡ºï¼ˆ0.8-1.2ã®ç¯„å›²ï¼‰
    df['adjustment_factor'] = 0.8 + (df['historical_place_rate'] * 0.4)
    df['adjustment_factor'] = df['adjustment_factor'].fillna(1.0).clip(0.8, 1.2)
    
    # æœ€çµ‚REQI
    df['race_level'] = df['base_race_level'] * df['adjustment_factor']
    
    logger.info("âœ… é«˜é€ŸREQIç®—å‡ºå®Œäº†")
    
    return df

@log_performance("REQIç‰¹å¾´é‡ç®—å‡º")
def calculate_race_level_features_with_position_weights(df: pd.DataFrame) -> pd.DataFrame:
    """è¤‡å‹å®Ÿç¸¾ã‚’åæ˜ ã—ãŸREQIç‰¹å¾´é‡ã‚’ç®—å‡ºã—ã¾ã™ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ã¸å§”è­²ï¼‰ã€‚

    Args:
        df (pd.DataFrame): ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ»é–‹å‚¬ãƒ»è·é›¢ãƒ»çµæœæƒ…å ±ã‚’å«ã‚€ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã€‚

    Returns:
        pd.DataFrame: REQI ãƒ¬ãƒ™ãƒ«åˆ—ã¨èª¿æ•´æ¸ˆã¿ race_level ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ã€‚
    """
    calculator = FeatureCalculator()
    return calculator.calculate_race_level_with_position_weights(df)

def create_stratification_categories(df: pd.DataFrame) -> pd.DataFrame:
    """å±¤åˆ¥ã‚«ãƒ†ã‚´ãƒªã‚’ä½œæˆã—ã¾ã™ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚
    
    Args:
        df (pd.DataFrame): é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã€‚
        
    Returns:
        pd.DataFrame: å¹´é½¢å±¤ãƒ»çµŒé¨“æ•°å±¤ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ã€‚
    """
    analyzer = StratifiedAnalyzer(min_sample_size=10)
    return analyzer.create_stratification_categories(df)

@log_performance("çµ±åˆå±¤åˆ¥åˆ†æ")
def perform_integrated_stratified_analysis(analysis_df: pd.DataFrame) -> Dict[str, Any]:
    """çµ±åˆã•ã‚ŒãŸå±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚
    
    Args:
        analysis_df (pd.DataFrame): åˆ†æå¯¾è±¡ã®é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã€‚
        
    Returns:
        Dict[str, Any]: å±¤åˆ¥åˆ†æçµæœã€‚
    """
    analyzer = StratifiedAnalyzer(min_sample_size=10)
    return analyzer.perform_integrated_analysis(analysis_df)

def generate_stratified_report(results: Dict[str, Any], analysis_df: pd.DataFrame, output_dir: Path) -> str:
    """å±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚
    
    Args:
        results (Dict[str, Any]): å±¤åˆ¥åˆ†æçµæœã€‚
        analysis_df (pd.DataFrame): åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‚
        output_dir (Path): å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
        
    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã€‚
    """
    generator = ReportGenerator()
    return generator.generate_stratified_report(results, analysis_df, output_dir)



def calculate_accurate_feature_levels(df: pd.DataFrame) -> pd.DataFrame:
    """å®Ÿéš›ã®CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æ­£ç¢ºã«è¨ˆç®—ã—ã¾ã™ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚
    
    Args:
        df (pd.DataFrame): å‡¦ç†å¯¾è±¡ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã€‚
        
    Returns:
        pd.DataFrame: grade_level, venue_level, distance_level åˆ—ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ã€‚
    """
    calculator = FeatureCalculator()
    return calculator.calculate_accurate_feature_levels(df)


@log_performance("ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œæ¤œè¨¼")
def validate_grade_estimation(data_dir: str, encoding: str = 'utf-8') -> Dict[str, Any]:
    """ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
    
    å…ƒã®ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹ãƒ¬ãƒ¼ã‚¹ã§è£œå®Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã—ã€
    ä¸€è‡´ç‡ã‚’è¨ˆç®—ã—ã¦è£œå®Œç²¾åº¦ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
    
    Args:
        data_dir (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        encoding (str): ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€‚
        
    Returns:
        Dict[str, Any]: æ¤œè¨¼çµæœï¼ˆä¸€è‡´ç‡ã€ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ä¸€è‡´ç‡ãªã©ï¼‰ã€‚
    """
    logger.info("ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼ã‚’é–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_all_data_once(data_dir, encoding)
    
    if df is None or len(df) == 0:
        logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return {'error': 'ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—'}
    
    logger.info(f"ğŸ“Š èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®ç¢ºèª
    grade_column = 'ã‚°ãƒ¬ãƒ¼ãƒ‰'
    if grade_column not in df.columns:
        # ä»£æ›¿ã‚«ãƒ©ãƒ åã‚’è©¦ã™
        for alt_col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'grade']:
            if alt_col in df.columns:
                grade_column = alt_col
                break
        else:
            logger.error(f"âŒ ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {'error': 'ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ãªã—'}
    
    # å…ƒã®ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹ãƒ¬ãƒ¼ã‚¹ï¼ˆæ¤œè¨¼ç”¨ï¼‰
    original_grade_mask = df[grade_column].notna()
    validation_df = df[original_grade_mask].copy()
    
    logger.info(f"ğŸ“Š æ¤œè¨¼å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(validation_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆå…ƒã®ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒå­˜åœ¨ï¼‰")
    
    if len(validation_df) == 0:
        logger.warning("âš ï¸ æ¤œè¨¼å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
        return {'error': 'æ¤œè¨¼å¯¾è±¡ãªã—'}
    
    # å…ƒã®ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ä¿å­˜
    original_grades = validation_df[grade_column].copy()
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã‚’ä¸€æ—¦æ¬ æå€¤ã«ã—ã¦ã€è£œå®Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨
    validation_df[grade_column] = np.nan
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šã‚’å®Ÿè¡Œ
    grade_estimator = GradeEstimator()
    estimated_df = grade_estimator.estimate_grade(validation_df, grade_column)
    
    # æ¨å®šã•ã‚ŒãŸã‚°ãƒ¬ãƒ¼ãƒ‰
    estimated_grades = estimated_df[grade_column]
    
    # ä¸€è‡´ç‡ã‚’è¨ˆç®—
    valid_mask = original_grades.notna() & estimated_grades.notna()
    
    if valid_mask.sum() == 0:
        logger.warning("âš ï¸ æ¯”è¼ƒå¯èƒ½ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
        return {'error': 'æ¯”è¼ƒå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãªã—'}
    
    original_valid = original_grades[valid_mask]
    estimated_valid = estimated_grades[valid_mask]
    
    # ä¸€è‡´ç‡ï¼ˆAccuracyï¼‰
    matches = (original_valid == estimated_valid).sum()
    total = len(original_valid)
    accuracy = matches / total
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ã®ä¸€è‡´ç‡
    grade_accuracy = {}
    grade_names = {1: 'G1', 2: 'G2', 3: 'G3', 4: 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰', 5: 'æ¡ä»¶æˆ¦', 6: 'L'}
    
    for grade in sorted(original_valid.unique()):
        if pd.notna(grade):
            grade_mask = original_valid == grade
            if grade_mask.sum() > 0:
                grade_matches = (original_valid[grade_mask] == estimated_valid[grade_mask]).sum()
                grade_total = grade_mask.sum()
                grade_acc = grade_matches / grade_total
                grade_name = grade_names.get(int(grade), f'ã‚°ãƒ¬ãƒ¼ãƒ‰{int(grade)}')
                grade_accuracy[grade_name] = {
                    'accuracy': grade_acc,
                    'matches': int(grade_matches),
                    'total': int(grade_total)
                }
    
    # çµæœã‚’æ•´ç†
    results = {
        'total_records': int(total),
        'matches': int(matches),
        'accuracy': accuracy,
        'accuracy_pct': f"{accuracy * 100:.1f}%",
        'grade_accuracy': grade_accuracy
    }
    
    # çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info("=" * 60)
    logger.info("ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼çµæœ")
    logger.info("=" * 60)
    logger.info(f"æ¤œè¨¼å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total:,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
    logger.info(f"ä¸€è‡´æ•°: {matches:,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
    logger.info(f"ä¸€è‡´ç‡ï¼ˆAccuracyï¼‰: {accuracy * 100:.1f}%")
    logger.info("")
    logger.info("ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ä¸€è‡´ç‡:")
    for grade_name, stats in grade_accuracy.items():
        logger.info(f"  {grade_name}: {stats['accuracy']*100:.1f}% ({stats['matches']:,}/{stats['total']:,})")
    logger.info("=" * 60)
    
    return results


@log_performance("EDAåˆ†æ")
def perform_eda_analysis(data_dir: str, output_dir: str, encoding: str = 'utf-8') -> Dict[str, Any]:
    """EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    
    åŸºæœ¬çµ±è¨ˆé‡ã€æ¬ æç‡ã€æ™‚ç³»åˆ—åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’ç¢ºèªã—ã€
    çµæœã‚’Markdownãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
    
    Args:
        data_dir (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        encoding (str): ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€‚
        
    Returns:
        Dict[str, Any]: EDAåˆ†æçµæœã€‚
    """
    logger.info("ğŸ“Š EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰ã‚’é–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_all_data_once(data_dir, encoding)
    
    if df is None or len(df) == 0:
        logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return {'error': 'ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—'}
    
    logger.info(f"ğŸ“Š èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰ Ã— {len(df.columns)}åˆ—")
    
    results = {
        'data_overview': {},
        'basic_statistics': {},
        'missing_values': {},
        'time_series_split': {}
    }
    
    # ========================================
    # 1. ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
    # ========================================
    logger.info("ğŸ“‹ 1. ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã‚’é›†è¨ˆä¸­...")
    
    results['data_overview'] = {
        'total_records': len(df),
        'total_columns': len(df.columns),
        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
        'duplicate_rows': int(df.duplicated().sum())
    }
    
    # å¹´ã®ç¯„å›²
    if 'å¹´' in df.columns:
        df['å¹´'] = pd.to_numeric(df['å¹´'], errors='coerce')
        results['data_overview']['year_range'] = {
            'min': int(df['å¹´'].min()) if df['å¹´'].notna().any() else None,
            'max': int(df['å¹´'].max()) if df['å¹´'].notna().any() else None
        }
    
    # ========================================
    # 2. åŸºæœ¬çµ±è¨ˆé‡ï¼ˆä¸»è¦æ•°å€¤åˆ—ï¼‰
    # ========================================
    logger.info("ğŸ“‹ 2. åŸºæœ¬çµ±è¨ˆé‡ã‚’è¨ˆç®—ä¸­...")
    
    # åˆ†æå¯¾è±¡ã®ä¸»è¦æ•°å€¤åˆ—
    key_numeric_cols = [
        'ç€é †', 'ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º', 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹', 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸Š',
        '10æ™‚å˜å‹ã‚ªãƒƒã‚º', '10æ™‚è¤‡å‹ã‚ªãƒƒã‚º', 'è·é›¢', 'é ­æ•°',
        '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', 'æœ¬è³é‡‘', 'ã‚°ãƒ¬ãƒ¼ãƒ‰'
    ]
    
    available_numeric_cols = [col for col in key_numeric_cols if col in df.columns]
    
    for col in available_numeric_cols:
        try:
            series = pd.to_numeric(df[col], errors='coerce')
            valid_count = series.notna().sum()
            
            if valid_count > 0:
                results['basic_statistics'][col] = {
                    'count': int(valid_count),
                    'mean': float(series.mean()),
                    'std': float(series.std()),
                    'min': float(series.min()),
                    '25%': float(series.quantile(0.25)),
                    '50%': float(series.quantile(0.50)),
                    '75%': float(series.quantile(0.75)),
                    'max': float(series.max())
                }
        except Exception as e:
            logger.warning(f"âš ï¸ {col}ã®çµ±è¨ˆè¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ========================================
    # 3. æ¬ æç‡åˆ†æ
    # ========================================
    logger.info("ğŸ“‹ 3. æ¬ æç‡ã‚’åˆ†æä¸­...")
    
    # åˆ—åˆ¥æ¬ æç‡
    missing_counts = df.isnull().sum()
    missing_pct = (missing_counts / len(df) * 100).round(2)
    
    results['missing_values']['by_column'] = {
        col: {
            'missing_count': int(missing_counts[col]),
            'missing_pct': float(missing_pct[col])
        }
        for col in missing_counts[missing_counts > 0].index
    }
    
    results['missing_values']['total_missing_cells'] = int(missing_counts.sum())
    results['missing_values']['total_cells'] = int(df.size)
    results['missing_values']['overall_missing_pct'] = float(missing_counts.sum() / df.size * 100)
    
    # å¹´åˆ¥Ã—ä¸»è¦åˆ—ã®æ¬ æç‡
    if 'å¹´' in df.columns:
        key_cols_for_missing = ['ã‚°ãƒ¬ãƒ¼ãƒ‰', '10æ™‚å˜å‹ã‚ªãƒƒã‚º', '10æ™‚è¤‡å‹ã‚ªãƒƒã‚º', 
                                'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹', 'é¨æ‰‹ã‚³ãƒ¼ãƒ‰', 'ç€é †']
        available_key_cols = [c for c in key_cols_for_missing if c in df.columns]
        
        if available_key_cols:
            try:
                year_missing = df.groupby('å¹´')[available_key_cols].apply(
                    lambda x: x.isnull().mean() * 100
                ).round(2)
                results['missing_values']['by_year'] = year_missing.to_dict()
            except Exception as e:
                logger.warning(f"âš ï¸ å¹´åˆ¥æ¬ æç‡ã®è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ========================================
    # 4. æ™‚ç³»åˆ—åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ç¢ºèª
    # ========================================
    logger.info("ğŸ“‹ 4. æ™‚ç³»åˆ—åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’ç¢ºèªä¸­...")
    
    if 'å¹´' in df.columns and df['å¹´'].notna().any():
        # è¨“ç·´æœŸé–“ï¼ˆ~2023å¹´ï¼‰ã¨ãƒ†ã‚¹ãƒˆæœŸé–“ï¼ˆ2024å¹´ï¼‰ã§åˆ†å‰²
        train_df = df[df['å¹´'] <= 2023]
        test_df = df[df['å¹´'] == 2024]
        
        def calc_period_stats(period_df, period_name):
            """æœŸé–“åˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—"""
            stats = {
                'record_count': len(period_df),
                'unique_horses': period_df['é¦¬å'].nunique() if 'é¦¬å' in period_df.columns else None
            }
            
            # ä¸»è¦æ•°å€¤åˆ—ã®çµ±è¨ˆ
            for col in ['ç€é †', 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹', 'è·é›¢']:
                if col in period_df.columns:
                    series = pd.to_numeric(period_df[col], errors='coerce')
                    if series.notna().sum() > 0:
                        stats[f'{col}_mean'] = float(series.mean())
                        stats[f'{col}_std'] = float(series.std())
            
            # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
            if 'ã‚°ãƒ¬ãƒ¼ãƒ‰' in period_df.columns:
                grade_dist = period_df['ã‚°ãƒ¬ãƒ¼ãƒ‰'].value_counts(normalize=True) * 100
                stats['grade_distribution'] = grade_dist.round(2).to_dict()
            
            return stats
        
        if len(train_df) > 0:
            results['time_series_split']['train_period'] = {
                'year_range': f"~2023å¹´",
                **calc_period_stats(train_df, 'è¨“ç·´æœŸé–“')
            }
        
        if len(test_df) > 0:
            results['time_series_split']['test_period'] = {
                'year_range': "2024å¹´",
                **calc_period_stats(test_df, 'ãƒ†ã‚¹ãƒˆæœŸé–“')
            }
        
        # ç‰¹æ€§ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        if len(train_df) > 0 and len(test_df) > 0:
            consistency_check = {}
            for col in ['ç€é †', 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹', 'è·é›¢']:
                if col in df.columns:
                    train_mean = pd.to_numeric(train_df[col], errors='coerce').mean()
                    test_mean = pd.to_numeric(test_df[col], errors='coerce').mean()
                    if pd.notna(train_mean) and pd.notna(test_mean) and train_mean != 0:
                        diff_pct = abs(test_mean - train_mean) / train_mean * 100
                        consistency_check[col] = {
                            'train_mean': float(train_mean),
                            'test_mean': float(test_mean),
                            'diff_pct': float(diff_pct),
                            'consistent': diff_pct < 20  # 20%ä»¥å†…ãªã‚‰ä¸€è²«æ€§ã‚ã‚Š
                        }
            results['time_series_split']['consistency_check'] = consistency_check
    
    # ========================================
    # 5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    # ========================================
    logger.info("ğŸ“‹ 5. EDAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    report_path = output_path / 'eda_report.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
        f.write("## 1. ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
        overview = results['data_overview']
        f.write(f"- **ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: {overview['total_records']:,}ä»¶\n")
        f.write(f"- **ç·åˆ—æ•°**: {overview['total_columns']}åˆ—\n")
        f.write(f"- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: {overview['memory_usage_mb']:.1f}MB\n")
        f.write(f"- **é‡è¤‡è¡Œæ•°**: {overview['duplicate_rows']:,}ä»¶\n")
        if 'year_range' in overview:
            yr = overview['year_range']
            f.write(f"- **ãƒ‡ãƒ¼ã‚¿æœŸé–“**: {yr['min']}å¹´ - {yr['max']}å¹´\n")
        f.write("\n")
        
        # åŸºæœ¬çµ±è¨ˆé‡
        f.write("## 2. åŸºæœ¬çµ±è¨ˆé‡ï¼ˆä¸»è¦æ•°å€¤åˆ—ï¼‰\n\n")
        f.write("| åˆ—å | æœ‰åŠ¹ä»¶æ•° | å¹³å‡ | æ¨™æº–åå·® | æœ€å° | 25% | 50% | 75% | æœ€å¤§ |\n")
        f.write("|------|----------|------|----------|------|-----|-----|-----|------|\n")
        
        for col, stats in results['basic_statistics'].items():
            f.write(f"| {col} | {stats['count']:,} | {stats['mean']:.2f} | {stats['std']:.2f} | "
                   f"{stats['min']:.2f} | {stats['25%']:.2f} | {stats['50%']:.2f} | "
                   f"{stats['75%']:.2f} | {stats['max']:.2f} |\n")
        f.write("\n")
        
        # æ¬ æç‡
        f.write("## 3. æ¬ æç‡åˆ†æ\n\n")
        mv = results['missing_values']
        f.write(f"- **ç·æ¬ æã‚»ãƒ«æ•°**: {mv['total_missing_cells']:,}ã‚»ãƒ«\n")
        f.write(f"- **å…¨ä½“æ¬ æç‡**: {mv['overall_missing_pct']:.2f}%\n\n")
        
        f.write("### 3.1 åˆ—åˆ¥æ¬ æç‡ï¼ˆæ¬ æãŒã‚ã‚‹åˆ—ã®ã¿ï¼‰\n\n")
        f.write("| åˆ—å | æ¬ æä»¶æ•° | æ¬ æç‡ |\n")
        f.write("|------|----------|--------|\n")
        
        # æ¬ æç‡ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_missing = sorted(
            mv['by_column'].items(),
            key=lambda x: x[1]['missing_pct'],
            reverse=True
        )[:20]  # ä¸Šä½20åˆ—ã®ã¿è¡¨ç¤º
        
        for col, stats in sorted_missing:
            f.write(f"| {col} | {stats['missing_count']:,} | {stats['missing_pct']:.2f}% |\n")
        f.write("\n")
        
        # å¹´åˆ¥æ¬ æç‡
        if 'by_year' in mv and mv['by_year']:
            f.write("### 3.2 å¹´åˆ¥Ã—ä¸»è¦åˆ—ã®æ¬ æç‡ï¼ˆ%ï¼‰\n\n")
            by_year = mv['by_year']
            if by_year:
                # æœ€åˆã®åˆ—åã‚’å–å¾—ã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
                first_col = list(by_year.keys())[0]
                years = sorted(by_year[first_col].keys())
                cols = list(by_year.keys())
                
                header = "| å¹´ | " + " | ".join(cols) + " |\n"
                separator = "|----" + "|------" * len(cols) + "|\n"
                f.write(header)
                f.write(separator)
                
                for year in years:
                    row = f"| {int(year)} |"
                    for col in cols:
                        val = by_year[col].get(year, 0)
                        row += f" {val:.1f}% |"
                    f.write(row + "\n")
                f.write("\n")
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        f.write("## 4. æ™‚ç³»åˆ—åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§\n\n")
        ts = results['time_series_split']
        
        if 'train_period' in ts and 'test_period' in ts:
            f.write("### 4.1 æœŸé–“åˆ¥ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
            f.write("| æœŸé–“ | ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° | ãƒ¦ãƒ‹ãƒ¼ã‚¯é¦¬æ•° |\n")
            f.write("|------|------------|-------------|\n")
            
            train = ts['train_period']
            test = ts['test_period']
            
            f.write(f"| è¨“ç·´æœŸé–“ï¼ˆ{train['year_range']}ï¼‰ | {train['record_count']:,} | "
                   f"{train.get('unique_horses', 'N/A'):,} |\n")
            f.write(f"| ãƒ†ã‚¹ãƒˆæœŸé–“ï¼ˆ{test['year_range']}ï¼‰ | {test['record_count']:,} | "
                   f"{test.get('unique_horses', 'N/A'):,} |\n")
            f.write("\n")
            
            # ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
            if 'consistency_check' in ts:
                f.write("### 4.2 ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯\n\n")
                f.write("| æŒ‡æ¨™ | è¨“ç·´æœŸé–“å¹³å‡ | ãƒ†ã‚¹ãƒˆæœŸé–“å¹³å‡ | å·®ç•°(%) | ä¸€è²«æ€§ |\n")
                f.write("|------|-------------|---------------|---------|--------|\n")
                
                for col, check in ts['consistency_check'].items():
                    status = "âœ… ä¸€è²«" if check['consistent'] else "âš ï¸ å·®ç•°ã‚ã‚Š"
                    f.write(f"| {col} | {check['train_mean']:.2f} | {check['test_mean']:.2f} | "
                           f"{check['diff_pct']:.1f}% | {status} |\n")
                f.write("\n")
                
                f.write("**åˆ¤å®šåŸºæº–**: å¹³å‡å€¤ã®å·®ç•°ãŒ20%ä»¥å†…ã§ã‚ã‚Œã°ã€Œä¸€è²«æ€§ã‚ã‚Šã€ã¨åˆ¤å®š\n\n")
        
        # çµè«–
        f.write("## 5. EDAçµè«–\n\n")
        f.write("### ãƒ‡ãƒ¼ã‚¿å“è³ªã®è©•ä¾¡\n\n")
        
        # æ¬ æç‡ã®è©•ä¾¡
        overall_missing = mv['overall_missing_pct']
        if overall_missing < 5:
            f.write("- âœ… **æ¬ æç‡**: è‰¯å¥½ï¼ˆå…¨ä½“æ¬ æç‡ < 5%ï¼‰\n")
        elif overall_missing < 15:
            f.write("- âš ï¸ **æ¬ æç‡**: è¨±å®¹ç¯„å›²ï¼ˆå…¨ä½“æ¬ æç‡ 5-15%ï¼‰\n")
        else:
            f.write("- âŒ **æ¬ æç‡**: è¦ç¢ºèªï¼ˆå…¨ä½“æ¬ æç‡ > 15%ï¼‰\n")
        
        # æ™‚ç³»åˆ—ä¸€è²«æ€§ã®è©•ä¾¡
        if 'consistency_check' in ts:
            all_consistent = all(c['consistent'] for c in ts['consistency_check'].values())
            if all_consistent:
                f.write("- âœ… **æ™‚ç³»åˆ—ä¸€è²«æ€§**: è‰¯å¥½ï¼ˆè¨“ç·´/ãƒ†ã‚¹ãƒˆæœŸé–“ã§ç‰¹æ€§ãŒä¸€è‡´ï¼‰\n")
            else:
                f.write("- âš ï¸ **æ™‚ç³»åˆ—ä¸€è²«æ€§**: ä¸€éƒ¨å·®ç•°ã‚ã‚Šï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã®å¯èƒ½æ€§ï¼‰\n")
        
        f.write("\n### åˆ†æã«ä½¿ç”¨å¯èƒ½ãªä¸»è¦åˆ—\n\n")
        for col in results['basic_statistics'].keys():
            stats = results['basic_statistics'][col]
            missing_info = mv['by_column'].get(col, {'missing_pct': 0})
            f.write(f"- **{col}**: æœ‰åŠ¹{stats['count']:,}ä»¶, æ¬ æ{missing_info.get('missing_pct', 0):.1f}%\n")
    
    logger.info(f"âœ… EDAãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_path}")
    
    # ãƒ­ã‚°å‡ºåŠ›
    logger.info("=" * 60)
    logger.info("ğŸ“Š EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰çµæœã‚µãƒãƒªãƒ¼")
    logger.info("=" * 60)
    logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {results['data_overview']['total_records']:,}ä»¶")
    logger.info(f"ç·åˆ—æ•°: {results['data_overview']['total_columns']}åˆ—")
    logger.info(f"å…¨ä½“æ¬ æç‡: {results['missing_values']['overall_missing_pct']:.2f}%")
    if 'train_period' in results['time_series_split']:
        logger.info(f"è¨“ç·´æœŸé–“ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {results['time_series_split']['train_period']['record_count']:,}ä»¶")
    if 'test_period' in results['time_series_split']:
        logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {results['time_series_split']['test_period']['record_count']:,}ä»¶")
    logger.info("=" * 60)
    
    return results


def analyze_by_periods_optimized(analyzer, periods, base_output_dir):
    """æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆæœ€é©åŒ–ç‰ˆãƒ»ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰ã€‚
    
    Args:
        analyzer: åˆ†æå™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
        periods: æœŸé–“ãƒªã‚¹ãƒˆã€‚
        base_output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
        
    Returns:
        Dict[str, Any]: æœŸé–“ã”ã¨ã®åˆ†æçµæœã€‚
    """
    # æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
    calculator = FeatureCalculator()
    from horse_racing.services.period_analysis_service import PeriodAnalysisService
    period_service = PeriodAnalysisService(calculator)
    return period_service.analyze_by_periods(analyzer, periods, base_output_dir)

def analyze_by_periods(analyzer, periods, base_output_dir):
    """æœŸé–“åˆ¥ã«åˆ†æã‚’å®Ÿè¡Œï¼ˆæœ€é©åŒ–ç‰ˆã‚’ä½¿ç”¨ï¼‰"""
    return analyze_by_periods_optimized(analyzer, periods, base_output_dir)


def generate_period_summary_report(all_results, output_dir):
    """æœŸé–“åˆ¥åˆ†æã®ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report_path = output_dir / 'ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ_æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## ğŸ“Š åˆ†ææœŸé–“ä¸€è¦§\n\n")
        f.write("| æœŸé–“ | å¯¾è±¡é¦¬æ•° | ç·ãƒ¬ãƒ¼ã‚¹æ•° | å¹³å‡ãƒ¬ãƒ™ãƒ«ç›¸é–¢ | æœ€é«˜ãƒ¬ãƒ™ãƒ«ç›¸é–¢ |\n")
        f.write("|------|----------|-----------|---------------|---------------|\n")
        
        for period_name, results in all_results.items():
            period_info = results.get('period_info', {})
            correlation_stats = results.get('correlation_stats', {})
            
            total_horses = period_info.get('total_horses', 0)
            total_races = period_info.get('total_races', 0)
            
            # ç›¸é–¢ä¿‚æ•°ã®å–å¾—
            corr_avg = correlation_stats.get('correlation_place_avg', 0.0)
            corr_max = correlation_stats.get('correlation_place_max', 0.0)
            
            f.write(f"| {period_name} | {total_horses:,}é ­ | {total_races:,}ãƒ¬ãƒ¼ã‚¹ | {corr_avg:.3f} | {corr_max:.3f} |\n")
        
        # å„æœŸé–“ã®è©³ç´°
        for period_name, results in all_results.items():
            f.write(f"\n## ğŸ“ˆ æœŸé–“: {period_name}\n\n")
            
            period_info = results.get('period_info', {})
            correlation_stats = results.get('correlation_stats', {})
            
            f.write("### åŸºæœ¬æƒ…å ±\n")
            f.write(f"- **åˆ†ææœŸé–“**: {period_info.get('start_year', 'ä¸æ˜')}å¹´ - {period_info.get('end_year', 'ä¸æ˜')}å¹´\n")
            f.write(f"- **å¯¾è±¡é¦¬æ•°**: {period_info.get('total_horses', 0):,}é ­\n")
            f.write(f"- **ç·ãƒ¬ãƒ¼ã‚¹æ•°**: {period_info.get('total_races', 0):,}ãƒ¬ãƒ¼ã‚¹\n\n")
            
            f.write("### ç›¸é–¢åˆ†æçµæœ\n")
            if correlation_stats:
                # å¹³å‡ãƒ¬ãƒ™ãƒ«åˆ†æ
                corr_place_avg = correlation_stats.get('correlation_place_avg', 0.0)
                r2_place_avg = correlation_stats.get('r2_place_avg', 0.0)
                
                # æœ€é«˜ãƒ¬ãƒ™ãƒ«åˆ†æ
                corr_place_max = correlation_stats.get('correlation_place_max', 0.0)
                r2_place_max = correlation_stats.get('r2_place_max', 0.0)
                
                f.write("**å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ vs è¤‡å‹ç‡**\n")
                f.write(f"- ç›¸é–¢ä¿‚æ•°: {corr_place_avg:.3f}\n")
                f.write(f"- æ±ºå®šä¿‚æ•° (RÂ²): {r2_place_avg:.3f}\n\n")
                
                f.write("**æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ vs è¤‡å‹ç‡**\n")
                f.write(f"- ç›¸é–¢ä¿‚æ•°: {corr_place_max:.3f}\n")
                f.write(f"- æ±ºå®šä¿‚æ•° (RÂ²): {r2_place_max:.3f}\n\n")
            else:
                f.write("- ç›¸é–¢åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—\n\n")
        
        f.write("\n## ğŸ’¡ ç·åˆçš„ãªå‚¾å‘ã¨çŸ¥è¦‹\n\n")
        
        # æœŸé–“åˆ¥ã®ç›¸é–¢ä¿‚æ•°å¤‰åŒ–
        if len(all_results) > 1:
            f.write("### æ™‚ç³»åˆ—å¤‰åŒ–\n")
            f.write("å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨è¤‡å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã®å¤‰åŒ–ï¼š\n")
            
            correlations_by_period = []
            for period_name, results in all_results.items():
                correlation_stats = results.get('correlation_stats', {})
                corr = correlation_stats.get('correlation_place_avg', 0.0)
                correlations_by_period.append((period_name, corr))
            
            for i, (period, corr) in enumerate(correlations_by_period):
                if i > 0:
                    prev_corr = correlations_by_period[i-1][1]
                    change = corr - prev_corr
                    trend = "ä¸Šæ˜‡" if change > 0.05 else "ä¸‹é™" if change < -0.05 else "æ¨ªã°ã„"
                    f.write(f"- {period}: {corr:.3f} ({trend})\n")
                else:
                    f.write(f"- {period}: {corr:.3f} (åŸºæº–)\n")
        
        f.write("\n### ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æã®ç‰¹å¾´\n")
        f.write("- ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¯ç«¶é¦¬å ´ã®æ ¼å¼åº¦ã¨å®ŸåŠ›ã®é–¢ä¿‚ã‚’æ•°å€¤åŒ–\n")
        f.write("- å¹³å‡ãƒ¬ãƒ™ãƒ«ï¼šé¦¬ã®ç¶™ç¶šçš„ãªå®ŸåŠ›ã‚’è¡¨ã™æŒ‡æ¨™\n")
        f.write("- æœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼šé¦¬ã®ãƒ”ãƒ¼ã‚¯æ™‚ã®å®ŸåŠ›ã‚’è¡¨ã™æŒ‡æ¨™\n")
        f.write("- æ™‚ç³»åˆ—åˆ†æã«ã‚ˆã‚Šã€ç«¶é¦¬ç•Œã®æ ¼å¼ä½“ç³»ã®å¤‰åŒ–ã‚’æŠŠæ¡å¯èƒ½\n")
    
    logger.info(f"æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

@log_performance("åŒ…æ‹¬çš„ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æ")
def perform_comprehensive_odds_analysis(data_dir: str, output_dir: str, sample_size: int = None, min_races: int = 6, start_date: str = None, end_date: str = None) -> Dict[str, Any]:
    """REQI ã¨ã‚ªãƒƒã‚ºã‚’æ¯”è¼ƒã™ã‚‹åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã€‚

    Args:
        data_dir (str): ã‚ªãƒƒã‚ºåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ ¼ç´ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
        output_dir (str): ãƒ¬ãƒãƒ¼ãƒˆã‚„å¯è¦–åŒ–ã‚’ä¿å­˜ã™ã‚‹å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
        sample_size (int | None): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å ´åˆã®ä»¶æ•°ä¸Šé™ã€‚
        min_races (int): é¦¬ã‚’åˆ†æå¯¾è±¡ã¨ã—ã¦æ®‹ã™æœ€ä½å‡ºèµ°å›æ•°ã€‚
        start_date (str | None): ``YYYYMMDD`` å½¢å¼ã®ä¸‹é™æ—¥ä»˜ã€‚
        end_date (str | None): ``YYYYMMDD`` å½¢å¼ã®ä¸Šé™æ—¥ä»˜ã€‚

    Returns:
        Dict[str, Any]: ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€ç›¸é–¢åˆ†æã€å›å¸°åˆ†æãªã©ã‚’ã¾ã¨ã‚ãŸçµæœã€‚
    """
    logger.info("ğŸ¯ åŒ…æ‹¬çš„ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’é–‹å§‹...")
    
    try:
        # OddsComparisonAnalyzerã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        analyzer = OddsComparisonAnalyzer(min_races=min_races)
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        combined_df = load_all_data_once(data_dir, 'utf-8')
        # æŒ‡å®šãŒã‚ã‚Œã°æ—¥ä»˜ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿
        combined_df = filter_by_date_range(combined_df, start_date, end_date)
        if combined_df.empty:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
        dataset_files = get_all_dataset_files(data_dir)
        file_count = len(dataset_files)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ãŒã‚ã‚‹å ´åˆã¯é©ç”¨
        if sample_size is not None and len(combined_df) > sample_size * 1000:  # æ¦‚ç®—ã§åˆ¶é™
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã‚’é©ç”¨: {sample_size * 1000}è¡Œ")
            combined_df = combined_df.sample(n=sample_size * 1000, random_state=42)
        
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df):,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
        log_dataframe_info(combined_df, "çµ±åˆã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿")
        
        # ã€é‡è¦ã€‘å¹´ã‚«ãƒ©ãƒ ã®ç¢ºèªã¨ç”Ÿæˆï¼ˆæ™‚ç³»åˆ—åˆ†å‰²ç”¨ï¼‰
        if 'å¹´' not in combined_df.columns and 'å¹´æœˆæ—¥' in combined_df.columns:
            logger.info("ğŸ“… å¹´æœˆæ—¥ã‹ã‚‰å¹´ã‚«ãƒ©ãƒ ã‚’ç”Ÿæˆä¸­...")
            combined_df['å¹´'] = pd.to_numeric(combined_df['å¹´æœˆæ—¥'].astype(str).str[:4], errors='coerce')
            logger.info(f"âœ… å¹´ã‚«ãƒ©ãƒ ç”Ÿæˆå®Œäº†: {combined_df['å¹´'].min():.0f}~{combined_df['å¹´'].max():.0f}å¹´")
        
        # ã€é‡è¦ã€‘combined_dfã«race_levelã‚’è¿½åŠ ï¼ˆæ™‚ç³»åˆ—åˆ†å‰²ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
        logger.info("ğŸ“Š ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«race_levelï¼ˆREQIï¼‰ã‚’è¨ˆç®—ä¸­...")
        combined_df = calculate_race_level_features_with_position_weights(combined_df)
        logger.info(f"âœ… race_levelè¨ˆç®—å®Œäº†: å¹³å‡å€¤={combined_df['race_level'].mean():.3f}")
        
        # HorseREQIè¨ˆç®—
        horse_stats_df = analyzer.calculate_horse_race_level(combined_df)
        logger.info(f"HorseREQIè¨ˆç®—å®Œäº†: {len(horse_stats_df):,}é ­")
        
        # ç›¸é–¢åˆ†æ
        correlation_results = analyzer.perform_correlation_analysis(horse_stats_df)
        
        # å›å¸°åˆ†æ
        regression_results = analyzer.perform_regression_analysis(horse_stats_df)
        
        # ã€è¿½åŠ ã€‘åŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒåˆ†æ
        logger.info("ğŸ“Š REQI vs ã‚ªãƒƒã‚ºåŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒã‚’å®Ÿè¡Œä¸­...")
        effect_size_results = compare_reqi_vs_odds_effect_size(horse_stats_df)
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        analysis_results = {
            'data_summary': {
                'total_records': len(combined_df),
                'horse_count': len(horse_stats_df),
                'file_count': file_count
            },
            'correlations': correlation_results,
            'regression': regression_results,
            'effect_size_comparison': effect_size_results
        }
        
        # ã€ä¿®æ­£ã€‘å¯è¦–åŒ–ã®ä½œæˆ
        logger.info("ğŸ“Š å¯è¦–åŒ–ï¼ˆæ•£å¸ƒå›³ãƒ»ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼‰ã‚’ä½œæˆä¸­...")
        try:
            # ç›¸é–¢åˆ†æã¨å›å¸°åˆ†æã®çµæœã‚’çµ±åˆ
            visualization_results = {
                'correlations': correlation_results['correlations'],
                'h2_verification': regression_results.get('h2_verification', {})
            }
            analyzer.create_visualizations(horse_stats_df, visualization_results, Path(output_dir))
            logger.info("âœ… å¯è¦–åŒ–ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ å¯è¦–åŒ–ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨åŠ¹æœã‚µã‚¤ã‚ºçµæœã‚‚æ¸¡ã™ï¼‰
        analyzer.generate_comprehensive_report(horse_stats_df, correlation_results, regression_results, Path(output_dir), combined_df, effect_size_results)
        
        return analysis_results
        
    except ImportError:
        # OddsComparisonAnalyzerãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ç°¡æ˜“ç‰ˆ
        logger.warning("OddsComparisonAnalyzerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ç°¡æ˜“ç‰ˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        return perform_simple_odds_analysis(data_dir, output_dir, sample_size, min_races, start_date, end_date)

def perform_simple_odds_analysis(data_dir: str, output_dir: str, sample_size: int = None, min_races: int = 6, start_date: str = None, end_date: str = None) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æ"""
    logger.info("ğŸ“Š ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ...")
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    combined_df = load_all_data_once(data_dir, 'utf-8')
    # æŒ‡å®šãŒã‚ã‚Œã°æ—¥ä»˜ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿
    combined_df = filter_by_date_range(combined_df, start_date, end_date)
    if combined_df.empty:
        raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
    dataset_files = get_all_dataset_files(data_dir)
    file_count = len(dataset_files)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ãŒã‚ã‚‹å ´åˆã¯é©ç”¨
    if sample_size is not None and len(combined_df) > sample_size * 1000:  # æ¦‚ç®—ã§åˆ¶é™
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã‚’é©ç”¨: {sample_size * 1000}è¡Œ")
        combined_df = combined_df.sample(n=sample_size * 1000, random_state=42)
    
    logger.info("ğŸ”— ç°¡æ˜“ç‰ˆãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
    logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df):,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
    log_dataframe_info(combined_df, "ç°¡æ˜“ç‰ˆçµ±åˆãƒ‡ãƒ¼ã‚¿")
    
    # åŸºæœ¬çš„ãªé¦¬çµ±è¨ˆè¨ˆç®—
    horse_stats = calculate_simple_horse_statistics(combined_df, min_races)
    logger.info(f"é¦¬çµ±è¨ˆè¨ˆç®—å®Œäº†: {len(horse_stats):,}é ­")
    
    # ç›¸é–¢åˆ†æ
    correlations = perform_simple_correlation_analysis(horse_stats)
    
    # å›å¸°åˆ†æ
    regression = perform_simple_regression_analysis(horse_stats)
    
    # ã€è¿½åŠ ã€‘åŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒåˆ†æ
    logger.info("ğŸ“Š REQI vs ã‚ªãƒƒã‚ºåŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒã‚’å®Ÿè¡Œä¸­...")
    effect_size_results = compare_reqi_vs_odds_effect_size(horse_stats)
    
    # çµæœ
    analysis_results = {
        'data_summary': {
            'total_records': len(combined_df),
            'horse_count': len(horse_stats),
            'file_count': file_count
        },
        'correlations': correlations,
        'regression': regression,
        'effect_size_comparison': effect_size_results
    }
    
    # ã€è¿½åŠ ã€‘ç°¡æ˜“ç‰ˆã§ã‚‚å¯è¦–åŒ–ã‚’ä½œæˆ
    logger.info("ğŸ“Š ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
    try:
        create_simple_visualizations(horse_stats, correlations, regression, Path(output_dir))
        logger.info("âœ… ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ã€ä¿®æ­£ã€‘ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆcombined_dfã‚’æ¸¡ã™ï¼‰
    generate_simple_report(analysis_results, Path(output_dir), combined_df)
    
    return analysis_results

@log_performance("ç°¡æ˜“é¦¬çµ±è¨ˆè¨ˆç®—")
def calculate_simple_horse_statistics(df: pd.DataFrame, min_races: int = 6) -> pd.DataFrame:
    """å±¤åˆ¥åˆ†æã¨åŒã˜ REQI ç®—å‡ºæ–¹æ³•ã§é¦¬ã”ã¨ã®çµ±è¨ˆå€¤ã‚’æ±‚ã‚ã‚‹ã€‚

    Args:
        df (pd.DataFrame): ãƒ¬ãƒ¼ã‚¹çµæœãƒ»ã‚ªãƒƒã‚ºãƒ»REQI ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã€‚
        min_races (int): åˆ†æå¯¾è±¡ã¨ã™ã‚‹æœ€ä½å‡ºèµ°å›æ•°ã€‚

    Returns:
        pd.DataFrame: é¦¬ã”ã¨ã®å‡ºèµ°æ•°ãƒ»å‹ç‡ãƒ»è¤‡å‹ç‡ãƒ»REQI æŒ‡æ¨™ãªã©ã‚’ã¾ã¨ã‚ãŸãƒ‡ãƒ¼ã‚¿ã€‚
    """
    # å¿…è¦ã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['é¦¬å', 'ç€é †']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
    
    # ã€çµ±ä¸€ã€‘èŠãƒ¬ãƒ¼ã‚¹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå±¤åˆ¥åˆ†æã¨çµ±ä¸€ï¼‰
    if 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰' in df.columns:
        original_count = len(df)
        df = df[df['èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰'] == 'èŠ']
        logger.info(f"ğŸ“Š èŠãƒ¬ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿: {original_count:,} â†’ {len(df):,}è¡Œ")
    
    # æ•°å€¤å¤‰æ›
    df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
    df = df[df['ç€é †'] > 0]
    
    # ã‚ªãƒƒã‚ºæƒ…å ±ã®å‡¦ç†
    if 'ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º' in df.columns:
        df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'] = pd.to_numeric(df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'], errors='coerce')
        df = df[df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'] > 0]
    
    if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in df.columns:
        df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'] = pd.to_numeric(df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'], errors='coerce')
        df = df[df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'] > 0]
    
    # ã€çµ±ä¸€ã€‘å±¤åˆ¥åˆ†æã¨åŒã˜ç€é †é‡ã¿ä»˜ãREQIè¨ˆç®—ã‚’é©ç”¨
    logger.info("ğŸ“Š å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã—ãŸç€é †é‡ã¿ä»˜ãREQIè¨ˆç®—ã‚’é©ç”¨ä¸­...")
    df_with_reqi = calculate_race_level_features_with_position_weights(df)
    
    # ã€é«˜é€ŸåŒ–ã€‘pandas groupbyã‚’ä½¿ç”¨ã—ã¦O(nÂ²)ã‚’O(n)ã«æ”¹å–„
    logger.info("ğŸš€ é«˜é€ŸåŒ–ç‰ˆé¦¬çµ±è¨ˆè¨ˆç®—ã‚’å®Ÿè¡Œä¸­ï¼ˆpandas groupbyä½¿ç”¨ï¼‰...")
    stats_calc_start = time.time()
    
    # é¦¬ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦çµ±è¨ˆã‚’ä¸€æ‹¬è¨ˆç®—
    horse_groups = df_with_reqi.groupby('é¦¬å')
    
    # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
    basic_stats = horse_groups.agg({
        'ç€é †': ['count', lambda x: (x == 1).mean(), lambda x: (x <= 3).mean()],
        'race_level': ['mean', 'max']
    }).round(6)
    
    # åˆ—åã‚’æ•´ç†
    basic_stats.columns = ['total_races', 'win_rate', 'place_rate', 'avg_race_level', 'max_race_level']
    
    # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹äºˆæ¸¬ç¢ºç‡ã®è¨ˆç®—ï¼ˆåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    odds_stats = pd.DataFrame(index=basic_stats.index)
    
    if 'ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º' in df_with_reqi.columns:
        odds_stats['avg_win_prob_from_odds'] = horse_groups['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'].apply(
            lambda x: (1 / x).mean() if len(x) > 0 else 0
        )
    else:
        odds_stats['avg_win_prob_from_odds'] = 0
    
    if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in df_with_reqi.columns:
        odds_stats['avg_place_prob_from_odds'] = horse_groups['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'].apply(
            lambda x: (1 / x).mean() if len(x) > 0 else 0
        )
    else:
        odds_stats['avg_place_prob_from_odds'] = 0
    
    # çµ±è¨ˆã‚’çµåˆ
    horse_stats_df = pd.concat([basic_stats, odds_stats], axis=1)
    
    # æœ€ä½å‡ºèµ°æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿
    horse_stats_df = horse_stats_df[horse_stats_df['total_races'] >= min_races]
    
    # é¦¬åã‚’åˆ—ã«è¿½åŠ 
    horse_stats_df['horse_name'] = horse_stats_df.index
    
    # åˆ—ã®é †åºã‚’æ•´ç†
    horse_stats_df = horse_stats_df[['horse_name', 'total_races', 'win_rate', 'place_rate', 
                                   'avg_win_prob_from_odds', 'avg_place_prob_from_odds',
                                   'avg_race_level', 'max_race_level']]
    
    stats_time = time.time() - stats_calc_start
    logger.info(f"âœ… é«˜é€ŸåŒ–ç‰ˆé¦¬çµ±è¨ˆè¨ˆç®—å®Œäº†: {len(horse_stats_df):,}é ­ ({stats_time:.2f}ç§’)")
    
    return horse_stats_df.set_index('horse_name')

def perform_simple_correlation_analysis(horse_stats: pd.DataFrame) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆç›¸é–¢åˆ†æï¼ˆå±¤åˆ¥åˆ†æã¨çµ±ä¸€ã—ãŸREQIæŒ‡æ¨™ã‚’ä½¿ç”¨ï¼‰"""
    correlations = {}
    target = 'place_rate'
    
    # ã€çµ±ä¸€ã€‘å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã—ãŸREQIæŒ‡æ¨™ã‚’ä½¿ç”¨
    variables = {
        'å¹³å‡REQI': 'avg_race_level',  # å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã®æŒ‡æ¨™
        'æœ€é«˜REQI': 'max_race_level',  # å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã®æŒ‡æ¨™
        'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬': 'avg_place_prob_from_odds',
        'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬': 'avg_win_prob_from_odds'
    }
    
    for name, var in variables.items():
        if var in horse_stats.columns:
            corr, p_value = pearsonr(horse_stats[var].fillna(0), horse_stats[target].fillna(0))
            correlations[name] = {
                'correlation': corr,
                'r_squared': corr ** 2,
                'p_value': p_value
            }
            logger.info(f"ğŸ“Š ç›¸é–¢åˆ†æ: {name} r={corr:.3f}, RÂ²={corr**2:.3f}, p={p_value:.3e}")
    
    return correlations

def perform_simple_regression_analysis(horse_stats: pd.DataFrame) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆå›å¸°åˆ†æ"""
    data = horse_stats.dropna().copy()
    if len(data) < 30:
        logger.warning("å›å¸°åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
        return {}
    
    y = data['place_rate'].values
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    split_idx = int(len(data) * 0.7)
    
    results = {}
    
    # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
    if 'avg_place_prob_from_odds' in data.columns:
        X_odds = data[['avg_place_prob_from_odds']].fillna(0).values
        X_odds_train, X_odds_test = X_odds[:split_idx], X_odds[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        model_odds = LinearRegression()
        model_odds.fit(X_odds_train, y_train)
        y_pred_odds = model_odds.predict(X_odds_test)
        
        results['odds_baseline'] = {
            'train_r2': model_odds.score(X_odds_train, y_train),
            'test_r2': r2_score(y_test, y_pred_odds),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_odds))
        }
    
    # ã€ä¿®æ­£ã€‘REQIï¼ˆå¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ï¼‰
    if 'avg_race_level' in data.columns:
        X_level = data[['avg_race_level']].fillna(0).values
        X_level_train, X_level_test = X_level[:split_idx], X_level[split_idx:]
        
        model_level = LinearRegression()
        model_level.fit(X_level_train, y_train)
        y_pred_level = model_level.predict(X_level_test)
        
        results['reqi_model'] = {
            'train_r2': model_level.score(X_level_train, y_train),
            'test_r2': r2_score(y_test, y_pred_level),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_level))
        }
    
    # ã€ä¿®æ­£ã€‘çµ±è¨ˆçš„æ¤œå®šã‚’å«ã‚€H2ä»®èª¬æ¤œè¨¼
    if 'odds_baseline' in results and 'reqi_model' in results:
        # åŸºæœ¬çš„ãªæ•°å€¤æ¯”è¼ƒ
        h2_supported = results['reqi_model']['test_r2'] > results['odds_baseline']['test_r2']
        improvement = results['reqi_model']['test_r2'] - results['odds_baseline']['test_r2']
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“è©•ä¾¡ï¼ˆæ”¹å–„å¹…ãŒ0.01ä»¥ä¸Šã‹ã¤æ­£ã®å€¤ï¼‰
        statistically_meaningful = improvement > 0.01 and h2_supported
        
        results['h2_verification'] = {
            'hypothesis_supported': h2_supported,
            'improvement': improvement,
            'statistically_meaningful': statistically_meaningful,
            'warning': 'æœ¬åˆ†æã¯ç°¡æ˜“ç‰ˆã§ã™ã€‚å³å¯†ãªçµ±è¨ˆçš„æ¤œå®šã«ã¯OddsComparisonAnalyzerã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚'
        }
    
    return results

def compare_reqi_vs_odds_effect_size(df: pd.DataFrame) -> Dict[str, Any]:
    """
    REQIã¨ã‚ªãƒƒã‚ºã®åŠ¹æœã‚µã‚¤ã‚ºï¼ˆCohen's dï¼‰ã‚’æ¯”è¼ƒã™ã‚‹
    
    Parameters
    ----------
    df : pd.DataFrame
        é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ï¼ˆREQIã€ã‚ªãƒƒã‚ºã€è¤‡å‹ç‡ç­‰ã‚’å«ã‚€ï¼‰
    
    Returns
    -------
    Dict[str, Any]
        REQIã¨ã‚ªãƒƒã‚ºã®åŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒçµæœ
    """
    logger.info("ğŸ“Š REQI vs ã‚ªãƒƒã‚ºåŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒã‚’é–‹å§‹...")
    
    results = {}
    
    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['avg_race_level', 'avg_place_prob_from_odds', 'place_rate']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        logger.warning(f"âš ï¸ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
        return {}
    
    # ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    clean_df = df[required_cols].dropna()
    logger.info(f"   åˆ†æå¯¾è±¡: {len(clean_df):,}é ­")
    
    if len(clean_df) < 100:
        logger.warning("âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ï¼ˆ100é ­æœªæº€ï¼‰")
        return {}
    
    # 1. REQIåŠ¹æœã‚µã‚¤ã‚ºã®è¨ˆç®—
    logger.info("ğŸ” REQIåŠ¹æœã‚µã‚¤ã‚ºè¨ˆç®—ä¸­...")
    reqi_median = clean_df['avg_race_level'].median()
    high_reqi = clean_df[clean_df['avg_race_level'] >= reqi_median]
    low_reqi = clean_df[clean_df['avg_race_level'] < reqi_median]
    
    reqi_high_rate = high_reqi['place_rate'].mean()
    reqi_low_rate = low_reqi['place_rate'].mean()
    
    # Cohen's dè¨ˆç®—ï¼ˆREQIï¼‰
    reqi_pooled_std = np.sqrt(((len(high_reqi)-1)*high_reqi['place_rate'].var() + 
                              (len(low_reqi)-1)*low_reqi['place_rate'].var()) / 
                             (len(high_reqi)+len(low_reqi)-2))
    reqi_cohens_d = (reqi_high_rate - reqi_low_rate) / reqi_pooled_std
    
    # 2. ã‚ªãƒƒã‚ºåŠ¹æœã‚µã‚¤ã‚ºã®è¨ˆç®—
    logger.info("ğŸ” ã‚ªãƒƒã‚ºåŠ¹æœã‚µã‚¤ã‚ºè¨ˆç®—ä¸­...")
    odds_median = clean_df['avg_place_prob_from_odds'].median()
    high_odds = clean_df[clean_df['avg_place_prob_from_odds'] >= odds_median]  # é«˜ç¢ºç‡=äººæ°—
    low_odds = clean_df[clean_df['avg_place_prob_from_odds'] < odds_median]   # ä½ç¢ºç‡=ä¸äººæ°—
    
    odds_high_rate = high_odds['place_rate'].mean()
    odds_low_rate = low_odds['place_rate'].mean()
    
    # Cohen's dè¨ˆç®—ï¼ˆã‚ªãƒƒã‚ºï¼‰
    odds_pooled_std = np.sqrt(((len(high_odds)-1)*high_odds['place_rate'].var() + 
                              (len(low_odds)-1)*low_odds['place_rate'].var()) / 
                             (len(high_odds)+len(low_odds)-2))
    odds_cohens_d = (odds_high_rate - odds_low_rate) / odds_pooled_std
    
    # 3. åŠ¹æœã‚µã‚¤ã‚ºã®è§£é‡ˆ
    def interpret_effect_size(d):
        if d < 0.2:
            return "å°åŠ¹æœ"
        elif d < 0.5:
            return "ä¸­åŠ¹æœ"
        elif d < 0.8:
            return "å¤§åŠ¹æœ"
        else:
            return "éå¸¸ã«å¤§åŠ¹æœ"
    
    reqi_interpretation = interpret_effect_size(reqi_cohens_d)
    odds_interpretation = interpret_effect_size(odds_cohens_d)
    
    # 4. çµæœã®æ•´ç†
    results = {
        'reqi_effect': {
            'high_group_rate': reqi_high_rate,
            'low_group_rate': reqi_low_rate,
            'rate_difference': reqi_high_rate - reqi_low_rate,
            'cohens_d': reqi_cohens_d,
            'interpretation': reqi_interpretation,
            'sample_size': len(high_reqi) + len(low_reqi)
        },
        'odds_effect': {
            'high_group_rate': odds_high_rate,
            'low_group_rate': odds_low_rate,
            'rate_difference': odds_high_rate - odds_low_rate,
            'cohens_d': odds_cohens_d,
            'interpretation': odds_interpretation,
            'sample_size': len(high_odds) + len(low_odds)
        },
        'comparison': {
            'reqi_vs_odds_ratio': reqi_cohens_d / odds_cohens_d if odds_cohens_d != 0 else np.nan,
            'odds_superior': odds_cohens_d > reqi_cohens_d,
            'both_significant': reqi_cohens_d >= 0.2 and odds_cohens_d >= 0.2
        }
    }
    
    # 5. ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"ğŸ“ˆ REQIåŠ¹æœã‚µã‚¤ã‚º: Cohen's d = {reqi_cohens_d:.3f} ({reqi_interpretation})")
    logger.info(f"   - é«˜REQIç¾¤: {reqi_high_rate:.1%}, ä½REQIç¾¤: {reqi_low_rate:.1%}")
    logger.info(f"ğŸ“ˆ ã‚ªãƒƒã‚ºåŠ¹æœã‚µã‚¤ã‚º: Cohen's d = {odds_cohens_d:.3f} ({odds_interpretation})")
    logger.info(f"   - äººæ°—é¦¬ç¾¤: {odds_high_rate:.1%}, ä¸äººæ°—é¦¬ç¾¤: {odds_low_rate:.1%}")
    
    if results['comparison']['odds_superior']:
        logger.info("âœ… ã‚ªãƒƒã‚ºã®æ–¹ãŒåŠ¹æœãŒå¤§ãã„")
    else:
        logger.info("âœ… REQIã®æ–¹ãŒåŠ¹æœãŒå¤§ãã„")
    
    return results

def calculate_betting_performance(combined_df: pd.DataFrame, strategy: str = 'odds', 
                                  train_end_year: int = 2023, test_year: int = 2024,
                                  min_races: int = 6) -> Dict[str, Any]:
    """
    æ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹æŠ•è³‡æˆ¦ç•¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæƒ…å ±æ¼æ´©ãªã—ï¼‰
    
    Parameters
    ----------
    combined_df : pd.DataFrame
        å…¨æœŸé–“ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆå¹´ã€é¦¬åã€ç€é †ã€ã‚ªãƒƒã‚ºã€race_levelç­‰ã‚’å«ã‚€ï¼‰
    strategy : str
        'odds': ã‚ªãƒƒã‚ºã®ã¿
        'reqi': REQIã®ã¿
        'integrated': çµ±åˆï¼ˆã‚ªãƒƒã‚º+REQIï¼‰
    train_end_year : int
        è¨“ç·´æœŸé–“ã®çµ‚äº†å¹´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023ï¼‰
    test_year : int
        ãƒ†ã‚¹ãƒˆå¹´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2024ï¼‰
    min_races : int
        æœ€ä½å‡ºèµ°å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 6ï¼‰
    
    Returns
    -------
    Dict[str, Any]
        çš„ä¸­ç‡ã€å¹³å‡é…å½“ã€å›åç‡ã€æŠ•è³‡é¡ã€å›åé¡ã€æç›Š
    """
    logger.info(f"ğŸ“Š æ™‚ç³»åˆ—åˆ†å‰²æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {strategy}")
    logger.info(f"   è¨“ç·´æœŸé–“: ~{train_end_year}å¹´, ãƒ†ã‚¹ãƒˆæœŸé–“: {test_year}å¹´")
    
    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['å¹´', 'é¦¬å', 'ç€é †']
    missing_cols = [col for col in required_cols if col not in combined_df.columns]
    if missing_cols:
        logger.warning(f"âš ï¸ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
        return {}
    
    # è¨“ç·´æœŸé–“ã¨ãƒ†ã‚¹ãƒˆæœŸé–“ã«åˆ†å‰²
    train_df = combined_df[combined_df['å¹´'] <= train_end_year].copy()
    test_df = combined_df[combined_df['å¹´'] == test_year].copy()
    
    logger.info(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df):,}ãƒ¬ãƒ¼ã‚¹")
    logger.info(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}ãƒ¬ãƒ¼ã‚¹")
    
    if len(train_df) == 0 or len(test_df) == 0:
        logger.warning("âš ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶")
        return {}
    
    # è¨“ç·´æœŸé–“ã§é¦¬çµ±è¨ˆã‚’è¨ˆç®—
    logger.info("   ğŸ“Š è¨“ç·´æœŸé–“ã§é¦¬çµ±è¨ˆã‚’è¨ˆç®—ä¸­...")
    train_df['place_flag'] = (train_df['ç€é †'] <= 3).astype(int)
    
    horse_stats_train = train_df.groupby('é¦¬å').agg({
        'ç€é †': 'count',  # å‡ºèµ°å›æ•°
        'place_flag': 'mean'  # è¤‡å‹ç‡
    })
    horse_stats_train.columns = ['total_races', 'place_rate_train']
    
    # ã‚ªãƒƒã‚ºã¨REQIã®å¹³å‡ã‚’è¨ˆç®—
    if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in train_df.columns:
        odds_stats = train_df.groupby('é¦¬å')['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'].mean()
        horse_stats_train['avg_place_odds'] = odds_stats
        horse_stats_train['avg_place_prob_from_odds'] = (1.0 / horse_stats_train['avg_place_odds']).clip(0, 1)
    
    if 'race_level' in train_df.columns:
        reqi_stats = train_df.groupby('é¦¬å')['race_level'].mean()
        horse_stats_train['avg_race_level'] = reqi_stats
    
    # æœ€ä½å‡ºèµ°å›æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿
    horse_stats_train = horse_stats_train[horse_stats_train['total_races'] >= min_races]
    logger.info(f"   âœ… è¨“ç·´æœŸé–“ã®é¦¬çµ±è¨ˆ: {len(horse_stats_train):,}é ­")
    
    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®å®Ÿéš›ã®çµæœã‚’æº–å‚™
    logger.info("   ğŸ“Š ãƒ†ã‚¹ãƒˆæœŸé–“ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")
    test_df['place_flag'] = (test_df['ç€é †'] <= 3).astype(int)
    
    # ä¸Šä½20%ã‚’é¸æŠï¼ˆè¨“ç·´æœŸé–“ã®çµ±è¨ˆã§åˆ¤æ–­ï¼‰
    top_pct = 0.2
    n_top = max(1, int(len(horse_stats_train) * top_pct))
    
    if strategy == 'odds':
        if 'avg_place_prob_from_odds' not in horse_stats_train.columns:
            logger.warning("âš ï¸ ã‚ªãƒƒã‚ºæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        data_clean = horse_stats_train.dropna(subset=['avg_place_prob_from_odds'])
        top_horses_list = data_clean.nlargest(n_top, 'avg_place_prob_from_odds').index.tolist()
        
    elif strategy == 'reqi':
        if 'avg_race_level' not in horse_stats_train.columns:
            logger.warning("âš ï¸ REQIæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        data_clean = horse_stats_train.dropna(subset=['avg_race_level'])
        top_horses_list = data_clean.nlargest(n_top, 'avg_race_level').index.tolist()
        
    elif strategy == 'integrated':
        required_cols = ['avg_place_prob_from_odds', 'avg_race_level']
        if not all(col in horse_stats_train.columns for col in required_cols):
            logger.warning(f"âš ï¸ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {required_cols}")
            return {}
        
        data_clean = horse_stats_train.dropna(subset=required_cols).copy()
        
        # æ­£è¦åŒ–ï¼ˆè¨“ç·´æœŸé–“ã®çµ±è¨ˆã§ï¼‰
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        
        data_clean['odds_normalized'] = scaler.fit_transform(data_clean[['avg_place_prob_from_odds']])
        data_clean['reqi_normalized'] = scaler.fit_transform(data_clean[['avg_race_level']])
        
        # çµ±åˆã‚¹ã‚³ã‚¢ï¼ˆã‚ªãƒƒã‚º64%ã€REQI 36%ã®é‡ã¿ - æ€§èƒ½æ¯”ã«åŸºã¥ãè«–ç†çš„é…åˆ†ï¼‰
        data_clean['integrated_score'] = (0.64 * data_clean['odds_normalized'] + 
                                         0.36 * data_clean['reqi_normalized'])
        
        top_horses_list = data_clean.nlargest(n_top, 'integrated_score').index.tolist()
    else:
        logger.error(f"âŒ ä¸æ˜ãªæˆ¦ç•¥: {strategy}")
        return {}
    
    if len(top_horses_list) == 0:
        logger.warning(f"âš ï¸ æˆ¦ç•¥ {strategy} ã§å¯¾è±¡é¦¬ãŒ0é ­")
        return {}
    
    logger.info(f"   âœ… é¸æŠé¦¬: {len(top_horses_list):,}é ­")
    
    # ã“ã‚Œã‚‰ã®é¦¬ãŒ2024å¹´ã«å‡ºèµ°ã—ãŸãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—
    test_races = test_df[test_df['é¦¬å'].isin(top_horses_list)].copy()
    
    if len(test_races) == 0:
        logger.warning(f"âš ï¸ æˆ¦ç•¥ {strategy} ã§2024å¹´ã®å‡ºèµ°ãƒ¬ãƒ¼ã‚¹ãŒ0ä»¶")
        return {}
    
    logger.info(f"   ğŸ“Š 2024å¹´ã®æŠ•è³‡å¯¾è±¡ãƒ¬ãƒ¼ã‚¹: {len(test_races):,}ãƒ¬ãƒ¼ã‚¹")
    
    # ãƒ¬ãƒ¼ã‚¹å˜ä½ã®æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    target_investment = 1000000  # ç›®æ¨™æŠ•è³‡é¡100ä¸‡å††
    bet_per_race = target_investment / len(test_races)
    total_investment = len(test_races) * bet_per_race
    
    # çš„ä¸­ãƒ¬ãƒ¼ã‚¹ï¼ˆ3ç€ä»¥å†…ï¼‰
    win_races = test_races[test_races['place_flag'] == 1]
    hit_count = len(win_races)
    hit_rate = hit_count / len(test_races)
    
    # ç·æ‰•æˆ»é¡ï¼ˆé…å½“ Ã— è³­ã‘é‡‘ï¼‰
    if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in win_races.columns:
        total_return = (win_races['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'] * bet_per_race).sum()
        avg_payout = win_races['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'].mean()
    else:
        total_return = 0
        avg_payout = 0
    
    roi = total_return / total_investment if total_investment > 0 else 0
    profit_loss = total_return - total_investment
    
    results = {
        'strategy': strategy,
        'train_period': f'~{train_end_year}å¹´',
        'test_period': f'{test_year}å¹´',
        'sample_size': len(top_horses_list),
        'total_races': len(test_races),
        'hit_races': hit_count,
        'hit_rate': hit_rate,
        'avg_payout': avg_payout,
        'roi': roi,
        'investment': total_investment,
        'return_amount': total_return,
        'profit_loss': profit_loss
    }
    
    logger.info(f"  ğŸ“Š æŠ•è³‡å¯¾è±¡: {len(test_races):,}ãƒ¬ãƒ¼ã‚¹")
    logger.info(f"  ğŸ“ˆ çš„ä¸­: {hit_count:,}å› / {len(test_races):,}ãƒ¬ãƒ¼ã‚¹ ({hit_rate*100:.1f}%)")
    logger.info(f"  ğŸ“ˆ å¹³å‡é…å½“: {avg_payout:.2f}å€")
    logger.info(f"  ğŸ“ˆ å›åç‡: {roi*100:.1f}%")
    logger.info(f"  ğŸ’° æç›Š: {profit_loss:+,.0f}å††")
    
    return results

def generate_betting_performance_section(combined_df: pd.DataFrame, train_end_year: int = 2023, 
                                        test_year: int = 2024, min_races: int = 6) -> str:
    """
    æ™‚ç³»åˆ—åˆ†å‰²æŠ•è³‡æˆ¦ç•¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
    
    Parameters
    ----------
    combined_df : pd.DataFrame
        å…¨æœŸé–“ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
    train_end_year : int
        è¨“ç·´æœŸé–“ã®çµ‚äº†å¹´
    test_year : int
        ãƒ†ã‚¹ãƒˆå¹´
    min_races : int
        æœ€ä½å‡ºèµ°å›æ•°
    
    Returns
    -------
    str
        ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
    """
    logger.info("ğŸ“‹ æ™‚ç³»åˆ—åˆ†å‰²æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...")
    
    # 3ã¤ã®æˆ¦ç•¥ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    strategies = ['odds', 'reqi', 'integrated']
    results = {}
    
    for strategy in strategies:
        result = calculate_betting_performance(combined_df, strategy, train_end_year, test_year, min_races)
        if result:
            results[strategy] = result
    
    if not results:
        return "\n## 5. æ™‚ç³»åˆ—åˆ†å‰²ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆ2024å¹´äºˆæ¸¬ï¼‰\n\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_lines = []
    report_lines.append("\n## 5. æ™‚ç³»åˆ—åˆ†å‰²ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆ2024å¹´äºˆæ¸¬ï¼‰")
    report_lines.append("")
    report_lines.append("### 5.1 åˆ†æè¨­è¨ˆ")
    report_lines.append("")
    report_lines.append("**ç›®çš„**: æƒ…å ±æ¼æ´©ã‚’æ’é™¤ã—ãŸæ­£ã—ã„äºˆæ¸¬è©•ä¾¡")
    report_lines.append("")
    report_lines.append(f"- **è¨“ç·´æœŸé–“**: ~{train_end_year}å¹´ã®ãƒ‡ãƒ¼ã‚¿ã§é¦¬çµ±è¨ˆã‚’è¨ˆç®—")
    report_lines.append(f"- **ãƒ†ã‚¹ãƒˆæœŸé–“**: {test_year}å¹´ã®ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ãƒ»è©•ä¾¡")
    report_lines.append("- **æ–¹æ³•**: è¨“ç·´æœŸé–“ã®çµ±è¨ˆã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆæœŸé–“ã‚’äºˆæ¸¬")
    report_lines.append("- **æƒ…å ±æ¼æ´©**: ãªã—ï¼ˆæœªæ¥ã®æƒ…å ±ã¯ä¸€åˆ‡ä½¿ç”¨ã—ã¦ã„ãªã„ï¼‰")
    report_lines.append("")
    report_lines.append("**æŠ•è³‡æˆ¦ç•¥ï¼ˆãƒ¬ãƒ¼ã‚¹å˜ä½ï¼‰**:")
    report_lines.append("1. è¨“ç·´æœŸé–“ï¼ˆ~2023å¹´ï¼‰ã§ä¸Šä½20%ã®é¦¬ã‚’é¸æŠ")
    report_lines.append("2. ãã®é¦¬ãŸã¡ãŒ2024å¹´ã«å‡ºèµ°ã—ãŸå…¨ãƒ¬ãƒ¼ã‚¹ã«è¤‡å‹æŠ•è³‡")
    report_lines.append("3. å„ãƒ¬ãƒ¼ã‚¹ã«å‡ç­‰é¡ã‚’æŠ•è³‡ï¼ˆç›®æ¨™100ä¸‡å†† Ã· ãƒ¬ãƒ¼ã‚¹æ•°ï¼‰")
    report_lines.append("4. 3ç€ä»¥å†…ã§çš„ä¸­ã€ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºã§æ‰•æˆ»")
    report_lines.append("")
    report_lines.append("- **ã‚ªãƒƒã‚ºã®ã¿**: è¨“ç·´æœŸé–“ã®è¤‡å‹ã‚ªãƒƒã‚ºäºˆæ¸¬ä¸Šä½20%ã®é¦¬")
    report_lines.append("- **REQIã®ã¿**: è¨“ç·´æœŸé–“ã®REQIä¸Šä½20%ã®é¦¬")
    report_lines.append("- **çµ±åˆæˆ¦ç•¥**: ã‚ªãƒƒã‚º70% + REQI30%ã‚¹ã‚³ã‚¢ä¸Šä½20%ã®é¦¬")
    report_lines.append("")
    report_lines.append("### 5.2 æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœï¼ˆãƒ¬ãƒ¼ã‚¹å˜ä½ï¼‰")
    report_lines.append("")
    report_lines.append("| æˆ¦ç•¥ | ãƒ¬ãƒ¼ã‚¹æ•° | çš„ä¸­æ•° | çš„ä¸­ç‡ | å¹³å‡é…å½“ | å›åç‡ | æŠ•è³‡é¡ | å›åé¡ | æç›Š |")
    report_lines.append("|-----|---------|-------|-------|---------|-------|-------|-------|------|")
    
    # æˆ¦ç•¥åã®ãƒãƒƒãƒ”ãƒ³ã‚°
    strategy_names = {
        'odds': 'ã‚ªãƒƒã‚ºã®ã¿',
        'reqi': 'REQIã®ã¿',
        'integrated': '**çµ±åˆ**'
    }
    
    for strategy in strategies:
        if strategy not in results:
            continue
        
        r = results[strategy]
        name = strategy_names.get(strategy, strategy)
        
        report_lines.append(
            f"| {name} | "
            f"{r.get('total_races', 0):,}ãƒ¬ãƒ¼ã‚¹ | "
            f"{r.get('hit_races', 0):,}å› | "
            f"{r['hit_rate']*100:.1f}% | "
            f"{r['avg_payout']:.2f}å€ | "
            f"{r['roi']*100:.1f}% | "
            f"{r['investment']/10000:.0f}ä¸‡å†† | "
            f"{r['return_amount']/10000:.1f}ä¸‡å†† | "
            f"{r['profit_loss']/10000:+.1f}ä¸‡å†† |"
        )
    
    report_lines.append("")
    
    # æ”¹å–„åŠ¹æœã®è¨ˆç®—
    if 'odds' in results and 'integrated' in results:
        hit_rate_improvement = (results['integrated']['hit_rate'] - 
                               results['odds']['hit_rate']) * 100
        roi_improvement = (results['integrated']['roi'] - 
                          results['odds']['roi']) * 100
        profit_improvement = (results['integrated']['profit_loss'] - 
                             results['odds']['profit_loss']) / 10000
        
        report_lines.append("**æ”¹å–„åŠ¹æœ**:")
        report_lines.append(f"- çš„ä¸­ç‡: {hit_rate_improvement:+.1f}ptï¼ˆ{results['odds']['hit_rate']*100:.1f}% â†’ {results['integrated']['hit_rate']*100:.1f}%ï¼‰")
        report_lines.append(f"- å›åç‡: {roi_improvement:+.1f}ptï¼ˆ{results['odds']['roi']*100:.1f}% â†’ {results['integrated']['roi']*100:.1f}%ï¼‰")
        report_lines.append(f"- æç›Š: {profit_improvement:+.1f}ä¸‡å††ï¼ˆ{results['odds']['profit_loss']/10000:+.1f}ä¸‡å†† â†’ {results['integrated']['profit_loss']/10000:+.1f}ä¸‡å††ï¼‰")
        report_lines.append("")
    
    report_lines.append("### 5.3 å®Ÿå‹™çš„è§£é‡ˆ")
    report_lines.append("")
    report_lines.append("**ãƒã‚¸ãƒ†ã‚£ãƒ–é¢**:")
    report_lines.append("- âœ… ãƒ¬ãƒ¼ã‚¹å˜ä½ã®å®ŸæŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå®Ÿéš›ã®è³­ã‘æ–¹ã«åŸºã¥ãè©•ä¾¡ï¼‰")
    report_lines.append("- âœ… æ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚Šæƒ…å ±æ¼æ´©ãªã—ã§è©•ä¾¡")
    report_lines.append("- âœ… è¨“ç·´æœŸé–“ã®çŸ¥è­˜ã®ã¿ã§2024å¹´ã‚’æ­£ã—ãäºˆæ¸¬")
    
    # ãƒ¬ãƒ¼ã‚¹æ•°ã®æƒ…å ±ã‚’è¿½åŠ 
    if 'integrated' in results and results['integrated'].get('total_races', 0) > 0:
        total_races = results['integrated']['total_races']
        report_lines.append(f"- ğŸ“Š å®ŸæŠ•è³‡å¯¾è±¡: 2024å¹´ã®{total_races:,}ãƒ¬ãƒ¼ã‚¹")
    
    if 'integrated' in results and 'odds' in results:
        if roi_improvement > 0:
            report_lines.append(f"- âœ… çµ±åˆæˆ¦ç•¥ãŒã‚ªãƒƒã‚ºå˜ç‹¬ã‚ˆã‚Šå„ªä½ï¼ˆå›åç‡{roi_improvement:+.1f}ptæ”¹å–„ï¼‰")
            if profit_improvement > 0:
                report_lines.append(f"- ğŸ’° æå¤±å‰Šæ¸›åŠ¹æœ: {profit_improvement:.1f}ä¸‡å††")
        else:
            report_lines.append(f"- âš ï¸ çµ±åˆæˆ¦ç•¥ã®æ”¹å–„ã¯é™å®šçš„ï¼ˆå›åç‡{roi_improvement:+.1f}ptï¼‰")
    
    report_lines.append("")
    report_lines.append("**åˆ¶ç´„äº‹é …**:")
    
    if 'integrated' in results and results['integrated']['roi'] < 1.0:
        report_lines.append("- âš ï¸ å›åç‡100%è¶…ãˆã«ã¯è‡³ã‚‰ãšã€æŠ•è³‡æˆ¦ç•¥ã¨ã—ã¦ã¯åç›Šæ€§ä¸è¶³")
    
    report_lines.append("- å®Ÿé‹ç”¨ã§ã¯æ‰‹æ•°æ–™ï¼ˆç´„25%ï¼‰ãƒ»ç¨é‡‘ã‚’è€ƒæ…®ã™ã‚‹ã¨ã€ã•ã‚‰ã«åç›Šæ€§ã¯ä½ä¸‹")
    report_lines.append("- REQIã¯ã€Œè£œåŠ©æŒ‡æ¨™ã€ã¨ã—ã¦ã®ä½ç½®ã¥ã‘ãŒå¦¥å½“")
    report_lines.append("")
    report_lines.append("### 5.4 çµè«–")
    report_lines.append("")
    report_lines.append("- âœ… **ãƒ¬ãƒ¼ã‚¹å˜ä½ã®å®ŸæŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**ã«ã‚ˆã‚‹ç¾å®Ÿçš„ãªè©•ä¾¡")
    report_lines.append("- âœ… **æ­£ã—ã„æ™‚ç³»åˆ—åˆ†å‰²ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ**ã«ã‚ˆã‚Šæƒ…å ±æ¼æ´©ã‚’å®Œå…¨ã«æ’é™¤")
    report_lines.append(f"- ğŸ“Š è¨“ç·´æœŸé–“ï¼ˆ~{train_end_year}å¹´ï¼‰â†’ ãƒ†ã‚¹ãƒˆæœŸé–“ï¼ˆ{test_year}å¹´ï¼‰ã®å³å¯†ãªæ¤œè¨¼")
    
    if 'integrated' in results and 'odds' in results:
        if roi_improvement > 0:
            report_lines.append(f"- âœ… REQIãŒã‚ªãƒƒã‚ºã‚’è£œå®Œã™ã‚‹åŠ¹æœã‚’ç¢ºèªï¼ˆå›åç‡{roi_improvement:+.1f}ptæ”¹å–„ï¼‰")
        else:
            report_lines.append("- âš ï¸ REQIã®è£œå®ŒåŠ¹æœã¯é™å®šçš„ã ãŒã€äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦ç´ ã¨ã—ã¦æœ‰ç”¨")
    
    report_lines.append("- ğŸ’¡ REQIã¯å˜ç‹¬ã§ã®åç›ŠåŒ–ã¯å›°é›£ã ãŒã€å¤šå¤‰é‡ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡ã¨ã—ã¦è²¢çŒ®")
    report_lines.append("")
    
    return "\n".join(report_lines)

def create_simple_visualizations(horse_stats: pd.DataFrame, correlations: Dict[str, Any], 
                                regression: Dict[str, Any], output_dir: Path):
    """ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºåˆ†æã®å¯è¦–åŒ–ä½œæˆ"""
    try:
        import matplotlib  # noqa: WPS433 (runtime import required)
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt  # noqa: WPS433
    except ImportError as import_error:
        logger.error(f"âŒ matplotlibã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {import_error}")
        logger.info("å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        return

    from horse_racing.utils.font_config import setup_japanese_fonts  # noqa: WPS433
    setup_japanese_fonts(suppress_warnings=True)

    try:
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        viz_dir = output_dir / "odds_comparison"
        viz_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ ç°¡æ˜“ç‰ˆå¯è¦–åŒ–å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {viz_dir}")

        # 1. ç›¸é–¢æ•£å¸ƒå›³
        logger.info("ğŸ“Š ç°¡æ˜“ç‰ˆç›¸é–¢æ•£å¸ƒå›³ã‚’ä½œæˆä¸­...")
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®ç›¸é–¢åˆ†æ', fontsize=16, fontweight='bold')

        # å¹³å‡REQI vs è¤‡å‹ç‡
        if 'avg_race_level' in horse_stats.columns and 'place_rate' in horse_stats.columns:
            axes[0, 0].scatter(horse_stats['avg_race_level'], horse_stats['place_rate'], alpha=0.6, s=20, color='blue')
            axes[0, 0].set_xlabel('å¹³å‡REQI')
            axes[0, 0].set_ylabel('è¤‡å‹ç‡')

            reqi_corr = correlations.get('å¹³å‡REQI', {}).get('correlation', 0)
            axes[0, 0].set_title(f'å¹³å‡REQI vs è¤‡å‹ç‡ (r={reqi_corr:.3f})')

        # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬ vs è¤‡å‹ç‡
        if 'avg_place_prob_from_odds' in horse_stats.columns and 'place_rate' in horse_stats.columns:
            axes[0, 1].scatter(horse_stats['avg_place_prob_from_odds'], horse_stats['place_rate'], alpha=0.6, s=20, color='green')
            axes[0, 1].set_xlabel('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬')
            axes[0, 1].set_ylabel('è¤‡å‹ç‡')

            odds_place_corr = correlations.get('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬', {}).get('correlation', 0)
            axes[0, 1].set_title(f'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬ vs è¤‡å‹ç‡ (r={odds_place_corr:.3f})')

        # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬ vs è¤‡å‹ç‡
        if 'avg_win_prob_from_odds' in horse_stats.columns and 'place_rate' in horse_stats.columns:
            axes[1, 0].scatter(horse_stats['avg_win_prob_from_odds'], horse_stats['place_rate'], alpha=0.6, s=20, color='orange')
            axes[1, 0].set_xlabel('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬')
            axes[1, 0].set_ylabel('è¤‡å‹ç‡')

            odds_win_corr = correlations.get('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬', {}).get('correlation', 0)
            axes[1, 0].set_title(f'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬ vs è¤‡å‹ç‡ (r={odds_win_corr:.3f})')

        # ç©ºã®4ç•ªç›®ã®ãƒ—ãƒ­ãƒƒãƒˆ
        axes[1, 1].text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«\nçµ±è¨ˆæƒ…å ±', ha='center', va='center', fontsize=14)
        axes[1, 1].text(0.5, 0.3, f'åˆ†æå¯¾è±¡: {len(horse_stats):,}é ­', ha='center', va='center', fontsize=12)
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].set_title('åˆ†ææ¦‚è¦')

        plt.tight_layout()
        scatter_plot_path = viz_dir / 'correlation_scatter_plots.png'
        plt.savefig(
            scatter_plot_path,
            dpi=300,
            bbox_inches='tight',
            facecolor='white',
            edgecolor='none',
            format='png',
            pad_inches=0.1,
        )
        plt.close()
        logger.info(f"âœ… ç›¸é–¢æ•£å¸ƒå›³ã‚’ä¿å­˜: {scatter_plot_path}")

        # 2. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆH2ä»®èª¬æ¤œè¨¼ï¼‰
        if regression and 'h2_verification' in regression:
            logger.info("ğŸ“Š H2ä»®èª¬æ¤œè¨¼ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆä¸­...")
            h2_results = regression['h2_verification']

            model_names = ['ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³', 'å¹³å‡REQI']
            r2_scores = [
                regression.get('odds_baseline', {}).get('test_r2', 0),
                regression.get('reqi_model', {}).get('test_r2', 0),
            ]

            plt.figure(figsize=(10, 6))
            bars = plt.bar(model_names, r2_scores, color=['#ff7f0e', '#2ca02c'])
            plt.ylabel('RÂ² (æ±ºå®šä¿‚æ•°)')
            plt.title('H2ä»®èª¬æ¤œè¨¼: å¹³å‡REQI ã®äºˆæ¸¬æ€§èƒ½')
            plt.ylim(0, max(r2_scores) * 1.2 if max(r2_scores) > 0 else 1)

            for bar, score in zip(bars, r2_scores):
                plt.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + max(r2_scores) * 0.01,
                    f'{score:.4f}',
                    ha='center',
                    va='bottom',
                    fontweight='bold',
                )

            if h2_results.get('hypothesis_supported', False):
                result_text = f"âœ… H2ä»®èª¬ã‚µãƒãƒ¼ãƒˆ\næ”¹å–„: {h2_results.get('improvement', 0):+.4f}"
                plt.text(
                    0.7,
                    max(r2_scores) * 0.8,
                    result_text,
                    fontsize=12,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"),
                )
            else:
                result_text = f"âŒ H2ä»®èª¬éã‚µãƒãƒ¼ãƒˆ\næ”¹å–„: {h2_results.get('improvement', 0):+.4f}"
                plt.text(
                    0.7,
                    max(r2_scores) * 0.8,
                    result_text,
                    fontsize=12,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral"),
                )

            plt.tight_layout()
            performance_plot_path = viz_dir / 'model_performance_comparison.png'
            plt.savefig(
                performance_plot_path,
                dpi=300,
                bbox_inches='tight',
                facecolor='white',
                edgecolor='none',
                format='png',
                pad_inches=0.1,
            )
            plt.close()
            logger.info(f"âœ… H2ä»®èª¬æ¤œè¨¼ãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜: {performance_plot_path}")

        created_files = list(viz_dir.glob("*.png"))
        if created_files:
            logger.info("ğŸ“ ä½œæˆã•ã‚ŒãŸç°¡æ˜“ç‰ˆå¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«:")
            for file_path in created_files:
                logger.info(f"   - {file_path.name}")

    except Exception as plot_error:
        logger.error(f"âŒ ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {plot_error}")
        try:
            plt.close('all')
        except Exception:
            pass

def generate_simple_report(results: Dict[str, Any], output_dir: Path, combined_df: pd.DataFrame = None):
    """ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ™‚ç³»åˆ—åˆ†å‰²æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    output_dir.mkdir(parents=True, exist_ok=True)
    report_path = output_dir / "horse_REQI_odds_analysis_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("**å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: analyze_horse_REQI.py\n\n")
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
        if 'data_summary' in results:
            f.write("## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
            summary = results['data_summary']
            f.write(f"- **ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: {summary.get('total_records', 'N/A'):,}\n")
            f.write(f"- **åˆ†æå¯¾è±¡é¦¬æ•°**: {summary.get('horse_count', 'N/A'):,}\n")
            f.write(f"- **å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {summary.get('file_count', 'N/A')}\n\n")
        
        # é‡ã¿æƒ…å ±
        try:
            from horse_racing.core.weight_manager import get_global_weights
            weights = get_global_weights()
            f.write("## REQIé‡ã¿æƒ…å ±\n\n")
            f.write("**è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã§ç®—å‡ºã•ã‚ŒãŸå›ºå®šé‡ã¿**:\n\n")
            f.write(f"- **ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿**: {weights['grade_weight']:.3f} ({weights['grade_weight']*100:.1f}%)\n")
            f.write(f"- **å ´æ‰€é‡ã¿**: {weights['venue_weight']:.3f} ({weights['venue_weight']*100:.1f}%)\n")
            f.write(f"- **è·é›¢é‡ã¿**: {weights['distance_weight']:.3f} ({weights['distance_weight']*100:.1f}%)\n\n")
            f.write("**é‡ã¿ç®—å‡ºæ–¹æ³•**: å„è¦ç´ ã¨å‹ç‡ï¼ˆwin_rateï¼‰ã®ç›¸é–¢ä¿‚æ•°ã®2ä¹—ã‚’æ­£è¦åŒ–\n\n")
        except Exception as e:
            logger.warning(f"âš ï¸ é‡ã¿æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
            f.write("## REQIé‡ã¿æƒ…å ±\n\n")
            f.write("**å›ºå®šé‡ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ï¼‰**:\n\n")
            f.write("- **ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿**: 0.636 (63.6%)\n")
            f.write("- **å ´æ‰€é‡ã¿**: 0.323 (32.3%)\n")
            f.write("- **è·é›¢é‡ã¿**: 0.041 (4.1%)\n\n")
        
        # ç›¸é–¢åˆ†æçµæœ
        if 'correlations' in results:
            f.write("## ç›¸é–¢åˆ†æçµæœ\n\n")
            f.write("| å¤‰æ•° | ç›¸é–¢ä¿‚æ•° | RÂ² | på€¤ |\n")
            f.write("|------|----------|----|---------|\n")
            
            for name, corr in results['correlations'].items():
                f.write(f"| {name} | {corr['correlation']:.3f} | {corr['r_squared']:.3f} | {corr['p_value']:.3e} |\n")
            f.write("\n")
        
        # å›å¸°åˆ†æçµæœ
        if 'regression' in results:
            f.write("## å›å¸°åˆ†æçµæœï¼ˆH2ä»®èª¬æ¤œè¨¼ï¼‰\n\n")
            regression = results['regression']
            
            f.write("| ãƒ¢ãƒ‡ãƒ« | è¨“ç·´RÂ² | æ¤œè¨¼RÂ² | RMSE |\n")
            f.write("|--------|---------|---------|-------|\n")
            
            if 'odds_baseline' in regression:
                model = regression['odds_baseline']
                f.write(f"| ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | {model.get('train_r2', 0):.4f} | {model.get('test_r2', 0):.4f} | {model.get('test_rmse', 0):.4f} |\n")
            
            if 'reqi_model' in regression:
                model = regression['reqi_model']
                f.write(f"| å¹³å‡REQI | {model.get('train_r2', 0):.4f} | {model.get('test_r2', 0):.4f} | {model.get('test_rmse', 0):.4f} |\n")
            
            # H2ä»®èª¬çµæœ
            if 'h2_verification' in regression:
                h2 = regression['h2_verification']
                f.write("\n### H2ä»®èª¬æ¤œè¨¼çµæœï¼ˆç°¡æ˜“ç‰ˆï¼‰\n\n")
                f.write(f"- **ä»®èª¬ã‚µãƒãƒ¼ãƒˆ**: {'âœ“ YES' if h2['hypothesis_supported'] else 'âœ— NO'}\n")
                f.write(f"- **æ€§èƒ½æ”¹å–„**: {h2['improvement']:+.4f}\n")
                f.write(f"- **çµ±è¨ˆçš„æ„å‘³**: {'âœ“ æœ‰æ„' if h2.get('statistically_meaningful', False) else 'âœ— é™å®šçš„'}\n")
                if 'warning' in h2:
                    f.write(f"- **æ³¨æ„**: {h2['warning']}\n")
                f.write("\n")
        
        # ã€ä¿®æ­£ã€‘æ™‚ç³»åˆ—åˆ†å‰²æŠ•è³‡æˆ¦ç•¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
        if combined_df is not None:
            logger.info("ğŸ“Š æ™‚ç³»åˆ—åˆ†å‰²æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã«è¿½åŠ ä¸­...")
            betting_section = generate_betting_performance_section(combined_df, train_end_year=2023, test_year=2024)
            f.write(betting_section)
        
        f.write("## çµè«–\n\n")
        f.write("å¹³å‡REQIï¼ˆç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼‰ã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®æ¯”è¼ƒåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n")
        f.write("ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å›ºå®šé‡ã¿æ³•ã‚’é©ç”¨ã—ãŸæ­£ç¢ºãªREQIè¨ˆç®—ã«ã‚ˆã‚Šã€çµ±è¨ˆçš„å¦¥å½“æ€§ã‚’ç¢ºä¿ã—ã¾ã—ãŸã€‚\n")
        
        # ã€ä¿®æ­£ã€‘æ™‚ç³»åˆ—åˆ†å‰²æŠ•è³‡æˆ¦ç•¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµè«–
        if combined_df is not None:
            f.write("\n### æ™‚ç³»åˆ—åˆ†å‰²ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ\n\n")
            f.write("æ­£ã—ã„æ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚\n")
            f.write("è¨“ç·´æœŸé–“ï¼ˆ~2023å¹´ï¼‰ã®çŸ¥è­˜ã®ã¿ã§2024å¹´ã‚’äºˆæ¸¬ã—ã€æƒ…å ±æ¼æ´©ã‚’å®Œå…¨ã«æ’é™¤ã—ã¦ã„ã¾ã™ã€‚\n")
            f.write("REQIãŒã‚ªãƒƒã‚ºã‚’è£œå®Œã™ã‚‹ç‰¹å¾´é‡ã¨ã—ã¦ã€çµ±è¨ˆçš„ãƒ»å®Ÿå‹™çš„ã«æœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n")
    
    logger.info(f"ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ: {report_path}")

@log_performance("è¨“ç·´æœŸé–“æ•£å¸ƒå›³ç”Ÿæˆ")
def generate_training_period_scatter_plots(data_dir: str, output_dir: str, encoding: str = 'utf-8') -> bool:
    """
    è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰å…¨ä½“ã§ã®å€‹åˆ¥è¦ç´ æ•£å¸ƒå›³ã‚’ç”Ÿæˆã™ã‚‹
    
    Args:
        data_dir (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        encoding (str): ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        
    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆã¯True
    """
    logger.info("ğŸ“Š è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã®æ•£å¸ƒå›³ç”Ÿæˆã‚’é–‹å§‹...")
    
    try:
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_path = Path(output_dir) / "training_period_visualizations"
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("ğŸ“– ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        loader = DataLoaderService()
        df = loader.load_csv_files(data_dir, encoding, use_cache=False)
        
        if df.empty:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—")
            return False
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ")
        
        # å¹´ã‚«ãƒ©ãƒ ã®ä½œæˆ
        if 'å¹´' not in df.columns and 'å¹´æœˆæ—¥' in df.columns:
            df['å¹´'] = pd.to_numeric(df['å¹´æœˆæ—¥'].astype(str).str[:4], errors='coerce')
        
        # è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿
        train_df = df[(df['å¹´'] >= 2010) & (df['å¹´'] <= 2020)].copy()
        logger.info(f"ğŸ“… è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿: {len(train_df):,}è¡Œ (2010-2020å¹´)")
        
        # ç‰¹å¾´é‡è¨ˆç®—
        logger.info("ğŸ§® ç‰¹å¾´é‡è¨ˆç®—ä¸­...")
        calculator = FeatureCalculator()
        train_df = calculator.calculate_reqi(train_df)
        
        # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—
        logger.info("ğŸ“Š é¦¬ã”ã¨ã®çµ±è¨ˆè¨ˆç®—ä¸­...")
        horse_stats = train_df.groupby('é¦¬å').agg({
            'grade_level': 'mean',
            'venue_level': 'mean',
            'distance_level': 'mean',
            'race_level': 'mean',
            'ç€é †': ['count', lambda x: (x <= 3).mean()]
        }).reset_index()
        
        # ã‚«ãƒ©ãƒ åã®æ•´ç†
        horse_stats.columns = ['é¦¬å', 'grade_level', 'venue_level', 'distance_level', 
                               'race_level', 'race_count', 'place_rate']
        
        # æœ€ä½å‡ºèµ°å›æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿
        horse_stats = horse_stats[horse_stats['race_count'] >= 6]
        logger.info(f"ğŸ“Š å¯¾è±¡é¦¬æ•°: {len(horse_stats):,}é ­ï¼ˆ6èµ°ä»¥ä¸Šï¼‰")
        
        # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        setup_japanese_fonts(suppress_warnings=True)
        apply_plot_style()
        
        # æ•£å¸ƒå›³ã®è¨­å®š
        features_to_plot = [
            {
                'x_col': 'grade_level',
                'x_label': 'ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«',
                'title': 'ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–¢ä¿‚ï¼ˆ2010-2020å¹´è¨“ç·´æœŸé–“ï¼‰',
                'filename': 'grade_level_place_rate_scatter_training.png'
            },
            {
                'x_col': 'venue_level',
                'x_label': 'å ´æ‰€ãƒ¬ãƒ™ãƒ«',
                'title': 'å ´æ‰€ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–¢ä¿‚ï¼ˆ2010-2020å¹´è¨“ç·´æœŸé–“ï¼‰',
                'filename': 'venue_level_place_rate_scatter_training.png'
            },
            {
                'x_col': 'distance_level',
                'x_label': 'è·é›¢ãƒ¬ãƒ™ãƒ«',
                'title': 'è·é›¢ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–¢ä¿‚ï¼ˆ2010-2020å¹´è¨“ç·´æœŸé–“ï¼‰',
                'filename': 'distance_level_place_rate_scatter_training.png'
            },
            {
                'x_col': 'race_level',
                'x_label': 'REQIï¼ˆç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼‰',
                'title': 'REQIï¼ˆç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼‰ã¨è¤‡å‹ç‡ã®é–¢ä¿‚ï¼ˆ2010-2020å¹´è¨“ç·´æœŸé–“ï¼‰',
                'filename': 'race_level_place_rate_scatter_training.png'
            }
        ]
        
        # å„è¦ç´ ã®æ•£å¸ƒå›³ã‚’ç”Ÿæˆ
        for config in features_to_plot:
            _create_scatter_plot(horse_stats, config, output_path)
        
        logger.info(f"âœ… æ•£å¸ƒå›³ç”Ÿæˆå®Œäº†: {output_path}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´æœŸé–“æ•£å¸ƒå›³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
        return False


def _create_scatter_plot(horse_stats: pd.DataFrame, config: dict, output_dir: Path):
    """æ•£å¸ƒå›³ã‚’ä½œæˆ"""
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
    except ImportError:
        logger.error("âŒ matplotlibã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—")
        return
    
    # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’å†é©ç”¨ï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
    selected_font = setup_japanese_fonts(suppress_warnings=True)
    
    # ãƒ•ã‚©ãƒ³ãƒˆåã‚’ç¢ºå®Ÿã«å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
    if selected_font is None:
        import platform
        if platform.system() == 'Windows':
            selected_font = 'Yu Gothic'
        else:
            selected_font = 'DejaVu Sans'
    
    x_col = config['x_col']
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    valid_data = horse_stats.dropna(subset=[x_col, 'place_rate'])
    x_data = valid_data[x_col]
    y_data = valid_data['place_rate']
    
    if len(x_data) < 10:
        logger.warning(f"âš ï¸ {config['title']}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return
    
    # çµ±è¨ˆè¨ˆç®—
    correlation, p_value = pearsonr(x_data, y_data)
    
    # å›å¸°åˆ†æ
    model = LinearRegression()
    X = x_data.values.reshape(-1, 1)
    y = y_data.values
    model.fit(X, y)
    r2 = model.score(X, y)
    
    logger.info(f"   ğŸ“ˆ {config['x_label']}: r={correlation:.3f}, RÂ²={r2:.4f}, p={p_value:.3e}")
    
    # æ•£å¸ƒå›³ä½œæˆ
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # æ•£å¸ƒå›³
    ax.scatter(x_data, y_data, alpha=0.6, s=50, color='steelblue', 
               edgecolors='white', linewidth=0.5)
    
    # å›å¸°ç›´ç·š
    x_range = np.linspace(x_data.min(), x_data.max(), 100)
    y_range = model.predict(x_range.reshape(-1, 1))
    ax.plot(x_range, y_range, 'r-', linewidth=2, 
            label=f'å›å¸°ç›´ç·š (RÂ² = {r2:.4f})')
    
    # è£…é£¾ï¼ˆãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
    ax.set_title(f'{config["title"]}\nç›¸é–¢ä¿‚æ•°: r={correlation:.3f} (p={p_value:.3e})', 
                 fontsize=14, pad=20, fontfamily=selected_font)
    ax.set_xlabel(config['x_label'], fontsize=12, fontfamily=selected_font)
    ax.set_ylabel('è¤‡å‹ç‡', fontsize=12, fontfamily=selected_font)
    ax.set_ylim(-0.05, 1.05)
    ax.grid(True, alpha=0.3)
    
    # å‡¡ä¾‹ã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    legend = ax.legend(fontsize=10, prop={'family': selected_font})
    
    # çµ±è¨ˆæƒ…å ±ãƒœãƒƒã‚¯ã‚¹
    stats_text = f'ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(x_data):,}é ­\n'
    stats_text += f'ç›¸é–¢ä¿‚æ•°: r={correlation:.3f}\n'
    stats_text += f'æ±ºå®šä¿‚æ•°: RÂ²={r2:.4f}\n'
    stats_text += f'på€¤: {p_value:.3e}\n'
    stats_text += f'æœ‰æ„æ€§: {"æœ‰æ„" if p_value < 0.05 else "éæœ‰æ„"}'
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    fig.text(0.78, 0.98, stats_text,
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),
            verticalalignment='top', fontsize=10,
            transform=fig.transFigure, fontfamily=selected_font)
    
    # è»¸ã®ãƒ©ãƒ™ãƒ«ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    for label in ax.get_xticklabels():
        label.set_fontfamily(selected_font)
    for label in ax.get_yticklabels():
        label.set_fontfamily(selected_font)
    
    plt.subplots_adjust(right=0.75)
    
    # ä¿å­˜
    output_path = output_dir / config['filename']
    plt.savefig(output_path, dpi=300, bbox_inches='tight', 
               facecolor='white', edgecolor='none')
    plt.close()
    
    logger.info(f"   ğŸ’¾ ä¿å­˜: {output_path}")

def _create_argument_parser() -> argparse.ArgumentParser:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ‘ãƒ¼ã‚µã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    parser = argparse.ArgumentParser(
        description='ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆçµ±åˆç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æ
  python analyze_horse_REQI.py --odds-analysis export/dataset --output-dir results/reqi_odds

  # å¾“æ¥ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ
  python analyze_horse_REQI.py export/with_bias --output-dir results/race_level_analysis

  # å±¤åˆ¥åˆ†æã®ã¿å®Ÿè¡Œ
  python analyze_horse_REQI.py --stratified-only --output-dir results/stratified_analysis

  # ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼
  python analyze_REQI.py --validate-grade export/dataset

  # EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰ã®å®Ÿè¡Œ
  python analyze_REQI.py --eda export/dataset --output-dir results/eda

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä¸»è¦æ©Ÿèƒ½:
  1. ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æ
  2. H2ä»®èª¬ã€ŒREQIãŒã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€ã®æ¤œè¨¼
  3. ç›¸é–¢åˆ†æã¨å›å¸°åˆ†æã«ã‚ˆã‚‹çµ±è¨ˆçš„è©•ä¾¡
  4. å±¤åˆ¥åˆ†æï¼ˆå¹´é½¢å±¤ãƒ»çµŒé¨“æ•°ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰
  5. æœŸé–“åˆ¥åˆ†æï¼ˆ3å¹´é–“éš”ã§ã®æ™‚ç³»åˆ—åˆ†æï¼‰
  6. ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼ï¼ˆä¸€è‡´ç‡è¨ˆç®—ï¼‰
  7. EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰- åŸºæœ¬çµ±è¨ˆé‡ã€æ¬ æç‡ã€æ™‚ç³»åˆ—åˆ†å‰²å¾Œã®ç‰¹æ€§ç¢ºèª
        """
    )
    parser.add_argument('input_path', nargs='?', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ (ä¾‹: export/with_bias)')
    parser.add_argument('--output-dir', default='results/race_level_analysis', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹')
    parser.add_argument('--min-races', type=int, default=6, help='åˆ†æå¯¾è±¡ã¨ã™ã‚‹æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°')
    parser.add_argument('--encoding', default='utf-8', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°')
    parser.add_argument('--start-date', help='åˆ†æé–‹å§‹æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰')
    parser.add_argument('--end-date', help='åˆ†æçµ‚äº†æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰')

    # æ–°æ©Ÿèƒ½ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--odds-analysis', action='store_true', help='ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ')
    parser.add_argument('--sample-size', type=int, default=None, help='ã‚ªãƒƒã‚ºåˆ†æã§ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰')

    # å¾“æ¥ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆç¶™ç¶šï¼‰
    parser.add_argument('--three-year-periods', action='store_true',
                        help='3å¹´é–“éš”ã§ã®æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨æœŸé–“åˆ†æï¼‰')
    parser.add_argument('--enable-stratified-analysis', action='store_true', default=True,
                        help='å±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œï¼ˆå¹´é½¢å±¤åˆ¥ã€çµŒé¨“æ•°åˆ¥ã€è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹')
    parser.add_argument('--disable-stratified-analysis', action='store_true',
                        help='å±¤åˆ¥åˆ†æã‚’ç„¡åŠ¹åŒ–ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ç”¨ï¼‰')
    parser.add_argument('--stratified-only', action='store_true',
                        help='å±¤åˆ¥åˆ†æã®ã¿ã‚’å®Ÿè¡Œï¼ˆexport/datasetã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ï¼‰')
    parser.add_argument('--validate-grade', action='store_true',
                        help='ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼ã‚’å®Ÿè¡Œï¼ˆä¸€è‡´ç‡ã‚’è¨ˆç®—ï¼‰')
    parser.add_argument('--eda', action='store_true',
                        help='EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰ã‚’å®Ÿè¡Œï¼ˆåŸºæœ¬çµ±è¨ˆé‡ã€æ¬ æç‡ã€æ™‚ç³»åˆ—åˆ†å‰²å¾Œã®ç‰¹æ€§ç¢ºèªï¼‰')
    parser.add_argument('--generate-training-scatter', action='store_true',
                        help='è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã®æ•£å¸ƒå›³ã‚’ç”Ÿæˆï¼ˆè«–æ–‡4.1.2ç¯€ç”¨ï¼‰')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                        default='INFO', help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š')
    parser.add_argument('--log-file', help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰')
    return parser


def _prepare_logging(args: argparse.Namespace) -> str:
    """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–ã—ã€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    log_file = args.log_file
    if log_file is None:
        out_dir = Path(getattr(args, 'output_dir', 'results'))
        log_dir = out_dir / 'logs'
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = str(log_dir / f'analyze_horse_REQI_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')

    setup_logging(log_level=args.log_level, log_file=log_file)
    return log_file


def _resolve_stratified_flag(args: argparse.Namespace) -> bool:
    """å±¤åˆ¥åˆ†æã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã‹åˆ¤å®šã™ã‚‹ã€‚"""
    return args.enable_stratified_analysis and not args.disable_stratified_analysis


def _resolve_dataset_dir(args: argparse.Namespace) -> str:
    """å±¤åˆ¥åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®šã™ã‚‹ã€‚"""
    return args.input_path or 'export/dataset'


def _create_unified_analyzer_if_needed(args: argparse.Namespace, enable_stratified: bool):
    """CLIã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«å¿œã˜ãŸçµ±ä¸€åˆ†æå™¨ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    if args.odds_analysis:
        return create_unified_analyzer('odds', args.min_races, enable_stratified)
    if args.three_year_periods:
        return create_unified_analyzer('period', args.min_races, enable_stratified)
    return None


def _load_and_preprocess_data(args: argparse.Namespace, analyzer, dataset_dir: str) -> pd.DataFrame:
    """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã¨åŸºæœ¬å‰å‡¦ç†ã‚’é©ç”¨ã™ã‚‹ã€‚"""
    target_path = args.input_path or dataset_dir

    if analyzer is not None:
        df = analyzer.load_data_unified(target_path, args.encoding)
        df = filter_by_date_range(df, getattr(args, 'start_date', None), getattr(args, 'end_date', None))
        df = analyzer.preprocess_data_unified(df)
    else:
        if target_path is None:
            raise ValueError("å…¥åŠ›ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        df = load_all_data_once(target_path, args.encoding)
        df = filter_by_date_range(df, getattr(args, 'start_date', None), getattr(args, 'end_date', None))
        if 'ç€é †' in df.columns:
            df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')

    log_dataframe_info(df, "å…¥åŠ›ãƒ‡ãƒ¼ã‚¿")
    logger.info(f"ğŸ“Š èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶")
    return df


def _run_odds_analysis(args: argparse.Namespace, output_dir: Path, dataset_dir: str) -> int:
    """ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã¨ä»˜éšãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
    logger.info("ğŸ¯ ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
    try:
        comp_results = perform_comprehensive_odds_analysis(
            data_dir=args.input_path or dataset_dir,
            output_dir=str(output_dir),
            sample_size=args.sample_size,
            min_races=args.min_races,
            start_date=args.start_date,
            end_date=args.end_date
        )
        logger.info("âœ… åŒ…æ‹¬ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        logger.info(
            "ğŸ“Š åˆ†æå¯¾è±¡: %sãƒ¬ã‚³ãƒ¼ãƒ‰, %sé ­",
            f"{comp_results.get('data_summary', {}).get('total_records', 0):,}",
            f"{comp_results.get('data_summary', {}).get('horse_count', 0):,}",
        )
        logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")

        if 'regression' in comp_results and 'h2_verification' in comp_results['regression']:
            h2 = comp_results['regression']['h2_verification']
            result_text = "ã‚µãƒãƒ¼ãƒˆ" if h2.get('hypothesis_supported', False) or h2.get('h2_hypothesis_supported', False) else "éã‚µãƒãƒ¼ãƒˆ"
            logger.info(f"ğŸ¯ H2ä»®èª¬ã€ŒREQIãŒã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€: {result_text}")
            improvement = h2.get('r2_improvement', h2.get('improvement', 0))
            logger.info(f"   æ€§èƒ½æ”¹å–„: {improvement:+.4f}")

        logger.info("â„¹ï¸ åŒ…æ‹¬ç‰ˆãŒå®Œäº†ã—ãŸãŸã‚ã€ç°¡æ˜“ç‰ˆã®å¼·åˆ¶ç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

        try:
            logger.info("ğŸ“‹ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
            stratified_dataset = create_stratified_dataset_from_export(dataset_dir, start_date=args.start_date, end_date=args.end_date)
            stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
            _ = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
            logger.info("âœ… çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        except Exception as stratified_error:
            logger.error(f"âŒ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(stratified_error)}")
            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)

        return 0
    except Exception as e:
        logger.error(f"âŒ åŒ…æ‹¬ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
        return 0


def _run_stratified_only(args: argparse.Namespace, dataset_dir: str, output_dir: Path) -> int:
    """å±¤åˆ¥åˆ†æã®ã¿ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
    logger.info("ğŸ“Š å±¤åˆ¥åˆ†æã®ã¿ã‚’å®Ÿè¡Œã—ã¾ã™...")
    try:
        stratified_dataset = create_stratified_dataset_from_export(dataset_dir, start_date=args.start_date, end_date=args.end_date)
        stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
        _ = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
        logger.info("âœ… å±¤åˆ¥åˆ†æã®ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(stratified_dataset):,}é ­")
        logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
        return 0
    except Exception as e:
        logger.error(f"âŒ å±¤åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
        return 1


def _run_period_analysis(analyzer, df: pd.DataFrame, args: argparse.Namespace, dataset_dir: str, output_dir: Path) -> int:
    """3å¹´é–“éš”ã§ã®æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
    logger.info("ğŸ“Š 3å¹´é–“éš”ã§ã®æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
    try:
        if 'å¹´' not in df.columns or not df['å¹´'].notna().any():
            logger.warning("âš ï¸ å¹´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 1

        min_year = int(df['å¹´'].min())
        max_year = int(df['å¹´'].max())
        logger.info(f"ğŸ“Š å¹´ãƒ‡ãƒ¼ã‚¿ç¯„å›²: {min_year}å¹´ - {max_year}å¹´")

        results = analyzer.analyze(df)

        if not results:
            logger.warning("âš ï¸ æœ‰åŠ¹ãªæœŸé–“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return 1

        logger.info(f"ğŸ“Š æœŸé–“åˆ¥åˆ†æå®Œäº†: {len(results)}æœŸé–“")

        try:
            logger.info("ğŸ“‹ æœŸé–“åˆ¥åˆ†æã®ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
            generate_period_summary_report(results, output_dir)
            logger.info("âœ… æœŸé–“åˆ¥åˆ†æç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        except Exception as summary_error:
            logger.error(f"âŒ ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(summary_error)}")

        try:
            logger.info("ğŸ“‹ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
            stratified_dataset = create_stratified_dataset_from_export(dataset_dir, start_date=args.start_date, end_date=args.end_date)
            stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
            _ = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
            logger.info("âœ… çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        except Exception as stratified_error:
            logger.error(f"âŒ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(stratified_error)}")
            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)

        logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
        logger.info(f"ğŸ“‹ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ: {output_dir}/ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ_æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ.md")
        logger.info(f"ğŸ“‹ å±¤åˆ¥ãƒ¬ãƒãƒ¼ãƒˆ: {output_dir}/stratified_analysis_integrated_report.md")
        return 0
    except Exception as e:
        logger.error(f"âŒ æœŸé–“åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
        return 1


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = _create_argument_parser()

    try:
        args = parser.parse_args()
        log_file = _prepare_logging(args)

        print("\n" + "=" * 80)
        print("ğŸ ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹: race_level_analysis_report.mdæº–æ‹ ")
        print("=" * 80)
        print("ğŸ“– å‚ç…§ãƒ¬ãƒãƒ¼ãƒˆ: race_level_analysis_report.md")
        print("ğŸ¯ REQIè¨ˆç®—æ–¹å¼: å‹•çš„é‡ã¿è¨ˆç®—æ³•ï¼ˆæ¯å›ç›¸é–¢åˆ†æã§ç®—å‡ºï¼‰")
        print("ğŸ“Š é‡ã¿ç®—å‡º: w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)")
        print("ğŸ”¬ çµ±è¨ˆçš„æ ¹æ‹ : å®Ÿæ¸¬ç›¸é–¢ä¿‚æ•°ã®2ä¹—å€¤æ­£è¦åŒ–")
        print("â³ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ä¸­...")
        print("=" * 80 + "\n")

        enable_stratified = _resolve_stratified_flag(args)
        dataset_dir = _resolve_dataset_dir(args)

        if enable_stratified:
            logger.info("ğŸ“Š å±¤åˆ¥åˆ†æ: æœ‰åŠ¹ï¼ˆå¹´é½¢å±¤åˆ¥ãƒ»çµŒé¨“æ•°åˆ¥ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰")
        else:
            logger.info("ğŸ“Š å±¤åˆ¥åˆ†æ: ç„¡åŠ¹ï¼ˆ--disable-stratified-analysisã§ç„¡åŠ¹åŒ–ï¼‰")

        logger.info("ğŸ‡ ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        logger.info(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
        logger.info(f"ğŸ–¥ï¸ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {args.log_level}")
        logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        log_system_resources()

        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        if not output_dir.exists() or not output_dir.is_dir():
            raise FileNotFoundError(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {output_dir}")

        if args.stratified_only:
            return _run_stratified_only(args, dataset_dir, output_dir)

        # ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼
        if args.validate_grade:
            logger.info("ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                grade_results = validate_grade_estimation(
                    data_dir=args.input_path or dataset_dir,
                    encoding=args.encoding
                )
                if 'error' not in grade_results:
                    logger.info(f"âœ… ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œæ¤œè¨¼å®Œäº†: ä¸€è‡´ç‡ {grade_results['accuracy_pct']}")
                    logger.info(f"ğŸ“Š æ¤œè¨¼å¯¾è±¡: {grade_results['total_records']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
                    
                    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                    result_path = output_dir / 'grade_estimation_validation.md'
                    with open(result_path, 'w', encoding='utf-8') as f:
                        f.write("# ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œã®å¦¥å½“æ€§æ¤œè¨¼çµæœ\n\n")
                        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                        f.write("## æ¦‚è¦\n\n")
                        f.write(f"- **æ¤œè¨¼å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: {grade_results['total_records']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰\n")
                        f.write(f"- **ä¸€è‡´æ•°**: {grade_results['matches']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰\n")
                        f.write(f"- **ä¸€è‡´ç‡ï¼ˆAccuracyï¼‰**: {grade_results['accuracy_pct']}\n\n")
                        f.write("## ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ä¸€è‡´ç‡\n\n")
                        f.write("| ã‚°ãƒ¬ãƒ¼ãƒ‰ | ä¸€è‡´ç‡ | ä¸€è‡´æ•° | ç·æ•° |\n")
                        f.write("|---------|--------|--------|------|\n")
                        for grade_name, stats in grade_results['grade_accuracy'].items():
                            f.write(f"| {grade_name} | {stats['accuracy']*100:.1f}% | {stats['matches']:,} | {stats['total']:,} |\n")
                        f.write("\n## è§£é‡ˆ\n\n")
                        f.write("ã“ã®æ¤œè¨¼ã¯ã€å…ƒã®ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹ãƒ¬ãƒ¼ã‚¹ã§è£œå®Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã—ã€\n")
                        f.write("æ¨å®šã•ã‚ŒãŸã‚°ãƒ¬ãƒ¼ãƒ‰ã¨å…ƒã®ã‚°ãƒ¬ãƒ¼ãƒ‰ã®ä¸€è‡´ç‡ã‚’è¨ˆç®—ã—ãŸã‚‚ã®ã§ã™ã€‚\n")
                    logger.info(f"ğŸ“‹ çµæœä¿å­˜å…ˆ: {result_path}")
                else:
                    logger.warning(f"âš ï¸ ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œæ¤œè¨¼ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ: {grade_results['error']}")
                return 0
            except Exception as e:
                logger.error(f"âŒ ã‚°ãƒ¬ãƒ¼ãƒ‰è£œå®Œæ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1

        # EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰
        if args.eda:
            logger.info("ğŸ“Š EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                eda_results = perform_eda_analysis(
                    data_dir=args.input_path or dataset_dir,
                    output_dir=str(output_dir),
                    encoding=args.encoding
                )
                if 'error' not in eda_results:
                    logger.info("âœ… EDAåˆ†æå®Œäº†")
                    logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡: {eda_results['data_overview']['total_records']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
                    logger.info(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å…ˆ: {output_dir / 'eda_report.md'}")
                else:
                    logger.warning(f"âš ï¸ EDAåˆ†æã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ: {eda_results['error']}")
                return 0
            except Exception as e:
                logger.error(f"âŒ EDAåˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1

        if not args.odds_analysis:
            args = validate_args(args)

        analyzer = _create_unified_analyzer_if_needed(args, enable_stratified)
        if analyzer is None:
            logger.info("ğŸ“Š çµ±ä¸€åˆ†æå™¨ã‚’ä½¿ç”¨ã›ãšã«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’å®Ÿæ–½ã—ã¾ã™...")
        else:
            logger.info(f"ğŸ“Š çµ±ä¸€åˆ†æå™¨: {analyzer.__class__.__name__}")

        df = _load_and_preprocess_data(args, analyzer, dataset_dir)

        try:
            weights_initialized = initialize_global_weights(args)
            if weights_initialized:
                logger.info("âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–å®Œäº†")
            else:
                logger.warning("âš ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ã«å¤±æ•—ã€å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å€‹åˆ¥è¨ˆç®—")
        except Exception as weight_error:
            logger.error(f"âŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(weight_error)}")
            logger.warning("âš ï¸ å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å€‹åˆ¥é‡ã¿è¨ˆç®—ã‚’å®Ÿè¡Œã—ã¾ã™")

        logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªæ¸ˆã¿: {output_dir.absolute()}")
        logger.info(f"ğŸ“ å…¥åŠ›ãƒ‘ã‚¹: {args.input_path}")
        logger.info(f"ğŸ“Š å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
        logger.info(f"ğŸ¯ æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°: {args.min_races}")
        if args.start_date:
            logger.info(f"ğŸ“… åˆ†æé–‹å§‹æ—¥: {args.start_date}")
        if args.end_date:
            logger.info(f"ğŸ“… åˆ†æçµ‚äº†æ—¥: {args.end_date}")

        if args.odds_analysis:
            return _run_odds_analysis(args, output_dir, dataset_dir)

        # è¨“ç·´æœŸé–“æ•£å¸ƒå›³ç”Ÿæˆï¼ˆå°‚ç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if args.generate_training_scatter:
            logger.info("ğŸ“Š è¨“ç·´æœŸé–“æ•£å¸ƒå›³ç”Ÿæˆã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                success = generate_training_period_scatter_plots(
                    data_dir=args.input_path or dataset_dir,
                    output_dir=str(output_dir),
                    encoding=args.encoding
                )
                if success:
                    logger.info("âœ… è¨“ç·´æœŸé–“æ•£å¸ƒå›³ç”Ÿæˆå®Œäº†")
                    logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir / 'training_period_visualizations'}")
                else:
                    logger.warning("âš ï¸ è¨“ç·´æœŸé–“æ•£å¸ƒå›³ç”Ÿæˆã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
                return 0 if success else 1
            except Exception as e:
                logger.error(f"âŒ è¨“ç·´æœŸé–“æ•£å¸ƒå›³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1

        if args.three_year_periods:
            if analyzer is None:
                logger.error("âŒ æœŸé–“åˆ¥åˆ†æã«ã¯çµ±ä¸€åˆ†æå™¨ãŒå¿…è¦ã§ã™")
                return 1
            result = _run_period_analysis(analyzer, df, args, dataset_dir, output_dir)
            
            # æœŸé–“åˆ¥åˆ†æå®Ÿè¡Œæ™‚ã«è¨“ç·´æœŸé–“æ•£å¸ƒå›³ã‚‚è‡ªå‹•ç”Ÿæˆ
            logger.info("ğŸ“Š æœŸé–“åˆ¥åˆ†æå®Œäº†å¾Œã€è¨“ç·´æœŸé–“æ•£å¸ƒå›³ã‚’è‡ªå‹•ç”Ÿæˆä¸­...")
            try:
                generate_training_period_scatter_plots(
                    data_dir=args.input_path or dataset_dir,
                    output_dir=str(output_dir),
                    encoding=args.encoding
                )
                logger.info("âœ… è¨“ç·´æœŸé–“æ•£å¸ƒå›³ã®è‡ªå‹•ç”Ÿæˆå®Œäº†")
            except Exception as scatter_error:
                logger.warning(f"âš ï¸ è¨“ç·´æœŸé–“æ•£å¸ƒå›³ã®è‡ªå‹•ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {str(scatter_error)}")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚æœŸé–“åˆ¥åˆ†æã®çµæœã¯è¿”ã™
            
            return result

        return 0

    except FileNotFoundError as e:
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        logger.error("   â€¢ å…¥åŠ›ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯è‹±æ•°å­—ã«å¤‰æ›´ã—ã¦ãã ã•ã„")
        logger.error("   â€¢ 'export/with_bias' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except ValueError as e:
        error_msg = str(e)
        logger.error(f"âŒ å…¥åŠ›å€¤ã‚¨ãƒ©ãƒ¼: {error_msg}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        
        if "æ¡ä»¶ã‚’æº€ãŸã™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" in error_msg:
            logger.error("   â€¢ --min-races ã®å€¤ã‚’å°ã•ãã—ã¦ã¿ã¦ãã ã•ã„ï¼ˆä¾‹: --min-races 3ï¼‰")
            logger.error("   â€¢ æœŸé–“æŒ‡å®šãŒç‹­ã™ãã‚‹å ´åˆã¯ç¯„å›²ã‚’åºƒã’ã¦ãã ã•ã„")
            logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹æœŸé–“ã‹ã©ã†ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        elif "æ—¥ä»˜å½¢å¼" in error_msg:
            logger.error("   â€¢ æ—¥ä»˜ã¯YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 20220101ï¼‰")
            logger.error("   â€¢ --start-date ã¨ --end-date ã®ä¸¡æ–¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        else:
            logger.error("   â€¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ --help ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’ç¢ºèªã§ãã¾ã™")
        
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except IndexError as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒçŸ­ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        logger.error("   â€¢ æ™‚ç³»åˆ—åˆ†å‰²ã«å¿…è¦ãªæœ€ä½3å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        logger.error("   â€¢ æœŸé–“æŒ‡å®šã‚’åºƒã’ã¦å†å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except KeyboardInterrupt:
        logger.warning("â¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        logger.info("ğŸ’¡ å‡¦ç†æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ã«ã¯:")
        logger.info("   â€¢ --min-races ã‚’å¤§ããã—ã¦ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ¸›ã‚‰ã™")
        logger.info("   â€¢ æœŸé–“ã‚’çŸ­ãã—ã¦å‡¦ç†ç¯„å›²ã‚’çµã‚‹")
        logger.info("   â€¢ --disable-stratified-analysis ã§å±¤åˆ¥åˆ†æã‚’ç„¡åŠ¹åŒ–")
        if log_file:
            logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        
        if "encoding" in error_msg.lower() or "unicode" in error_msg.lower():
            logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ã¾ãŸã¯Shift-JISã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        elif "memory" in error_msg.lower():
            logger.error("   â€¢ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ --min-races ã‚’å¤§ããã—ã¦ãƒ‡ãƒ¼ã‚¿é‡ã‚’æ¸›ã‚‰ã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ä¸è¦ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¦ãã ã•ã„")
        elif "permission" in error_msg.lower():
            logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®å•é¡ŒãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ç®¡ç†è€…æ¨©é™ã§å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„")
        else:
            logger.error("   â€¢ --log-level DEBUG ã§è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ Pythonã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        logger.error("ğŸ” è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
        logger.error(f"   ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥: {type(e).__name__}")
        logger.error(f"   ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_msg}")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        logger.error("è©³ç´°ãªã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:", exc_info=True)
        return 1

if __name__ == '__main__':
    sys.exit(main())