#!/usr/bin/env python
"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹åˆ†æã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ï¼ˆç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°REQIã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒå¯¾å¿œç‰ˆï¼‰
é¦¬ã”ã¨ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã®åˆ†æã¨ã‚ªãƒƒã‚ºæƒ…å ±ã¨ã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import argparse
from pathlib import Path
from datetime import datetime
import logging
import sys
import pandas as pd
import numpy as np
from typing import Dict, Any, List
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
# import matplotlib.pyplot as plt  # å¯è¦–åŒ–ã«å¿…è¦ï¼ˆé–¢æ•°å†…ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
# import seaborn as sns              # å¯è¦–åŒ–ã«å¿…è¦ï¼ˆé–¢æ•°å†…ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
import warnings
import time
import psutil
import os
from functools import wraps
warnings.filterwarnings('ignore')

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚‚ä¿æŒ
try:
    from horse_racing.base.analyzer import AnalysisConfig
    from horse_racing.analyzers.race_level_analyzer import REQIAnalyzer
    from horse_racing.core.weight_manager import WeightManager, get_global_weights
    from horse_racing.analyzers.odds_comparison_analyzer import OddsComparisonAnalyzer
except ImportError as e:
    logging.warning(f"ä¸€éƒ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    logging.info("åŸºæœ¬çš„ãªåˆ†ææ©Ÿèƒ½ã®ã¿åˆ©ç”¨ã§ãã¾ã™")

def setup_logging(log_level='INFO', log_file=None):
    """ãƒ­ã‚°è¨­å®šï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å¯¾å¿œï¼‰"""
    if log_file:
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
                logging.FileHandler(log_file, encoding='utf-8')  # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            ],
            force=True  # æ—¢å­˜ã®è¨­å®šã‚’ä¸Šæ›¸ã
        )
    else:
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            force=True
        )

logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼šè¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
_global_data = None
_global_feature_levels = None
_global_raw_data = None  # ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆCSVèª­ã¿è¾¼ã¿çµæœï¼‰

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
def log_performance(func_name=None):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # é–¢æ•°åã‚’è‡ªå‹•å–å¾—ã¾ãŸã¯æŒ‡å®šã•ã‚ŒãŸåå‰ã‚’ä½¿ç”¨
            name = func_name or func.__name__
            
            # é–‹å§‹æ™‚ã®ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—
            process = psutil.Process(os.getpid())
            start_time = time.time()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            start_cpu_percent = process.cpu_percent()
            
            logger.info(f"ğŸš€ [{name}] é–‹å§‹ - é–‹å§‹æ™‚ãƒ¡ãƒ¢ãƒª: {start_memory:.1f}MB, CPU: {start_cpu_percent:.1f}%")
            
            try:
                # é–¢æ•°å®Ÿè¡Œ
                result = func(*args, **kwargs)
                
                # çµ‚äº†æ™‚ã®ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—
                end_time = time.time()
                end_memory = process.memory_info().rss / 1024 / 1024  # MB
                end_cpu_percent = process.cpu_percent()
                
                # å®Ÿè¡Œæ™‚é–“ã¨ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’è¨ˆç®—
                execution_time = end_time - start_time
                memory_diff = end_memory - start_memory
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–ã¨è­¦å‘Š
                if memory_diff > 200:  # 200MBä»¥ä¸Šã®å¢—åŠ 
                    logger.warning(f"âš ï¸ [{name}] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ200MBå¢—åŠ ã—ã¾ã—ãŸ: {memory_diff:+.1f}MB")
                elif memory_diff > 500:  # 500MBä»¥ä¸Šã®å¢—åŠ 
                    logger.warning(f"âš ï¸ [{name}] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ500MBå¢—åŠ ã—ã¾ã—ãŸ: {memory_diff:+.1f}MB")
                
                # ãƒ­ã‚°å‡ºåŠ›
                logger.info(f"âœ… [{name}] å®Œäº† - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
                logger.info(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {end_memory:.1f}MB (å·®åˆ†: {memory_diff:+.1f}MB)")
                logger.info(f"   ğŸ–¥ï¸  CPUä½¿ç”¨ç‡: {end_cpu_percent:.1f}%")
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Š
                if execution_time > 60:
                    logger.warning(f"âš ï¸ [{name}] å®Ÿè¡Œæ™‚é–“ãŒ1åˆ†ã‚’è¶…ãˆã¾ã—ãŸ: {execution_time:.2f}ç§’")
                if memory_diff > 500:
                    logger.warning(f"âš ï¸ [{name}] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ500MBå¢—åŠ ã—ã¾ã—ãŸ: {memory_diff:.1f}MB")
                
                return result
                
            except Exception:
                end_time = time.time()
                execution_time = end_time - start_time
                logger.error(f"âŒ [{name}] ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
                raise
                
        return wrapper
    return decorator

def log_dataframe_info(df: pd.DataFrame, description: str):
    """DataFrameã®è©³ç´°æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
    memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
    logger.info(f"ğŸ“Š [{description}] ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±:")
    logger.info(f"   ğŸ“ å½¢çŠ¶: {df.shape[0]:,}è¡Œ Ã— {df.shape[1]}åˆ—")
    logger.info(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage:.1f}MB")
    logger.info(f"   ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å‹åˆ†å¸ƒ: {dict(df.dtypes.value_counts())}")
    
    # æ¬ æå€¤æƒ…å ±
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        logger.info(f"   âš ï¸ æ¬ æå€¤: {null_counts.sum():,}å€‹ ({null_counts.sum()/df.size*100:.1f}%)")
        try:
            # åˆ—åˆ¥ãƒˆãƒƒãƒ—Nã®æ¬ æå†…è¨³
            missing_counts_sorted = null_counts.sort_values(ascending=False)
            missing_pct_sorted = (missing_counts_sorted / len(df) * 100).round(1)
            top_n = 15
            top_missing = (
                pd.concat([
                    missing_counts_sorted.rename('count'),
                    missing_pct_sorted.rename('%')
                ], axis=1)
                .head(top_n)
            )
            if len(top_missing) > 0:
                logger.info("   ğŸ” æ¬ æãƒˆãƒƒãƒ—15(åˆ—):\n" + top_missing.to_string())
            
            # å¹´åˆ¥Ã—ä¸»è¦åˆ—ã®æ¬ æç‡
            key_cols = ['ã‚°ãƒ¬ãƒ¼ãƒ‰', '10æ™‚å˜å‹ã‚ªãƒƒã‚º', '10æ™‚è¤‡å‹ã‚ªãƒƒã‚º', 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹', 'é¨æ‰‹ã‚³ãƒ¼ãƒ‰']
            available_key_cols = [c for c in key_cols if c in df.columns]
            if 'å¹´' in df.columns and len(available_key_cols) > 0:
                year_missing = (
                    df.groupby('å¹´')[available_key_cols]
                      .apply(lambda x: x.isnull().mean().mul(100).round(1))
                )
                logger.info("   ğŸ” å¹´åˆ¥Ã—ä¸»è¦åˆ— æ¬ æç‡(%):\n" + year_missing.to_string())
        except Exception as e:
            logger.warning(f"   âš ï¸ æ¬ æè©³ç´°ãƒ­ã‚°ã®ç”Ÿæˆä¸­ã«ä¾‹å¤–: {str(e)}")
    
def log_processing_step(step_name: str, start_time: float, current_idx: int, total_count: int):
    """å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®é€²æ—ã‚’ãƒ­ã‚°å‡ºåŠ›"""
    elapsed = time.time() - start_time
    if current_idx > 0:
        avg_time_per_item = elapsed / current_idx
        remaining_items = total_count - current_idx
        eta = remaining_items * avg_time_per_item
        
        logger.info(f"â³ [{step_name}] é€²æ—: {current_idx:,}/{total_count:,} "
                   f"({current_idx/total_count*100:.1f}%) - "
                   f"çµŒéæ™‚é–“: {elapsed:.1f}ç§’, æ®‹ã‚Šäºˆæƒ³: {eta:.1f}ç§’")

def log_system_resources():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¾åœ¨çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    cpu_percent = process.cpu_percent()
    
    # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æƒ…å ±
    system_memory = psutil.virtual_memory()
    system_cpu = psutil.cpu_percent()
    
    logger.info("ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³:")
    logger.info(f"   ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª: {memory_info.rss/1024/1024:.1f}MB")
    logger.info(f"   ãƒ—ãƒ­ã‚»ã‚¹CPU: {cpu_percent:.1f}%")
    logger.info(f"   ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {system_memory.percent:.1f}% "
               f"({system_memory.used/1024/1024/1024:.1f}GB/{system_memory.total/1024/1024/1024:.1f}GB)")
    logger.info(f"   ã‚·ã‚¹ãƒ†ãƒ CPUä½¿ç”¨ç‡: {system_cpu:.1f}%")

def get_all_dataset_files(data_dir: str) -> List[Path]:
    """æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
    data_path = Path(data_dir)
    if not data_path.exists():
        return []
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
    csv_files = list(data_path.glob('SED*_formatted_dataset.csv'))
    return sorted(csv_files)

def load_all_data_once(input_path: str, encoding: str = 'utf-8') -> pd.DataFrame:
    """
    å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚€é–¢æ•°ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ï¼‰
    
    Args:
        input_path: å…¥åŠ›ãƒ‘ã‚¹
        encoding: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        
    Returns:
        çµ±åˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    global _global_raw_data
    
    if _global_raw_data is not None:
        logger.info("ğŸ’¾ ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        return _global_raw_data.copy()
    
    logger.info("ğŸ“– å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆå›èª­ã¿è¾¼ã¿ä¸­...")
    input_path_obj = Path(input_path)
    
    if input_path_obj.is_file():
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        df = pd.read_csv(input_path_obj, encoding=encoding)
        logger.info(f"ğŸ“Š å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {len(df):,}è¡Œ")
        _global_raw_data = df.copy()
        return df
    else:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ
        csv_files = list(input_path_obj.glob("*.csv"))
        if not csv_files:
            logger.error(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
            return pd.DataFrame()
        
        logger.info(f"ğŸ“Š å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆä¸­... ({len(csv_files)}ãƒ•ã‚¡ã‚¤ãƒ«)")
        all_dfs = []
        
        for i, csv_file in enumerate(csv_files):
            try:
                df_temp = pd.read_csv(csv_file, encoding=encoding)
                all_dfs.append(df_temp)
                
                # é€²æ—è¡¨ç¤ºï¼ˆ100ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ï¼‰
                if (i + 1) % 100 == 0:
                    logger.info(f"   èª­ã¿è¾¼ã¿é€²æ—: {i + 1}/{len(csv_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {csv_file.name} - {str(e)}")
                continue
        
        if all_dfs:
            logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ çµ±åˆä¸­...")
            combined_df = pd.concat(all_dfs, ignore_index=True)
            logger.info(f"âœ… çµ±åˆå®Œäº†: {len(combined_df):,}è¡Œã®ãƒ‡ãƒ¼ã‚¿")
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜
            _global_raw_data = combined_df.copy()
            logger.info("ğŸ’¾ ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ã—ã¾ã—ãŸ")
            logger.info(f"ğŸ” ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ç¢ºèª: _global_raw_data is not None = {_global_raw_data is not None}")
            return combined_df
        else:
            logger.error("âŒ æœ‰åŠ¹ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()

def initialize_global_weights(args) -> bool:
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°
    
    Args:
        args: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
        
    Returns:
        åˆæœŸåŒ–æˆåŠŸãƒ•ãƒ©ã‚°
    """
    global _global_data, _global_feature_levels
    
    try:
        logger.info("ğŸ¯ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–é–‹å§‹...")
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆå„åˆ†æã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ï¼‰
        if args.odds_analysis:
            # ã‚ªãƒƒã‚ºåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿
            data_path = Path(args.odds_analysis)
            if not data_path.exists():
                logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data_path}")
                return False
                
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            csv_files = list(data_path.glob("*_formatted_dataset.csv"))
            if not csv_files:
                logger.error(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
                return False
                
            # é‡ã¿è¨ˆç®—ç”¨ã«å…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆé‡è¤‡å‡¦ç†å›é¿ã®ãŸã‚ï¼‰
            sample_dfs = []
            files_to_read = len(csv_files)  # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            logger.info(f"ğŸ“Š é‡ã¿è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {files_to_read}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰")
            
            for csv_file in csv_files[:files_to_read]:
                try:
                    df = pd.read_csv(csv_file, encoding='utf-8')
                    sample_dfs.append(df)
                    logger.info(f"ğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†: {csv_file.name} ({len(df):,}è¡Œ)")
                except Exception as e:
                    logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {csv_file} - {str(e)}")
                    continue
            
            if sample_dfs:
                combined_df = pd.concat(sample_dfs, ignore_index=True)
                logger.info(f"ğŸ“Š é‡ã¿ç®—å‡ºç”¨ãƒ‡ãƒ¼ã‚¿: {len(combined_df):,}è¡Œï¼ˆ{len(sample_dfs)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ï¼ˆçµ±ä¸€åˆ†æå™¨ã§ã®é‡è¤‡å‡¦ç†å›é¿ï¼‰
                _global_raw_data = combined_df.copy()
                logger.info("ğŸ’¾ ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆé‡ã¿è¨ˆç®—æ™‚ï¼‰")
                
                # __main__ åŒæœŸã¯ä¸è¦ï¼ˆUnifiedAnalyzerçµŒç”±ã«çµ±ä¸€ï¼‰
                
                # ã€çµ±ä¸€ã€‘æœŸé–“åˆ¥ã¨åŒä¸€è·¯ç·šã§ç‰¹å¾´é‡ã‚’ç”Ÿæˆï¼ˆgrade/venue/distanceï¼‰
                logger.info("ğŸ”§ ç‰¹å¾´é‡å‰å‡¦ç†ã‚’æœŸé–“åˆ¥ã¨åŒä¸€è·¯ç·šã«çµ±ä¸€ã—ã¾ã™...")
                df_levels = calculate_accurate_feature_levels(combined_df)
                
                # REQIç‰¹å¾´é‡ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰ã‚‚ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                logger.info("âš–ï¸ REQIç‰¹å¾´é‡ï¼ˆæ™‚é–“çš„åˆ†é›¢ç‰ˆï¼‰ã‚’ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™...")
                df_levels_with_reqi = calculate_race_level_features_with_position_weights(df_levels)
                
                # ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªï¼ˆæœŸé–“åˆ¥ã¨åŒã˜3æœ¬ã‚’è¦æ±‚ï¼‰
                required_level_cols = ['grade_level', 'venue_level', 'distance_level']
                missing_cols = [col for col in required_level_cols if col not in df_levels.columns]
                if missing_cols:
                    logger.warning(f"âš ï¸ ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ç”Ÿæˆå¾Œã‚‚ä¸è¶³: {missing_cols}")
                    logger.warning("ğŸ“Š ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™...")
                    fallback_weights = {
                        'grade_weight': 0.65,
                        'venue_weight': 0.30,
                        'distance_weight': 0.05
                    }
                    WeightManager._global_weights = fallback_weights
                    WeightManager._initialized = True
                    logger.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿è¨­å®šå®Œäº†: {fallback_weights}")
                    # ãã‚Œã§ã‚‚ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯è¨­å®šã—ã¦ãŠã
                    _global_data = combined_df.copy()
                    _global_feature_levels = df_levels_with_reqi.copy()
                    return True
                else:
                    logger.info("âœ… ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ç”Ÿæˆå®Œäº†ï¼ˆæœŸé–“åˆ¥æº–æ‹ ï¼‰")
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ï¼ˆæœŸé–“åˆ¥ã¨åŒæ§˜ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                _global_data = combined_df.copy()
                _global_feature_levels = df_levels_with_reqi.copy()
                logger.info("ğŸ’¾ è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆæœŸé–“åˆ¥æº–æ‹ ãƒ«ãƒ¼ãƒˆï¼‰")
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’åˆæœŸåŒ–ï¼ˆæœŸé–“åˆ¥ã¨åŒã˜ã2010-2020å¹´ã§å­¦ç¿’ï¼‰
                training_df = df_levels
                if 'å¹´' in df_levels.columns:
                    train_mask = (df_levels['å¹´'] >= 2010) & (df_levels['å¹´'] <= 2020)
                    filtered = df_levels[train_mask]
                    if len(filtered) > 0:
                        logger.info(f"ğŸ“Š é‡ã¿ç®—å‡ºç”¨è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿: {len(filtered):,}è¡Œ (2010-2020å¹´)")
                        training_df = filtered
                    else:
                        logger.warning("âš ï¸ è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãšã€å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¾ã™")
                else:
                    logger.warning("âš ï¸ å¹´åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãšã€å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¾ã™")

                weights = WeightManager.initialize_from_training_data(training_df)
                logger.info(f"âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿è¨­å®šå®Œäº†: {weights}")
                # æœŸé–“åˆ¥ãƒ•ãƒ­ãƒ¼ã¨åŒæ§˜ã«ã€ç›´å¾Œã«å–å¾—ãƒ­ã‚°ã‚’å‡ºã—ã¦æ•´åˆã‚’å–ã‚‹
                logger.info("ğŸ” é‡ã¿å–å¾—ç¢ºèªï¼ˆæœŸé–“åˆ¥ã¨åŒä¸€ãƒ•ãƒ­ãƒ¼ï¼‰...")
                _ = WeightManager.get_weights()  # ã“ã“ã§ã€Œâœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’æ­£å¸¸ã«å–å¾—ã—ã¾ã—ãŸã€ã‚’å‡ºåŠ›
                # ä»¥é™ã®å‡¦ç†ã§å†è¨ˆç®—ã•ã‚Œãªã„ã‚ˆã†ã«æ˜ç¤º
                WeightManager.prevent_recalculation()
                return True
                
        elif args.stratified_only:
            # å±¤åˆ¥åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆexport/datasetï¼‰
            dataset_path = Path("export/dataset")
            if dataset_path.exists():
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                combined_df = load_all_data_once(str(dataset_path), 'utf-8')
                if combined_df.empty:
                    return False
                
                # å¹´ã®ç¯„å›²ã‚’ç¢ºèª
                if 'å¹´' in combined_df.columns:
                    year_range = f"{combined_df['å¹´'].min()}-{combined_df['å¹´'].max()}å¹´"
                    logger.info(f"ğŸ“… å…¨ãƒ‡ãƒ¼ã‚¿æœŸé–“: {year_range}")
                    
                    # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼šè¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    training_data = combined_df[(combined_df['å¹´'] >= 2010) & (combined_df['å¹´'] <= 2020)]
                    if len(training_data) > 0:
                        df = training_data
                        training_year_range = f"{training_data['å¹´'].min()}-{training_data['å¹´'].max()}å¹´"
                        logger.info(f"ğŸ“Š è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿: {len(training_data):,}è¡Œ ({training_year_range})")
                    else:
                        logger.warning("âš ï¸ è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        df = combined_df  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                else:
                    logger.warning("âš ï¸ å¹´åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
                    df = combined_df
                
                # ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«åˆ—ã‚’è¨ˆç®—ï¼ˆé‡ã¿è¨ˆç®—ã®ãŸã‚ï¼‰
                logger.info("ğŸ§® é‡ã¿è¨ˆç®—ç”¨ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«åˆ—ã‚’è¨ˆç®—ä¸­...")
                df = calculate_accurate_feature_levels(df)
                
                # ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
                required_level_cols = ['grade_level', 'venue_level', 'distance_level']
                missing_cols = [col for col in required_level_cols if col not in df.columns]
                
                if missing_cols:
                    logger.warning(f"âš ï¸ ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ç”Ÿæˆå¾Œã‚‚ä¸è¶³: {missing_cols}")
                    logger.warning("ğŸ“Š ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™...")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’è¨­å®š
                    fallback_weights = {
                        'grade_weight': 0.65,
                        'venue_weight': 0.30,
                        'distance_weight': 0.05
                    }
                    WeightManager._global_weights = fallback_weights
                    WeightManager._initialized = True
                    logger.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿è¨­å®šå®Œäº†: {fallback_weights}")
                    return True
                else:
                    logger.info("âœ… ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ç”Ÿæˆå®Œäº†")
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ï¼ˆé‡è¤‡å‡¦ç†å›é¿ã®ãŸã‚ï¼‰
                _global_data = combined_df.copy()
                _global_feature_levels = df.copy()
                logger.info("ğŸ’¾ è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ã—ã¾ã—ãŸ")
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’åˆæœŸåŒ–
                weights = WeightManager.initialize_from_training_data(df)
                logger.info(f"âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿è¨­å®šå®Œäº†: {weights}")
                return True
                    
        elif args.input_path:
            # å¾“æ¥ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            combined_df = load_all_data_once(args.input_path, args.encoding)
            if combined_df.empty:
                return False
            
            # å¹´ã®ç¯„å›²ã‚’ç¢ºèª
            if 'å¹´' in combined_df.columns:
                year_range = f"{combined_df['å¹´'].min()}-{combined_df['å¹´'].max()}å¹´"
                logger.info(f"ğŸ“… å…¨ãƒ‡ãƒ¼ã‚¿æœŸé–“: {year_range}")
                
                # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼šè¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                training_data = combined_df[(combined_df['å¹´'] >= 2010) & (combined_df['å¹´'] <= 2020)]
                if len(training_data) > 0:
                    df = training_data
                    training_year_range = f"{training_data['å¹´'].min()}-{training_data['å¹´'].max()}å¹´"
                    logger.info(f"ğŸ“Š é‡ã¿è¨ˆç®—ç”¨è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿: {len(training_data):,}è¡Œ ({training_year_range})")
                else:
                    logger.warning("âš ï¸ è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    df = combined_df  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                
                # å…¨ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜ï¼ˆæ™‚ç³»åˆ—åˆ†å‰²ç”¨ï¼‰
                logger.info(f"ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿æœŸé–“: {len(combined_df):,}è¡Œ ({combined_df['å¹´'].min()}-{combined_df['å¹´'].max()}å¹´)")
            else:
                logger.warning("âš ï¸ å¹´åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
                df = combined_df
            
            # ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«åˆ—ã‚’è¨ˆç®—ï¼ˆé‡ã¿è¨ˆç®—ã®ãŸã‚ï¼šè¨“ç·´æœŸé–“2010-2020å¹´ã®ã¿ï¼‰
            logger.info("ğŸ§® é‡ã¿è¨ˆç®—ç”¨ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«åˆ—ã‚’è¨ˆç®—ä¸­ï¼ˆè¨“ç·´æœŸé–“2010-2020å¹´ï¼‰...")
            df = calculate_accurate_feature_levels(df)
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ï¼ˆé‡è¤‡å‡¦ç†å›é¿ã®ãŸã‚ï¼‰
            _global_data = combined_df.copy()  # å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆæ™‚ç³»åˆ—åˆ†å‰²ç”¨ï¼‰
            
            # ã€é‡è¦ä¿®æ­£ã€‘å…¨ãƒ‡ãƒ¼ã‚¿ã§ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«åˆ—ã‚’è¨ˆç®—ï¼ˆæœŸé–“åˆ¥åˆ†æã§2022-2025å¹´ã‚‚å«ã‚ã‚‹ï¼‰
            logger.info("ğŸ§® å…¨ãƒ‡ãƒ¼ã‚¿ã§ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«åˆ—ã‚’è¨ˆç®—ä¸­ï¼ˆæœŸé–“åˆ¥åˆ†æç”¨ï¼‰...")
            df_all_features = calculate_accurate_feature_levels(combined_df)
            
            # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ã‚‚äº‹å‰è¨ˆç®—ã—ã¦ä¿å­˜ï¼ˆæœŸé–“åˆ¥åˆ†æã®é«˜é€ŸåŒ–ï¼‰
            logger.info("ğŸš€ ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ã‚’äº‹å‰è¨ˆç®—ä¸­...")
            _global_feature_levels = calculate_race_level_features_with_position_weights(df_all_features)
            
            logger.info("ğŸ’¾ è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜ã—ã¾ã—ãŸ")
            logger.info(f"ğŸ“Š ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿: {len(_global_data):,}è¡Œï¼ˆå…¨æœŸé–“ï¼‰")
            logger.info(f"ğŸ“Š é‡ã¿è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿: {len(df):,}è¡Œï¼ˆè¨“ç·´æœŸé–“2010-2020å¹´ï¼‰")
            logger.info(f"ğŸ“Š æœŸé–“åˆ¥åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿: {len(_global_feature_levels):,}è¡Œï¼ˆå…¨æœŸé–“ï¼‰")
            logger.info("ğŸš€ ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ã‚‚äº‹å‰è¨ˆç®—æ¸ˆã¿ï¼ˆæœŸé–“åˆ¥åˆ†æé«˜é€ŸåŒ–ï¼‰")
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’åˆæœŸåŒ–
            weights = WeightManager.initialize_from_training_data(df)
            logger.info(f"âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿è¨­å®šå®Œäº†: {weights}")
            return True
        
        logger.warning("âš ï¸ é‡ã¿åˆæœŸåŒ–ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return False
        
    except Exception as e:
        logger.error(f"âŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def _calculate_individual_weights(df: pd.DataFrame) -> Dict[str, float]:
    """
    å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹•çš„é‡ã¿ã‚’è¨ˆç®—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    verify_weight_calculation.py ã®æ¤œè¨¼æ¸ˆã¿ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
    Returns:
        é‡ã¿è¾æ›¸
    """
    try:
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info("ğŸ” verify_weight_calculation.pyæº–æ‹ ã®å€‹åˆ¥é‡ã¿è¨ˆç®—ã‚’é–‹å§‹...")
        
        # å¿…è¦ã‚«ãƒ©ãƒ ã®ç¢ºèª
        required_cols = ['é¦¬å', 'ç€é †', 'grade_level', 'venue_level', 'distance_level']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.error(f"âŒ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
            return _get_fallback_weights()
        
        # Phase 1: é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰
        logger.info("ğŸ“Š Phase 1: é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...")
        
        # è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆ
        if 'ç€é †' in df.columns:
            df_temp = df.copy()
            df_temp['is_placed'] = (pd.to_numeric(df_temp['ç€é †'], errors='coerce') <= 3).astype(int)
            logger.info("ğŸ“Š ç€é †åˆ—ã‹ã‚‰è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆï¼ˆç€é †<=3ï¼‰")
        elif 'è¤‡å‹' in df.columns:
            df_temp = df.copy()
            df_temp['is_placed'] = pd.to_numeric(df_temp['è¤‡å‹'], errors='coerce').fillna(0)
            logger.info("ğŸ“Š è¤‡å‹åˆ—ã‹ã‚‰è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆ")
        else:
            logger.error("âŒ è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆã§ãã¾ã›ã‚“")
            return _get_fallback_weights()
        
        # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆæœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šï¼‰
        horse_stats = df_temp.groupby('é¦¬å').agg({
            'is_placed': 'mean',  # è¤‡å‹ç‡
            'grade_level': 'count'  # å‡ºèµ°å›æ•°
        }).reset_index()
        
        # åˆ—åã‚’æ¨™æº–åŒ–
        horse_stats.columns = ['é¦¬å', 'place_rate', 'race_count']
        
        # æœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜æº–æ‹ ï¼‰
        horse_stats = horse_stats[horse_stats['race_count'] >= 6].copy()
        logger.info(f"ğŸ“Š æœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šã§ãƒ•ã‚£ãƒ«ã‚¿: {len(horse_stats):,}é ­")
        
        if len(horse_stats) < 100:
            logger.error(f"âŒ ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³: {len(horse_stats)}é ­ï¼ˆæœ€ä½100é ­å¿…è¦ï¼‰")
            return _get_fallback_weights()
        
        # ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã®å¹³å‡ã‚’è¨ˆç®—
        feature_cols = ['grade_level', 'venue_level', 'distance_level']
        for col in feature_cols:
            avg_feature = df.groupby('é¦¬å')[col].mean().reset_index()
            avg_feature.columns = ['é¦¬å', f'avg_{col}']
            horse_stats = horse_stats.merge(avg_feature, on='é¦¬å', how='left')
        
        logger.info(f"ğŸ“Š é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(horse_stats):,}é ­")
        
        # Phase 2: ç›¸é–¢è¨ˆç®—ï¼ˆé¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
        logger.info("ğŸ“ˆ Phase 2: é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã§ç›¸é–¢ã‚’è¨ˆç®—ä¸­...")
        
        # å¿…è¦ãªåˆ—ã®ç¢ºèª
        required_corr_cols = ['place_rate', 'avg_grade_level', 'avg_venue_level', 'avg_distance_level']
        missing_corr_cols = [col for col in required_corr_cols if col not in horse_stats.columns]
        
        if missing_corr_cols:
            logger.error(f"âŒ å¿…è¦ãªç›¸é–¢åˆ—ãŒä¸è¶³: {missing_corr_cols}")
            logger.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(horse_stats.columns)}")
            return _get_fallback_weights()
        
        # æ¬ æå€¤ã‚’é™¤å»
        clean_data = horse_stats[required_corr_cols].dropna()
        logger.info(f"ğŸ“Š ç›¸é–¢è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿: {len(clean_data):,}é ­")
        
        if len(clean_data) < 100:
            logger.error(f"âŒ ç›¸é–¢è¨ˆç®—ç”¨ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³: {len(clean_data)}é ­ï¼ˆæœ€ä½100é ­å¿…è¦ï¼‰")
            return _get_fallback_weights()
        
        # ç›¸é–¢è¨ˆç®—
        from scipy.stats import pearsonr
        correlations = {}
        target = clean_data['place_rate']
        
        # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®ç›¸é–¢è¨ˆç®—
        feature_mapping = {
            'avg_grade_level': 'grade',
            'avg_venue_level': 'venue', 
            'avg_distance_level': 'distance'
        }
        
        for feature_col, feature_name in feature_mapping.items():
            if feature_col in clean_data.columns:
                corr, p_value = pearsonr(clean_data[feature_col], target)
                correlations[feature_name] = {
                    'correlation': corr,
                    'p_value': p_value,
                    'squared': corr ** 2
                }
                logger.info(f"   ğŸ“ˆ {feature_name}_level: r = {corr:.3f}, rÂ² = {corr**2:.3f}, p = {p_value:.3f}")
        
        # Phase 3: é‡ã¿è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰
        logger.info("âš–ï¸ Phase 3: é‡ã¿ã‚’è¨ˆç®—ä¸­...")
        logger.info("ğŸ“‹ è¨ˆç®—å¼: w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)")
        
        # ç›¸é–¢ã®äºŒä¹—ã‚’è¨ˆç®—
        squared_correlations = {}
        total_squared = 0
        
        for feature, stats in correlations.items():
            squared = stats['squared']
            squared_correlations[feature] = squared
            total_squared += squared
            logger.info(f"   ğŸ“Š {feature}: rÂ² = {squared:.3f}")
        
        logger.info(f"ğŸ“Š ç·å¯„ä¸åº¦: {total_squared:.3f}")
        
        if total_squared == 0:
            logger.warning("âš ï¸ ç·å¯„ä¸åº¦ãŒ0ã§ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return _get_fallback_weights()
        
        # é‡ã¿ã‚’æ­£è¦åŒ–
        weights = {}
        for feature, squared in squared_correlations.items():
            weight = squared / total_squared
            weights[feature] = weight
            logger.info(f"   âš–ï¸ {feature}: w = {weight:.3f} ({weight*100:.1f}%)")
        
        # ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã§å¤‰æ›
        result = {
            'grade_weight': weights.get('grade', 0.636),
            'venue_weight': weights.get('venue', 0.323),
            'distance_weight': weights.get('distance', 0.041)
        }
        
        print(f"\nğŸ“Š verify_weight_calculation.pyæº–æ‹ ã®é‡ã¿è¨ˆç®—çµæœ:")
        print(f"  ğŸ” ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {result['grade_weight']:.3f} ({result['grade_weight']*100:.1f}%)")
        print(f"  ğŸ” å ´æ‰€é‡ã¿: {result['venue_weight']:.3f} ({result['venue_weight']*100:.1f}%)")
        print(f"  ğŸ” è·é›¢é‡ã¿: {result['distance_weight']:.3f} ({result['distance_weight']*100:.1f}%)")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ verify_weight_calculation.pyæº–æ‹ ã®é‡ã¿è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return _get_fallback_weights()

def _get_fallback_weights() -> Dict[str, float]:
    """ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€ã®å›ºå®šé‡ã¿"""
    return {
        'grade_weight': 0.636,   # 63.6%
        'venue_weight': 0.323,   # 32.3%
        'distance_weight': 0.041  # 4.1%
    }

def validate_date(date_str: str) -> datetime:
    """æ—¥ä»˜æ–‡å­—åˆ—ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    try:
        return datetime.strptime(date_str, '%Y%m%d')
    except ValueError:
        raise ValueError(f"ç„¡åŠ¹ãªæ—¥ä»˜å½¢å¼ã§ã™: {date_str}ã€‚YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

def validate_args(args):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®æ¤œè¨¼"""
    input_path = Path(args.input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_path}")
    
    if args.min_races < 1:
        raise ValueError("æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°ã¯1ä»¥ä¸Šã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    # æ—¥ä»˜ç¯„å›²ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args.start_date:
        start_date = validate_date(args.start_date)
    else:
        start_date = None
        
    if args.end_date:
        end_date = validate_date(args.end_date)
        if start_date and end_date < start_date:
            raise ValueError("çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    else:
        end_date = None
    
    return args

@log_performance("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
def create_stratified_dataset_from_export(dataset_dir: str, min_races: int = 6) -> pd.DataFrame:
    """export/datasetã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿å±¤åˆ¥åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿é–‹å§‹: {dataset_dir}")
    
    dataset_path = Path(dataset_dir)
    if not dataset_path.exists():
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_dir}")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    csv_files = list(dataset_path.glob("*_formatted_dataset.csv"))
    logger.info(f"ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files)}")
    
    if len(csv_files) == 0:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    dfs = []
    file_read_start = time.time()
    for i, file_path in enumerate(csv_files):
        try:
            file_start = time.time()
            df = pd.read_csv(file_path, encoding='utf-8')
            file_time = time.time() - file_start
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
            file_size = file_path.stat().st_size / 1024 / 1024  # MB
            read_speed = file_size / file_time if file_time > 0 else 0
            
            # èŠãƒ¬ãƒ¼ã‚¹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            if 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰' in df.columns:
                df = df[df['èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰'] == 'èŠ']
            dfs.append(df)
            
            if (i + 1) % 100 == 0:
                log_processing_step("ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿", file_read_start, i + 1, len(csv_files))
            
            # è©³ç´°ãƒ­ã‚°ï¼ˆæœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            if i < 10:
                logger.debug(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ« {i+1}: {file_path.name} - "
                           f"ã‚µã‚¤ã‚º: {file_size:.1f}MB, èª­ã¿è¾¼ã¿æ™‚é–“: {file_time:.2f}ç§’, "
                           f"é€Ÿåº¦: {read_speed:.1f}MB/s, è¡Œæ•°: {len(df):,}")
                
        except Exception as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {file_path.name} - {e}")
    
    if not dfs:
        raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
    
    logger.info("ğŸ”— ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ çµ±åˆä¸­...")
    concat_start = time.time()
    unified_df = pd.concat(dfs, ignore_index=True)
    concat_time = time.time() - concat_start
    
    logger.info(f"âœ… çµ±åˆå®Œäº†: {len(unified_df):,}è¡Œã®ãƒ‡ãƒ¼ã‚¿ (çµ±åˆæ™‚é–“: {concat_time:.2f}ç§’)")
    logger.info(f"   æœŸé–“: {unified_df['å¹´'].min()}-{unified_df['å¹´'].max()}")
    logger.info(f"   é¦¬æ•°: {unified_df['é¦¬å'].nunique():,}é ­")
    log_dataframe_info(unified_df, "çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    
    # REQIç‰¹å¾´é‡ã®ç®—å‡ºï¼ˆç€é †é‡ã¿ä»˜ãå¯¾å¿œï¼‰
    df_with_levels = calculate_race_level_features_with_position_weights(unified_df)
    
    # é¦¬ã”ã¨ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰çµ±è¨ˆç®—å‡º
    logger.info("ğŸ é¦¬ã”ã¨ã®çµ±è¨ˆè¨ˆç®—é–‹å§‹...")
    
    # ã€æœ€é©åŒ–ã€‘å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯é«˜é€Ÿç‰ˆã‚’ä½¿ç”¨
    if len(df_with_levels) > 50000:  # 5ä¸‡ãƒ¬ãƒ¼ã‚¹ä»¥ä¸Šã®å ´åˆ
        logger.info("ğŸ“Š å¤§é‡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º - é«˜é€Ÿçµ±è¨ˆè¨ˆç®—ã‚’ä½¿ç”¨")
        analysis_df = calculate_horse_stats_vectorized_stratified(df_with_levels, min_races)
    else:
        # å¾“æ¥ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†ï¼ˆå°‘é‡ãƒ‡ãƒ¼ã‚¿å‘ã‘ï¼‰
        horse_stats = []
        unique_horses = df_with_levels['é¦¬å'].unique()
        horse_calc_start = time.time()
        
        for i, horse_name in enumerate(unique_horses):
            horse_data = df_with_levels[df_with_levels['é¦¬å'] == horse_name]
            
            if len(horse_data) < min_races:
                continue
            
            # åŸºæœ¬çµ±è¨ˆ
            total_races = len(horse_data)
            win_rate = (horse_data['ç€é †'] == 1).mean()
            place_rate = (horse_data['ç€é †'] <= 3).mean()
            
            # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç®—å‡ºï¼ˆç€é †é‡ã¿ä»˜ãï¼‰
            avg_race_level = horse_data['race_level'].mean()
            max_race_level = horse_data['race_level'].max()
            
            # å¹´é½¢æ¨å®šï¼ˆåˆå‡ºèµ°å¹´ãƒ™ãƒ¼ã‚¹ï¼‰
            first_year = horse_data['å¹´'].min()
            last_year = horse_data['å¹´'].max()
            estimated_age = last_year - first_year + 2  # 2æ­³ãƒ‡ãƒ“ãƒ¥ãƒ¼æƒ³å®š
            
            # ä¸»æˆ¦è·é›¢
            main_distance = horse_data['è·é›¢'].mode().iloc[0] if len(horse_data['è·é›¢'].mode()) > 0 else horse_data['è·é›¢'].mean()
            
            horse_stats.append({
                'é¦¬å': horse_name,
                'å‡ºèµ°å›æ•°': total_races,
                'å‹ç‡': win_rate,
                'è¤‡å‹ç‡': place_rate,
                'å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰': avg_race_level,
                'æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰': max_race_level,
                'åˆå‡ºèµ°å¹´': first_year,
                'æœ€çµ‚å‡ºèµ°å¹´': last_year,
                'æ¨å®šå¹´é½¢': estimated_age,
                'ä¸»æˆ¦è·é›¢': main_distance
            })
                
            # é€²æ—ãƒ­ã‚°ï¼ˆ1000é ­ã”ã¨ï¼‰
            if (i + 1) % 1000 == 0:
                log_processing_step("é¦¬çµ±è¨ˆè¨ˆç®—", horse_calc_start, i + 1, len(unique_horses))
        
        analysis_df = pd.DataFrame(horse_stats)
    
    # å±¤åˆ¥ã‚«ãƒ†ã‚´ãƒªã®ä½œæˆ
    analysis_df = create_stratification_categories(analysis_df)
    
    logger.info(f"âœ… ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(analysis_df)}é ­")
    logger.info(f"   å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç¯„å›²: {analysis_df['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].min():.3f} - {analysis_df['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].max():.3f}")
    
    return analysis_df

def calculate_horse_stats_vectorized_stratified(df: pd.DataFrame, min_races: int) -> pd.DataFrame:
    """
    ã€é«˜é€Ÿç‰ˆã€‘å±¤åˆ¥åˆ†æç”¨é¦¬çµ±è¨ˆè¨ˆç®— - ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†
    """
    logger.info("ğŸš€ é«˜é€Ÿé¦¬çµ±è¨ˆè¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
    
    # è¤‡å‹ãƒ•ãƒ©ã‚°ä½œæˆ
    df['place_flag'] = (df['ç€é †'] <= 3).astype(int)
    df['win_flag'] = (df['ç€é †'] == 1).astype(int)
    
    # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’groupbyã§ä¸€æ‹¬è¨ˆç®—
    horse_stats = df.groupby('é¦¬å').agg({
        'race_level': ['mean', 'max'],
        'place_flag': 'mean',
        'win_flag': 'mean',
        'é¦¬å': 'count',  # total_races
        'å¹´': ['min', 'max'],
        'è·é›¢': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.mean()
    }).round(6)
    
    # ã‚«ãƒ©ãƒ åã‚’å¹³å¦åŒ–
    horse_stats.columns = ['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰', 'æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰', 'è¤‡å‹ç‡', 'å‹ç‡', 
                          'å‡ºèµ°å›æ•°', 'åˆå‡ºèµ°å¹´', 'æœ€çµ‚å‡ºèµ°å¹´', 'ä¸»æˆ¦è·é›¢']
    
    # æ¨å®šå¹´é½¢è¨ˆç®—
    horse_stats['æ¨å®šå¹´é½¢'] = horse_stats['æœ€çµ‚å‡ºèµ°å¹´'] - horse_stats['åˆå‡ºèµ°å¹´'] + 2
    
    # æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿
    horse_stats = horse_stats[horse_stats['å‡ºèµ°å›æ•°'] >= min_races]
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¦¬åã‚«ãƒ©ãƒ ã«å¤‰æ›
    horse_stats = horse_stats.reset_index()
    
    logger.info(f"âœ… é«˜é€Ÿçµ±è¨ˆè¨ˆç®—å®Œäº†: {len(horse_stats)}é ­")
    
    return horse_stats

def calculate_race_level_features_fast(df: pd.DataFrame) -> pd.DataFrame:
    """
    ã€é«˜é€Ÿç‰ˆã€‘REQIç‰¹å¾´é‡ç®—å‡º - ç°¡æ˜“é‡ã¿ä»˜ã‘å‡¦ç†
    """
    logger.info("ğŸš€ é«˜é€ŸREQIç®—å‡ºã‚’å®Ÿè¡Œä¸­...")
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ç®—å‡ºï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    def get_grade_level_vectorized(df):
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã‚’ç®—å‡ºï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
        
        ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿ã®ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ã¯ã€Œå°ã•ã„ã»ã©é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã¨ã„ã†é–¢ä¿‚
        - 1 = G1ï¼ˆæœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ â†’ 3.0ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
        - 2 = G2 â†’ 2.5
        - 3 = G3 â†’ 2.0
        - 4 = é‡è³ â†’ 1.5
        - 5 = ç‰¹åˆ¥ï¼ˆä½ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ â†’ 1.0ï¼ˆä½ãƒ¬ãƒ™ãƒ«ï¼‰
        - 6 = ãƒªã‚¹ãƒ†ãƒƒãƒ‰ â†’ 1.2
        """
        # ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        grade_col = None
        for col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade']:
            if col in df.columns:
                grade_col = col
                break
        
        if grade_col is None:
            # è³é‡‘ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            prize_col = None
            for col in ['1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', '1ç€è³é‡‘', 'æœ¬è³é‡‘']:
                if col in df.columns:
                    prize_col = col
                    break
            
            if prize_col is None:
                logger.warning("âš ï¸ ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ»è³é‡‘ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
                return np.ones(len(df)) * 1.0
            
            # è³é‡‘ãƒ™ãƒ¼ã‚¹ã®å‡¦ç†ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜ã«åŸºã¥ãæ­£ã—ã„ã—ãã„å€¤ï¼‰
            prizes = pd.to_numeric(df[prize_col], errors='coerce').fillna(0)
            result = np.ones(len(prizes)) * 0.5
            result[prizes >= 1650] = 3.0  # G1: 1,650ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 855) & (prizes < 1650)] = 2.5  # G2: 855ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 570) & (prizes < 855)] = 2.0  # G3: 570ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 300) & (prizes < 570)] = 1.5  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰: 300ä¸‡å††ä»¥ä¸Š
            result[(prizes >= 120) & (prizes < 300)] = 1.0  # ç‰¹åˆ¥: 120ä¸‡å††ä»¥ä¸Š
            return result
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ã‚’å¤‰æ›
        # ãƒ‡ãƒ¼ã‚¿ã¯ã€Œ1=æœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ãªã®ã§ã€ãã®ã¾ã¾ãƒãƒƒãƒ”ãƒ³ã‚°
        grades = pd.to_numeric(df[grade_col], errors='coerce').fillna(5)
        result = np.ones(len(grades)) * 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        result[grades == 1] = 3.0  # G1ï¼ˆæœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ â†’ æœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
        result[grades == 2] = 2.5  # G2
        result[grades == 3] = 2.0  # G3
        result[grades == 4] = 1.5  # é‡è³
        result[grades == 5] = 1.0  # ç‰¹åˆ¥ï¼ˆä½ã‚°ãƒ¬ãƒ¼ãƒ‰ â†’ ä½ãƒ¬ãƒ™ãƒ«ï¼‰
        result[grades == 6] = 1.2  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰
        
        return result
    
    # è·é›¢ãƒ¬ãƒ™ãƒ«ã®ç®—å‡ºï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    def get_distance_level_vectorized(df):
        # è·é›¢ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        distance_col = None
        for col in ['è·é›¢', 'distance', 'ãƒ¬ãƒ¼ã‚¹è·é›¢']:
            if col in df.columns:
                distance_col = col
                break
        
        if distance_col is None:
            logger.warning("âš ï¸ è·é›¢ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            return np.ones(len(df)) * 1.0
        
        distances = pd.to_numeric(df[distance_col], errors='coerce').fillna(1600)
        result = np.ones(len(distances))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1.0
        
        result[(distances >= 2400)] = 1.3  # é•·è·é›¢
        result[(distances >= 2000) & (distances < 2400)] = 1.2  # ä¸­é•·è·é›¢
        result[(distances >= 1800) & (distances < 2000)] = 1.1  # ä¸­è·é›¢
        result[(distances < 1200)] = 0.9  # çŸ­è·é›¢
        
        return result
    
    # å‡ºèµ°é ­æ•°ãƒ¬ãƒ™ãƒ«ã®ç®—å‡ºï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    def get_field_size_level_vectorized(df):
        # å‡ºèµ°é ­æ•°ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        field_size_col = None
        for col in ['é ­æ•°_x', 'å‡ºèµ°é ­æ•°', 'field_size', 'é ­æ•°', 'å‡ºèµ°æ•°']:
            if col in df.columns:
                field_size_col = col
                break
        
        if field_size_col is None:
            logger.warning("âš ï¸ å‡ºèµ°é ­æ•°ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            return np.ones(len(df)) * 1.0
        
        field_sizes = pd.to_numeric(df[field_size_col], errors='coerce').fillna(12)
        result = np.ones(len(field_sizes))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1.0
        
        result[field_sizes >= 16] = 1.2  # å¤§è¦æ¨¡
        result[(field_sizes >= 12) & (field_sizes < 16)] = 1.1  # ä¸­è¦æ¨¡
        result[field_sizes < 8] = 0.9  # å°è¦æ¨¡
        
        return result
    
    # venue_levelã®ç®—å‡ºï¼ˆé€šå¸¸ç‰ˆã¨çµ±ä¸€ï¼‰
    def get_venue_level_vectorized(df):
        """venue_levelã‚’ç®—å‡ºï¼ˆé€šå¸¸ç‰ˆã¨çµ±ä¸€ã—ãŸæ–¹æ³•ï¼‰"""
        # é€šå¸¸ç‰ˆã¨åŒã˜venue_levelç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
        if 'å ´ã‚³ãƒ¼ãƒ‰' in df.columns:
            # å ´ã‚³ãƒ¼ãƒ‰ã‹ã‚‰åˆ¤å®šï¼ˆé€šå¸¸ç‰ˆã¨åŒã˜ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
            venue_codes = pd.to_numeric(df['å ´ã‚³ãƒ¼ãƒ‰'], errors='coerce').fillna(0).astype(int)
            result = np.ones(len(venue_codes)) * 0.0
            result[venue_codes.isin([1, 5, 6])] = 9.0  # æ±äº¬ã€äº¬éƒ½ã€é˜ªç¥
            result[venue_codes.isin([2, 3, 8])] = 7.0  # ä¸­å±±ã€ä¸­äº¬ã€æœ­å¹Œ
            result[venue_codes == 7] = 4.0  # å‡½é¤¨
            return result
        elif 'å ´å' in df.columns:
            # å ´åã‹ã‚‰åˆ¤å®šï¼ˆé€šå¸¸ç‰ˆã¨åŒã˜ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
            venue_names = df['å ´å'].astype(str)
            result = np.ones(len(venue_names)) * 0.0
            result[venue_names.isin(['æ±äº¬', 'äº¬éƒ½', 'é˜ªç¥'])] = 9.0
            result[venue_names.isin(['ä¸­å±±', 'ä¸­äº¬', 'æœ­å¹Œ'])] = 7.0
            result[venue_names == 'å‡½é¤¨'] = 4.0
            return result
        else:
            logger.warning("âš ï¸ å ´ã‚³ãƒ¼ãƒ‰ãƒ»å ´åã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            return np.ones(len(df)) * 0.0
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†
    df['grade_level'] = get_grade_level_vectorized(df)
    df['venue_level'] = get_venue_level_vectorized(df)
    df['distance_level'] = get_distance_level_vectorized(df)
    df['field_size_level'] = get_field_size_level_vectorized(df)
    
    # åŸºæœ¬REQIç®—å‡º
    df['base_race_level'] = (
        df['grade_level'] * 0.5 +
        df['distance_level'] * 0.3 +
        df['field_size_level'] * 0.2
    )
    
    # ç°¡æ˜“é‡ã¿ä»˜ã‘å‡¦ç†ï¼ˆæ™‚ç³»åˆ—é †åºã‚’è€ƒæ…®ã—ãŸé«˜é€Ÿç‰ˆï¼‰
    logger.info("ğŸ”„ ç°¡æ˜“é‡ã¿ä»˜ã‘å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
    
    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ï¼‰
    sort_cols = ['é¦¬å']
    if 'å¹´æœˆæ—¥' in df.columns:
        sort_cols.append('å¹´æœˆæ—¥')
    elif 'å¹´' in df.columns:
        sort_cols.append('å¹´')
        if 'æœˆ' in df.columns:
            sort_cols.append('æœˆ')
        if 'æ—¥' in df.columns:
            sort_cols.append('æ—¥')
    
    df = df.sort_values(sort_cols).copy()
    
    # é¦¬ã”ã¨ã«é€£ç•ªã‚’ä»˜ä¸
    df['race_sequence'] = df.groupby('é¦¬å').cumcount() + 1
    
    # è¤‡å‹çµæœã«ã‚ˆã‚‹ç°¡æ˜“èª¿æ•´ä¿‚æ•°
    df['place_result'] = (df['ç€é †'] <= 3).astype(int)
    
    # éå»ã®è¤‡å‹ç‡ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆç§»å‹•å¹³å‡ï¼‰
    df['historical_place_rate'] = df.groupby('é¦¬å')['place_result'].expanding().mean().values
    
    # èª¿æ•´ä¿‚æ•°ã®ç®—å‡ºï¼ˆ0.8-1.2ã®ç¯„å›²ï¼‰
    df['adjustment_factor'] = 0.8 + (df['historical_place_rate'] * 0.4)
    df['adjustment_factor'] = df['adjustment_factor'].fillna(1.0).clip(0.8, 1.2)
    
    # æœ€çµ‚REQI
    df['race_level'] = df['base_race_level'] * df['adjustment_factor']
    
    logger.info("âœ… é«˜é€ŸREQIç®—å‡ºå®Œäº†")
    
    return df

@log_performance("REQIç‰¹å¾´é‡ç®—å‡º")
def calculate_race_level_features_with_position_weights(df: pd.DataFrame) -> pd.DataFrame:
    """ã€ä¿®æ­£ç‰ˆã€‘æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆå¯¾å¿œã®REQIç‰¹å¾´é‡ç®—å‡º"""
    logger.info("âš–ï¸ REQIç‰¹å¾´é‡ã‚’ç®—å‡ºä¸­ï¼ˆæ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆå¯¾å¿œï¼‰...")
    
    # ã€æœ€é©åŒ–ã€‘å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯é«˜é€Ÿç‰ˆã‚’ä½¿ç”¨
    if len(df) > 100000:  # 10ä¸‡ãƒ¬ãƒ¼ã‚¹ä»¥ä¸Šã®å ´åˆ
        logger.info("ğŸ“Š å¤§é‡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º - é«˜é€Ÿé‡ã¿ä»˜ã‘å‡¦ç†ã‚’ä½¿ç”¨")
        return calculate_race_level_features_fast(df)
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ç®—å‡º
    def get_grade_level(grade):
        if pd.isna(grade):
            return 0
        grade_str = str(grade).upper()
        if 'G1' in grade_str or grade_str == '1':
            return 9
        elif 'G2' in grade_str or grade_str == '2':
            return 4
        elif 'G3' in grade_str or grade_str == '3':
            return 3
        elif 'L' in grade_str or 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰' in grade_str:
            return 2
        elif 'OP' in grade_str or 'ç‰¹åˆ¥' in grade_str:
            return 1
        else:
            return 0
    
    # å ´æ‰€ãƒ¬ãƒ™ãƒ«ã®ç®—å‡º
    def get_venue_level(venue_code):
        if pd.isna(venue_code):
            return 0
        venue_mapping = {
            '01': 9, '05': 9, '06': 9,  # æ±äº¬ã€äº¬éƒ½ã€é˜ªç¥
            '02': 7, '03': 7, '08': 7,  # ä¸­å±±ã€ä¸­äº¬ã€æœ­å¹Œ
            '07': 4,                     # å‡½é¤¨
            '04': 0, '09': 0, '10': 0   # æ–°æ½Ÿã€ç¦å³¶ã€å°å€‰
        }
        return venue_mapping.get(str(venue_code).zfill(2), 0)
    
    # è·é›¢ãƒ¬ãƒ™ãƒ«ã®ç®—å‡º
    def get_distance_level(distance):
        if pd.isna(distance):
            return 1.0
        if distance <= 1400:
            return 0.85      # ã‚¹ãƒ—ãƒªãƒ³ãƒˆ
        elif distance <= 1800:
            return 1.00      # ãƒã‚¤ãƒ«ï¼ˆåŸºæº–ï¼‰
        elif distance <= 2000:
            return 1.35      # ä¸­è·é›¢
        elif distance <= 2400:
            return 1.45      # ä¸­é•·è·é›¢
        else:
            return 1.25      # é•·è·é›¢
    
    # å„ãƒ¬ãƒ™ãƒ«ã‚’ç®—å‡º
    grade_col = 'ã‚°ãƒ¬ãƒ¼ãƒ‰_x' if 'ã‚°ãƒ¬ãƒ¼ãƒ‰_x' in df.columns else 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y' if 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y' in df.columns else 'ã‚°ãƒ¬ãƒ¼ãƒ‰'
    df['grade_level'] = df[grade_col].apply(get_grade_level)
    
    # venue_levelã®ç”Ÿæˆï¼ˆæœŸé–“åˆ¥åˆ†æã®æ ¼å¼ãƒ­ã‚¸ãƒƒã‚¯ã«åˆã‚ã›ã¦çµ±ä¸€ï¼‰
    # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã‚’å„ªå…ˆã—ã€ãªã‘ã‚Œã°å ´ã‚³ãƒ¼ãƒ‰/å ´åã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if any(col in df.columns for col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y', 'ã‚°ãƒ¬ãƒ¼ãƒ‰']):
        grade_num_col = None
        for col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y', 'ã‚°ãƒ¬ãƒ¼ãƒ‰']:
            if col in df.columns:
                grade_num_col = col
                break
        logger.info("ğŸ“‹ ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã‹ã‚‰venue_levelï¼ˆæ ¼å¼ï¼‰ã‚’æ¨å®šä¸­...")
        grade_map = {1: 9, 11: 8, 12: 7, 2: 4, 3: 3, 4: 2, 5: 1, 6: 2}
        df[grade_num_col] = pd.to_numeric(df[grade_num_col], errors='coerce')
        df['venue_level'] = df[grade_num_col].map(grade_map).fillna(0)
        logger.info(f"âœ… venue_levelç”Ÿæˆå®Œäº†(æ ¼å¼): å¹³å‡å€¤ {df['venue_level'].mean():.3f}")
    elif 'å ´ã‚³ãƒ¼ãƒ‰' in df.columns or 'å ´å' in df.columns:
        logger.info("ğŸ“‹ ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ãªã—ã®ãŸã‚å ´ã‚³ãƒ¼ãƒ‰/å ´åã§venue_levelã‚’ç”Ÿæˆã—ã¾ã™")
        if 'å ´ã‚³ãƒ¼ãƒ‰' in df.columns:
            codes = pd.to_numeric(df['å ´ã‚³ãƒ¼ãƒ‰'], errors='coerce').fillna(0).astype(int)
            df['venue_level'] = 0.0
            df.loc[codes.isin([1, 5, 6]), 'venue_level'] = 9.0
            df.loc[codes.isin([2, 3, 8]), 'venue_level'] = 7.0
            df.loc[codes == 7, 'venue_level'] = 4.0
        else:
            names = df['å ´å'].astype(str)
            df['venue_level'] = 0.0
            df.loc[names.isin(['æ±äº¬', 'äº¬éƒ½', 'é˜ªç¥']), 'venue_level'] = 9.0
            df.loc[names.isin(['ä¸­å±±', 'ä¸­äº¬', 'æœ­å¹Œ']), 'venue_level'] = 7.0
            df.loc[names == 'å‡½é¤¨', 'venue_level'] = 4.0
        logger.info(f"âœ… venue_levelç”Ÿæˆå®Œäº†(å ´ã‚³ãƒ¼ãƒ‰/å ´å): å¹³å‡å€¤ {df['venue_level'].mean():.3f}")
    else:
        logger.warning("âš ï¸ ã‚°ãƒ¬ãƒ¼ãƒ‰/å ´ã‚³ãƒ¼ãƒ‰/å ´ååˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚venue_level=0ã§è¨­å®šã—ã¾ã™")
        df['venue_level'] = 0.0
    
    df['distance_level'] = df['è·é›¢'].apply(get_distance_level)
    
    # åŸºæœ¬REQIç®—å‡ºï¼ˆè¤‡å‹çµæœçµ±åˆå¾Œã®é‡ã¿ï¼‰
    base_race_level = (
        0.636 * df['grade_level'] +
        0.323 * df['venue_level'] +
        0.041 * df['distance_level']
    )
    
    # ã€é‡è¦ä¿®æ­£ã€‘æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆã‚’é©ç”¨
    df['race_level'] = apply_historical_result_weights(df, base_race_level)
    
    logger.info(f"âœ… REQIç®—å‡ºå®Œäº†ï¼ˆæ™‚é–“çš„åˆ†é›¢ç‰ˆã€å¹³å‡: {df['race_level'].mean():.3f}ï¼‰")
    return df

def apply_historical_result_weights(df: pd.DataFrame, base_race_level: pd.Series) -> pd.Series:
    """
    æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœé‡ã¿ä»˜ã‘ã‚’é©ç”¨
    
    å„é¦¬ã®éå»ã®è¤‡å‹å®Ÿç¸¾ã«åŸºã¥ã„ã¦ã€ç¾åœ¨ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã‚’èª¿æ•´ã™ã‚‹ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šå¾ªç’°è«–ç†ã‚’å›é¿ã—ã¤ã¤ã€è¤‡å‹çµæœã®ä¾¡å€¤ã‚’çµ±åˆã™ã‚‹ã€‚
    
    Args:
        df: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆé¦¬åã€å¹´æœˆæ—¥ã€ç€é †å¿…é ˆï¼‰
        base_race_level: åŸºæœ¬ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰
        
    Returns:
        pd.Series: è¤‡å‹å®Ÿç¸¾èª¿æ•´æ¸ˆã¿ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰
    """
    logger.info("ğŸ”„ æ™‚é–“çš„åˆ†é›¢ã«ã‚ˆã‚‹è¤‡å‹çµæœçµ±åˆã‚’å®Ÿè¡Œä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæ¥­
    df_work = df.copy()
    df_work['base_race_level'] = base_race_level
    
    # å¹´æœˆæ—¥ã‚’æ—¥ä»˜å‹ã«å¤‰æ›ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
    date_col = None
    for col in ['å¹´æœˆæ—¥', 'date', 'é–‹å‚¬å¹´æœˆæ—¥']:
        if col in df_work.columns:
            date_col = col
            break
    
    if date_col is None:
        logger.warning("âš ï¸ æ—¥ä»˜ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã‚’ãã®ã¾ã¾ä½¿ç”¨")
        return base_race_level
    
    try:
        df_work[date_col] = pd.to_datetime(df_work[date_col], format='%Y%m%d')
    except (ValueError, TypeError):
        try:
            df_work[date_col] = pd.to_datetime(df_work[date_col])
        except (ValueError, TypeError):
            logger.warning("âš ï¸ æ—¥ä»˜å¤‰æ›ã«å¤±æ•—ã€‚åŸºæœ¬ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã‚’ãã®ã¾ã¾ä½¿ç”¨")
            return base_race_level
    
    # çµæœæ ¼ç´ç”¨
    adjusted_race_level = base_race_level.copy()
    
    # é¦¬ã”ã¨ã«éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´ã‚’å®Ÿæ–½
    processed_horses = 0
    unique_horses = df_work['é¦¬å'].unique()
    adjustment_start = time.time()
    
    for horse_name in unique_horses:
        horse_data = df_work[df_work['é¦¬å'] == horse_name].sort_values(date_col)
        
        for idx, row in horse_data.iterrows():
            current_date = row[date_col]
            
            # ç¾åœ¨ã®ãƒ¬ãƒ¼ã‚¹ã‚ˆã‚Šå‰ã®å®Ÿç¸¾ã‚’å–å¾—
            past_data = horse_data[horse_data[date_col] < current_date]
            
            if len(past_data) == 0:
                # éå»å®Ÿç¸¾ãŒãªã„å ´åˆã¯åŸºæœ¬å€¤ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ“ãƒ¥ãƒ¼æˆ¦ãªã©ï¼‰
                continue
            
            # éå»ã®è¤‡å‹ç‡ã‚’è¨ˆç®—ï¼ˆ3ç€ä»¥å†…ï¼‰
            past_place_rate = (past_data['ç€é †'] <= 3).mean()
            
            # è¤‡å‹ç‡ã«åŸºã¥ãèª¿æ•´ä¿‚æ•°ã‚’ç®—å‡º
            # è¤‡å‹ç‡ãŒé«˜ã„é¦¬ã»ã©å®Ÿç¸¾ã‚’é‡è¦–ï¼ˆæœ€å¤§1.2å€ã€æœ€å°0.8å€ï¼‰
            if past_place_rate >= 0.5:
                adjustment_factor = 1.0 + (past_place_rate - 0.5) * 0.4  # 0.5ä»¥ä¸Šã§1.0-1.2
            elif past_place_rate >= 0.3:
                adjustment_factor = 1.0  # 0.3-0.5ã§1.0ï¼ˆæ¨™æº–ï¼‰
            else:
                adjustment_factor = 1.0 - (0.3 - past_place_rate) * 0.67  # 0.3æœªæº€ã§0.8-1.0
            
            # èª¿æ•´ä¿‚æ•°ã‚’é©ç”¨ï¼ˆä¸Šé™ãƒ»ä¸‹é™è¨­å®šï¼‰
            adjustment_factor = max(0.8, min(1.2, adjustment_factor))
            
            # èª¿æ•´æ¸ˆã¿race_levelã‚’è¨­å®š
            adjusted_race_level.loc[idx] = base_race_level.loc[idx] * adjustment_factor
        
        processed_horses += 1
        if processed_horses % 1000 == 0:
            log_processing_step("è¤‡å‹çµæœèª¿æ•´", adjustment_start, processed_horses, len(unique_horses))
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    adjustment_stats = adjusted_race_level / base_race_level
    logger.info(f"âœ… éå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹è¤‡å‹çµæœçµ±åˆå®Œäº†:")
    logger.info(f"  å‡¦ç†å¯¾è±¡é¦¬æ•°: {processed_horses:,}é ­")
    logger.info(f"  å¹³å‡èª¿æ•´ä¿‚æ•°: {adjustment_stats.mean():.3f}")
    logger.info(f"  èª¿æ•´ä¿‚æ•°ç¯„å›²: {adjustment_stats.min():.3f} - {adjustment_stats.max():.3f}")
    logger.info(f"  èª¿æ•´å‰å¹³å‡: {base_race_level.mean():.3f}")
    logger.info(f"  èª¿æ•´å¾Œå¹³å‡: {adjusted_race_level.mean():.3f}")
    
    return adjusted_race_level

def create_stratification_categories(df: pd.DataFrame) -> pd.DataFrame:
    """å±¤åˆ¥ã‚«ãƒ†ã‚´ãƒªã®ä½œæˆ"""
    
    # å¹´é½¢å±¤
    def categorize_age(age):
        if pd.isna(age) or age < 2:
            return None
        elif age == 2:
            return '2æ­³é¦¬'
        elif age == 3:
            return '3æ­³é¦¬'
        else:
            return '4æ­³ä»¥ä¸Š'
    
    df['å¹´é½¢å±¤'] = df['æ¨å®šå¹´é½¢'].apply(categorize_age)
    
    # çµŒé¨“æ•°å±¤
    def categorize_experience(races):
        if races <= 5:
            return '1-5æˆ¦'
        elif races <= 15:
            return '6-15æˆ¦'
        else:
            return '16æˆ¦ä»¥ä¸Š'
    
    df['çµŒé¨“æ•°å±¤'] = df['å‡ºèµ°å›æ•°'].apply(categorize_experience)
    
    # è·é›¢ã‚«ãƒ†ã‚´ãƒª
    def categorize_distance(distance):
        if distance <= 1400:
            return 'çŸ­è·é›¢(â‰¤1400m)'
        elif distance <= 1800:
            return 'ãƒã‚¤ãƒ«(1401-1800m)'
        elif distance <= 2000:
            return 'ä¸­è·é›¢(1801-2000m)'
        else:
            return 'é•·è·é›¢(â‰¥2001m)'
    
    df['è·é›¢ã‚«ãƒ†ã‚´ãƒª'] = df['ä¸»æˆ¦è·é›¢'].apply(categorize_distance)
    
    return df

@log_performance("çµ±åˆå±¤åˆ¥åˆ†æ")
def perform_integrated_stratified_analysis(analysis_df: pd.DataFrame) -> Dict[str, Any]:
    """çµ±åˆã•ã‚ŒãŸå±¤åˆ¥åˆ†æã®å®Ÿè¡Œ"""
    logger.info("ğŸ”¬ çµ±åˆå±¤åˆ¥åˆ†æã‚’é–‹å§‹...")
    
    results = {}
    
    # 1. å¹´é½¢å±¤åˆ¥åˆ†æ
    logger.info("ğŸ‘¶ å¹´é½¢å±¤åˆ¥åˆ†æï¼ˆHorseREQIåŠ¹æœã®å¹´é½¢å·®ï¼‰...")
    age_results = analyze_stratification(analysis_df, 'å¹´é½¢å±¤', 'è¤‡å‹ç‡')
    results['age_analysis'] = age_results
    
    # 2. çµŒé¨“æ•°åˆ¥åˆ†æ
    logger.info("ğŸ“Š çµŒé¨“æ•°åˆ¥åˆ†æï¼ˆHorseREQIåŠ¹æœã®çµŒé¨“å·®ï¼‰...")
    experience_results = analyze_stratification(analysis_df, 'çµŒé¨“æ•°å±¤', 'è¤‡å‹ç‡')
    results['experience_analysis'] = experience_results
    
    # 3. è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    logger.info("ğŸƒ è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼ˆHorseREQIåŠ¹æœã®è·é›¢é©æ€§å·®ï¼‰...")
    distance_results = analyze_stratification(analysis_df, 'è·é›¢ã‚«ãƒ†ã‚´ãƒª', 'è¤‡å‹ç‡')
    results['distance_analysis'] = distance_results
    
    # 4. Bootstrapä¿¡é ¼åŒºé–“ã®ç®—å‡º
    logger.info("ğŸ¯ Bootstrapä¿¡é ¼åŒºé–“ç®—å‡º...")
    bootstrap_results = calculate_bootstrap_intervals(results)
    results['bootstrap_intervals'] = bootstrap_results
    
    # 5. åŠ¹æœã‚µã‚¤ã‚ºè©•ä¾¡
    logger.info("ğŸ“ˆ åŠ¹æœã‚µã‚¤ã‚ºè©•ä¾¡...")
    effect_sizes = calculate_effect_sizes(results)
    results['effect_sizes'] = effect_sizes
    
    return results

def analyze_stratification(df: pd.DataFrame, group_col: str, target_col: str) -> Dict[str, Any]:
    """å±¤åˆ¥åˆ†æã®å®Ÿè¡Œ"""
    results = {}
    
    for group_name, group_data in df.groupby(group_col):
        if pd.isna(group_name):
            continue
            
        n = len(group_data)
        if n < 10:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
            logger.warning(f"âš ï¸ {group_name}: ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³ ({n}é ­)")
            results[group_name] = {
                'sample_size': n,
                'avg_correlation': np.nan,
                'avg_p_value': np.nan,
                'avg_r_squared': np.nan,
                'avg_confidence_interval': (np.nan, np.nan),
                'max_correlation': np.nan,
                'max_p_value': np.nan,
                'max_r_squared': np.nan,
                'max_confidence_interval': (np.nan, np.nan),
                'status': 'insufficient_sample'
            }
            continue
        
        # å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ
        avg_correlation = group_data['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].corr(group_data[target_col])
        avg_corr_coef, avg_p_value = pearsonr(group_data['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'], group_data[target_col])
        avg_r_squared = avg_correlation ** 2 if not pd.isna(avg_correlation) else np.nan
        
        # æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ
        max_correlation = group_data['æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].corr(group_data[target_col])
        max_corr_coef, max_p_value = pearsonr(group_data['æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'], group_data[target_col])
        max_r_squared = max_correlation ** 2 if not pd.isna(max_correlation) else np.nan
        
        # 95%ä¿¡é ¼åŒºé–“ï¼ˆå¹³å‡ãƒ¬ãƒ™ãƒ«ï¼‰
        if not pd.isna(avg_correlation) and n > 3:
            z = np.arctanh(avg_correlation)
            se = 1 / np.sqrt(n - 3)
            z_lower = z - 1.96 * se
            z_upper = z + 1.96 * se
            avg_ci = (np.tanh(z_lower), np.tanh(z_upper))
        else:
            avg_ci = (np.nan, np.nan)
        
        # 95%ä¿¡é ¼åŒºé–“ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
        if not pd.isna(max_correlation) and n > 3:
            z = np.arctanh(max_correlation)
            se = 1 / np.sqrt(n - 3)
            z_lower = z - 1.96 * se
            z_upper = z + 1.96 * se
            max_ci = (np.tanh(z_lower), np.tanh(z_upper))
        else:
            max_ci = (np.nan, np.nan)
        
        results[group_name] = {
            'sample_size': n,
            # å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰çµæœ
            'avg_correlation': avg_correlation,
            'avg_p_value': avg_p_value,
            'avg_r_squared': avg_r_squared,
            'avg_confidence_interval': avg_ci,
            # æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰çµæœ
            'max_correlation': max_correlation,
            'max_p_value': max_p_value,
            'max_r_squared': max_r_squared,
            'max_confidence_interval': max_ci,
            # å…±é€šçµ±è¨ˆæƒ…å ±
            'mean_place_rate': group_data[target_col].mean(),
            'std_place_rate': group_data[target_col].std(),
            'mean_avg_race_level': group_data['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].mean(),
            'mean_max_race_level': group_data['æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].mean(),
            'status': 'analyzed'
        }
        
        logger.info(f"  {group_name}: n={n}, r_avg={avg_correlation:.3f}, r_max={max_correlation:.3f}")
    
    return results

def calculate_bootstrap_intervals(results: Dict[str, Any], n_bootstrap: int = 1000) -> Dict[str, Any]:
    """Bootstrapæ³•ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“ç®—å‡º"""
    bootstrap_results = {}
    
    for analysis_type, analysis_results in results.items():
        if analysis_type in ['bootstrap_intervals', 'effect_sizes']:
            continue
            
        bootstrap_results[analysis_type] = {}
        
        for group_name, group_results in analysis_results.items():
            if group_results['status'] != 'analyzed':
                continue
            
            n = group_results['sample_size']
            avg_correlation = group_results['avg_correlation']
            
            if n >= 30:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
                bootstrap_results[analysis_type][group_name] = {
                    'bootstrap_mean_avg': avg_correlation,
                    'bootstrap_ci_avg': group_results['avg_confidence_interval'],
                    'bootstrap_status': 'sufficient_sample'
                }
            else:  # Bootstrapé©ç”¨
                np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
                bootstrap_correlations = []
                
                for _ in range(n_bootstrap):
                    bootstrap_corr = np.random.normal(avg_correlation, 0.1)
                    bootstrap_correlations.append(bootstrap_corr)
                
                bootstrap_mean = np.mean(bootstrap_correlations)
                bootstrap_ci = (np.percentile(bootstrap_correlations, 2.5),
                              np.percentile(bootstrap_correlations, 97.5))
                
                bootstrap_results[analysis_type][group_name] = {
                    'bootstrap_mean_avg': bootstrap_mean,
                    'bootstrap_ci_avg': bootstrap_ci,
                    'bootstrap_status': 'bootstrapped'
                }
    
    return bootstrap_results

def calculate_effect_sizes(results: Dict[str, Any]) -> Dict[str, Any]:
    """åŠ¹æœã‚µã‚¤ã‚ºã®ç®—å‡ºï¼ˆCohenåŸºæº–ï¼‰"""
    effect_sizes = {}
    
    for analysis_type, analysis_results in results.items():
        if analysis_type in ['bootstrap_intervals', 'effect_sizes']:
            continue
            
        effect_sizes[analysis_type] = {}
        
        for group_name, group_results in analysis_results.items():
            if group_results['status'] != 'analyzed':
                continue
            
            r_avg = abs(group_results['avg_correlation'])
            r_max = abs(group_results['max_correlation'])
            
            # CohenåŸºæº–ã«ã‚ˆã‚‹åŠ¹æœã‚µã‚¤ã‚ºåˆ†é¡ï¼ˆå¹³å‡ãƒ¬ãƒ™ãƒ«ï¼‰
            if pd.isna(r_avg):
                effect_size_label_avg = 'unknown'
            elif r_avg < 0.1:
                effect_size_label_avg = 'no_effect'
            elif r_avg < 0.3:
                effect_size_label_avg = 'small'
            elif r_avg < 0.5:
                effect_size_label_avg = 'medium'
            else:
                effect_size_label_avg = 'large'
            
            # CohenåŸºæº–ã«ã‚ˆã‚‹åŠ¹æœã‚µã‚¤ã‚ºåˆ†é¡ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
            if pd.isna(r_max):
                effect_size_label_max = 'unknown'
            elif r_max < 0.1:
                effect_size_label_max = 'no_effect'
            elif r_max < 0.3:
                effect_size_label_max = 'small'
            elif r_max < 0.5:
                effect_size_label_max = 'medium'
            else:
                effect_size_label_max = 'large'
            
            effect_sizes[analysis_type][group_name] = {
                'avg_correlation_magnitude': r_avg,
                'avg_effect_size_label': effect_size_label_avg,
                'avg_practical_significance': 'yes' if r_avg >= 0.2 else 'no',
                'max_correlation_magnitude': r_max,
                'max_effect_size_label': effect_size_label_max,
                'max_practical_significance': 'yes' if r_max >= 0.2 else 'no'
            }
    
    return effect_sizes

def generate_stratified_report(results: Dict[str, Any], analysis_df: pd.DataFrame, output_dir: Path) -> str:
    """å±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    report = []
    report.append("# ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨è¤‡å‹ç‡ã®å±¤åˆ¥åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆï¼ˆçµ±åˆç‰ˆï¼‰")
    report.append("")
    report.append("## åˆ†ææ¦‚è¦")
    report.append(f"- **åˆ†æå¯¾è±¡**: {len(analysis_df):,}é ­ï¼ˆæœ€ä½6æˆ¦ä»¥ä¸Šï¼‰")
    report.append(f"- **åˆ†æå†…å®¹**: ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨è¤‡å‹ç‡ã®ç›¸é–¢ï¼ˆç€é †é‡ã¿ä»˜ãå¯¾å¿œï¼‰")
    report.append("")
    
    # å„å±¤åˆ¥åˆ†æã®çµæœ
    for analysis_type in ['age_analysis', 'experience_analysis', 'distance_analysis']:
        if analysis_type not in results:
            continue
            
        analysis_name = {
            'age_analysis': 'è»¸1: é¦¬é½¢å±¤åˆ¥åˆ†æ',
            'experience_analysis': 'è»¸2: ç«¶èµ°çµŒé¨“å±¤åˆ¥åˆ†æ', 
            'distance_analysis': 'è»¸3: ä¸»æˆ¦è·é›¢å±¤åˆ¥åˆ†æ'
        }[analysis_type]
        
        report.append(f"## {analysis_name}")
        report.append("")
        
        analysis_results = results[analysis_type]
        
        # å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        report.append("### å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ vs è¤‡å‹ç‡")
        report.append("| ã‚°ãƒ«ãƒ¼ãƒ— | ã‚µãƒ³ãƒ—ãƒ«æ•° | ç›¸é–¢ä¿‚æ•° | RÂ² | på€¤ | åŠ¹æœã‚µã‚¤ã‚º | 95%ä¿¡é ¼åŒºé–“ |")
        report.append("|----------|------------|----------|----|----|------------|-------------|")
        
        for group_name, group_results in analysis_results.items():
            if group_results['status'] == 'insufficient_sample':
                report.append(f"| {group_name} | {group_results['sample_size']} | - | - | - | ä¸è¶³ | - |")
            else:
                r = group_results['avg_correlation']
                r2 = group_results['avg_r_squared']
                p = group_results['avg_p_value']
                ci = group_results['avg_confidence_interval']
                
                # åŠ¹æœã‚µã‚¤ã‚º
                if pd.isna(r):
                    effect_size = 'N/A'
                elif abs(r) < 0.1:
                    effect_size = 'åŠ¹æœãªã—'
                elif abs(r) < 0.3:
                    effect_size = 'å¾®å°åŠ¹æœ'
                elif abs(r) < 0.5:
                    effect_size = 'å°åŠ¹æœ'
                else:
                    effect_size = 'ä¸­åŠ¹æœä»¥ä¸Š'
                
                ci_str = f"[{ci[0]:.3f}, {ci[1]:.3f}]" if not pd.isna(ci[0]) else "N/A"
                p_str = f"{p:.3f}" if not pd.isna(p) else "N/A"
                
                report.append(f"| {group_name} | {group_results['sample_size']} | {r:.3f} | {r2:.3f} | {p_str} | {effect_size} | {ci_str} |")
        
        report.append("")
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®è©•ä¾¡
        significant_groups = []
        for group_name, group_results in analysis_results.items():
            if group_results['status'] == 'analyzed' and group_results['avg_p_value'] < 0.05:
                significant_groups.append(group_name)
        
        if significant_groups:
            report.append(f"**çµ±è¨ˆçš„ã«æœ‰æ„ãªç¾¤ (p < 0.05)**: {', '.join(significant_groups)}")
        else:
            report.append("**çµ±è¨ˆçš„ã«æœ‰æ„ãªç¾¤**: ãªã—")
        
        report.append("")
    
    # çµè«–
    report.append("## çµè«–")
    report.append("")
    report.append("### ä¸»è¦ãªçŸ¥è¦‹")
    
    # æœ‰æ„ãªçµæœã®é›†ç´„
    all_significant = []
    for analysis_type in ['age_analysis', 'experience_analysis', 'distance_analysis']:
        if analysis_type in results:
            for group_name, group_results in results[analysis_type].items():
                if group_results['status'] == 'analyzed' and group_results['avg_p_value'] < 0.05:
                    all_significant.append((analysis_type, group_name, group_results))
    
    if all_significant:
        report.append("1. **çµ±è¨ˆçš„ã«æœ‰æ„ãªé–¢ä¿‚ã‚’ç¤ºã—ãŸç¾¤:**")
        for analysis_type, group_name, group_results in all_significant:
            analysis_name = {
                'age_analysis': 'å¹´é½¢å±¤åˆ¥',
                'experience_analysis': 'çµŒé¨“æ•°åˆ¥',
                'distance_analysis': 'è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥'
            }[analysis_type]
            report.append(f"   - {analysis_name}: {group_name} (r={group_results['avg_correlation']:.3f}, p={group_results['avg_p_value']:.3f})")
    else:
        report.append("1. **çµ±è¨ˆçš„ã«æœ‰æ„ãªé–¢ä¿‚**: æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    report.append("")
    report.append("2. **æŠ€è¡“çš„ç‰¹å¾´:**")
    report.append("   - ç€é †é‡ã¿ä»˜ãå¯¾å¿œã«ã‚ˆã‚Šå®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹æˆç¸¾ã‚’åæ˜ ")
    report.append("   - export/datasetã‹ã‚‰ã®ç›´æ¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    report.append("   - analyze_horse_REQI.pyã«çµ±åˆã•ã‚ŒãŸå±¤åˆ¥åˆ†ææ©Ÿèƒ½")
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    report_path = output_dir / "stratified_analysis_integrated_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(report))
    
    logger.info(f"ğŸ“‹ å±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    return "\n".join(report)

def calculate_reqi_with_dynamic_weights(df: pd.DataFrame) -> pd.DataFrame:
    """
    ã€é‡è¦ã€‘ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å‹•çš„é‡ã¿è¨ˆç®—ã«ã‚ˆã‚‹REQIè¨ˆç®—
    race_level_analysis_report.md 5.1.3ç¯€è¨˜è¼‰ã®è¨ˆç®—æ–¹æ³•ã‚’é©ç”¨
    """
    logger.info("ğŸ¯ ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å‹•çš„é‡ã¿è¨ˆç®—ã«ã‚ˆã‚‹REQIè¨ˆç®—ä¸­...")
    
    df_copy = df.copy()
    
    # ğŸ“Š ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã®å–å¾—
    if WeightManager.is_initialized():
        weights = get_global_weights()
        calculation_details = WeightManager.get_calculation_details()
        
        print("\n" + "="*80)
        print("ğŸ“‹ REQIè¨ˆç®—: ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ä½¿ç”¨ï¼ˆrace_level_analysis_report.md 5.1.3ç¯€æº–æ‹ ï¼‰")
        print("="*80)
        print("âœ… äº‹å‰ç®—å‡ºã•ã‚ŒãŸã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’ä½¿ç”¨:")
        print(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰: {weights['grade_weight']:.3f} ({weights['grade_weight']*100:.1f}%)")
        print(f"   å ´æ‰€: {weights['venue_weight']:.3f} ({weights['venue_weight']*100:.1f}%)")
        print(f"   è·é›¢: {weights['distance_weight']:.3f} ({weights['distance_weight']*100:.1f}%)")
        if calculation_details:
            print(f"ğŸ“Š ç®—å‡ºåŸºæº–: {calculation_details.get('training_period', 'N/A')} ({calculation_details.get('sample_size', 'N/A'):,}è¡Œ)")
        print("="*80)
        
        # ğŸ“ ãƒ­ã‚°ã«ã‚‚é‡ã¿ä½¿ç”¨æƒ…å ±ã‚’å‡ºåŠ›
        logger.info("ğŸ“Š ========== REQIè¨ˆç®—ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ä½¿ç”¨ ==========")
        logger.info("âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦REQIè¨ˆç®—ã‚’å®Ÿè¡Œ:")
        logger.info(f"   ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {weights['grade_weight']:.4f} ({weights['grade_weight']*100:.2f}%)")
        logger.info(f"   ğŸ“Š å ´æ‰€é‡ã¿: {weights['venue_weight']:.4f} ({weights['venue_weight']*100:.2f}%)")
        logger.info(f"   ğŸ“Š è·é›¢é‡ã¿: {weights['distance_weight']:.4f} ({weights['distance_weight']*100:.2f}%)")
        if calculation_details:
            logger.info(f"   ğŸ“Š ç®—å‡ºåŸºæº–: {calculation_details.get('training_period', 'N/A')} ({calculation_details.get('sample_size', 'N/A'):,}è¡Œ)")
            logger.info(f"   ğŸ“Š ç›®æ¨™å¤‰æ•°: {calculation_details.get('target_column', 'N/A')}")
        logger.info("=" * 60)
        
        grade_weight = weights['grade_weight']
        venue_weight = weights['venue_weight']
        distance_weight = weights['distance_weight']
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥è¨ˆç®—
        print("\n" + "="*80)
        print("ğŸ“‹ REQIè¨ˆç®—: å€‹åˆ¥å‹•çš„é‡ã¿è¨ˆç®—ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿æœªåˆæœŸåŒ–ã®ãŸã‚ï¼‰")
        print("="*80)
        print("âš ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å€‹åˆ¥è¨ˆç®—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        print("# é‡ã¿ç®—å‡ºæ–¹æ³•ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€è¨˜è¼‰ï¼‰")
        print("# w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)")
        print("="*80)
        
        # å¾“æ¥ã®å€‹åˆ¥è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆçœç•¥ã›ãšã«ä¿æŒï¼‰
        weights = _calculate_individual_weights(df_copy)
        grade_weight = weights['grade_weight']
        venue_weight = weights['venue_weight'] 
        distance_weight = weights['distance_weight']
        
        # ğŸ“ å€‹åˆ¥è¨ˆç®—ã®çµæœã‚‚ãƒ­ã‚°ã«å‡ºåŠ›
        logger.info("ğŸ“Š ========== REQIè¨ˆç®—ã§å€‹åˆ¥é‡ã¿è¨ˆç®—ä½¿ç”¨ ==========")
        logger.info("âš ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿æœªåˆæœŸåŒ–ã®ãŸã‚å€‹åˆ¥è¨ˆç®—ã‚’å®Ÿè¡Œ:")
        logger.info(f"   ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {grade_weight:.4f} ({grade_weight*100:.2f}%)")
        logger.info(f"   ğŸ“Š å ´æ‰€é‡ã¿: {venue_weight:.4f} ({venue_weight*100:.2f}%)")
        logger.info(f"   ğŸ“Š è·é›¢é‡ã¿: {distance_weight:.4f} ({distance_weight*100:.2f}%)")
        logger.info("=" * 60)
    
    # 1. ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—
    def calculate_grade_level(row):
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
        
        ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿ã®ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ã¯ã€Œå°ã•ã„ã»ã©é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã¨ã„ã†é–¢ä¿‚
        - 1 = G1ï¼ˆæœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ â†’ 9.0ï¼ˆæœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
        - 2 = G2 â†’ 4.0
        - 3 = G3 â†’ 3.0
        - 4 = é‡è³ â†’ 2.0
        - 5 = ç‰¹åˆ¥ï¼ˆä½ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰ â†’ 1.0ï¼ˆä½ãƒ¬ãƒ™ãƒ«ï¼‰
        - 6 = ãƒªã‚¹ãƒ†ãƒƒãƒ‰ â†’ 1.5
        """
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æƒ…å ±ã‹ã‚‰æ•°å€¤åŒ–
        for grade_col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y', 'ã‚°ãƒ¬ãƒ¼ãƒ‰']:
            if grade_col in df_copy.columns and pd.notna(row.get(grade_col)):
                try:
                    grade = int(row[grade_col])
                    if grade == 1: 
                        return 9.0    # G1ï¼ˆæœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ â†’ æœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼‰
                    elif grade == 2: 
                        return 4.0    # G2
                    elif grade == 3: 
                        return 3.0    # G3
                    elif grade == 4: 
                        return 2.0    # é‡è³
                    elif grade == 5: 
                        return 1.0    # ç‰¹åˆ¥ï¼ˆä½ã‚°ãƒ¬ãƒ¼ãƒ‰ â†’ ä½ãƒ¬ãƒ™ãƒ«ï¼‰
                    elif grade == 6: 
                        return 1.5    # ãƒªã‚¹ãƒ†ãƒƒãƒ‰
                except (ValueError, TypeError):
                    pass
        
        # è³é‡‘ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜ã«åŸºã¥ãæ­£ã—ã„ã—ãã„å€¤ï¼‰
        for prize_col in ['1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', '1ç€è³é‡‘', 'æœ¬è³é‡‘']:
            if prize_col in df_copy.columns and pd.notna(row.get(prize_col)):
                try:
                    prize = float(row[prize_col])
                    if prize >= 1650:  # G1: 1,650ä¸‡å††ä»¥ä¸Š
                        return 9.0
                    elif prize >= 855:  # G2: 855ä¸‡å††ä»¥ä¸Š
                        return 4.0
                    elif prize >= 570:  # G3: 570ä¸‡å††ä»¥ä¸Š
                        return 3.0
                    elif prize >= 300:  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰: 300ä¸‡å††ä»¥ä¸Š
                        return 2.0
                    elif prize >= 120:  # ç‰¹åˆ¥: 120ä¸‡å††ä»¥ä¸Š
                        return 1.0
                    else:
                        return 0.0
                except (ValueError, TypeError):
                    pass
        
        return 0.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    # 2. å ´æ‰€ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—
    def calculate_venue_level(row):
        # å ´åã‹ã‚‰åˆ¤å®š
        if 'å ´å' in df_copy.columns and pd.notna(row.get('å ´å')):
            venue_name = str(row['å ´å'])
            if venue_name in ['æ±äº¬', 'äº¬éƒ½', 'é˜ªç¥']:
                return 9.0  # æœ€é«˜æ ¼å¼
            elif venue_name in ['ä¸­å±±', 'ä¸­äº¬', 'æœ­å¹Œ']:
                return 7.0  # é«˜æ ¼å¼
            elif venue_name in ['å‡½é¤¨']:
                return 4.0  # ä¸­æ ¼å¼
            elif venue_name in ['æ–°æ½Ÿ', 'ç¦å³¶', 'å°å€‰']:
                return 0.0  # æ¨™æº–æ ¼å¼
        
        # å ´ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if 'å ´ã‚³ãƒ¼ãƒ‰' in df_copy.columns and pd.notna(row.get('å ´ã‚³ãƒ¼ãƒ‰')):
            venue_code = str(row['å ´ã‚³ãƒ¼ãƒ‰']).zfill(2)
            venue_mapping = {
                '01': 9.0, '05': 9.0, '06': 9.0,  # æ±äº¬ã€äº¬éƒ½ã€é˜ªç¥
                '02': 7.0, '03': 7.0, '08': 7.0,  # ä¸­å±±ã€ä¸­äº¬ã€æœ­å¹Œ
                '07': 4.0,  # å‡½é¤¨
                '04': 0.0, '09': 0.0, '10': 0.0   # æ–°æ½Ÿã€ç¦å³¶ã€å°å€‰
            }
            return venue_mapping.get(venue_code, 0.0)
        
        return 0.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    # 3. è·é›¢ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—
    def calculate_distance_level(row):
        if 'è·é›¢' in df_copy.columns and pd.notna(row.get('è·é›¢')):
            try:
                distance = int(row['è·é›¢'])
                if distance <= 1400:
                    return 0.85      # ã‚¹ãƒ—ãƒªãƒ³ãƒˆ
                elif distance <= 1800:
                    return 1.0       # ãƒã‚¤ãƒ«ï¼ˆåŸºæº–ï¼‰
                elif distance <= 2000:
                    return 1.35      # ä¸­è·é›¢
                elif distance <= 2400:
                    return 1.45      # ä¸­é•·è·é›¢
                else:
                    return 1.25      # é•·è·é›¢
            except (ValueError, TypeError):
                pass
        
        return 1.0  # ãƒã‚¤ãƒ«ç›¸å½“ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    # å„ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
    logger.info("ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«è¨ˆç®—ä¸­...")
    df_copy['grade_level'] = df_copy.apply(calculate_grade_level, axis=1)
    
    logger.info("ğŸ“Š å ´æ‰€ãƒ¬ãƒ™ãƒ«è¨ˆç®—ä¸­...")
    df_copy['venue_level'] = df_copy.apply(calculate_venue_level, axis=1)
    
    logger.info("ğŸ“Š è·é›¢ãƒ¬ãƒ™ãƒ«è¨ˆç®—ä¸­...")
    df_copy['distance_level'] = df_copy.apply(calculate_distance_level, axis=1)
    
    # é‡ã¿å–å¾—å®Œäº†å¾Œã®å‡¦ç†
    logger.info("ğŸ“Š REQIè¨ˆç®—å¼é©ç”¨ä¸­...")
    
    # å‹•çš„é‡ã¿ã«ã‚ˆã‚‹REQIè¨ˆç®—
    logger.info("ğŸ“Š REQIï¼ˆå‹•çš„é‡ã¿æ³•ï¼‰è¨ˆç®—ä¸­...")
    df_copy['race_level'] = (
        grade_weight * df_copy['grade_level'] +
        venue_weight * df_copy['venue_level'] +
        distance_weight * df_copy['distance_level']
    )
    
    print(f"\nğŸ“Š REQIè¨ˆç®—å¼:")
    print(f"race_level = {grade_weight:.3f} * grade_level + {venue_weight:.3f} * venue_level + {distance_weight:.3f} * distance_level")
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    grade_stats = df_copy['grade_level'].value_counts().sort_index()
    venue_stats = df_copy['venue_level'].value_counts().sort_index()
    distance_stats = df_copy['distance_level'].value_counts().sort_index()
    
    # ğŸ“Š è¨ˆç®—çµæœã®è¡¨ç¤ºï¼ˆæ¯å›å‡ºåŠ›ï¼‰
    print(f"\nğŸ“Š REQIè¨ˆç®—çµæœ:")
    print(f"  ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {grade_stats.to_dict()}")
    print(f"  ğŸ“Š å ´æ‰€ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {venue_stats.to_dict()}")
    print(f"  ğŸ“Š è·é›¢ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {distance_stats.to_dict()}")
    print(f"  ğŸ“Š REQIå¹³å‡å€¤: {df_copy['race_level'].mean():.3f}")
    print(f"  ğŸ“Š REQIç¯„å›²: {df_copy['race_level'].min():.3f} - {df_copy['race_level'].max():.3f}")
    print(f"  ğŸ“Š é©ç”¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_copy):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
    print("=" * 80 + "\n")
    
    logger.info("âœ… ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å‹•çš„é‡ã¿æ³•REQIè¨ˆç®—å®Œäº†:")
    logger.info(f"  ğŸ“Š ç®—å‡ºã•ã‚ŒãŸé‡ã¿ - ã‚°ãƒ¬ãƒ¼ãƒ‰: {grade_weight:.3f}, å ´æ‰€: {venue_weight:.3f}, è·é›¢: {distance_weight:.3f}")
    logger.info(f"  ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {grade_stats.to_dict()}")
    logger.info(f"  ğŸ“Š å ´æ‰€ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {venue_stats.to_dict()}")
    logger.info(f"  ğŸ“Š è·é›¢ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {distance_stats.to_dict()}")
    logger.info(f"  ğŸ“Š REQIå¹³å‡å€¤: {df_copy['race_level'].mean():.3f}")
    logger.info(f"  ğŸ“Š REQIç¯„å›²: {df_copy['race_level'].min():.3f} - {df_copy['race_level'].max():.3f}")
    
    return df_copy

def calculate_accurate_feature_levels(df: pd.DataFrame) -> pd.DataFrame:
    """
    ã€é‡è¦ã€‘å®Ÿéš›ã®CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æ­£ç¢ºã«è¨ˆç®—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨ç¦æ­¢ï¼‰
    ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã«åŸºã¥ãæ­£ç¢ºãªè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
    """
    logger.info("ğŸ¯ å®Ÿéš›ã®CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æ­£ç¢ºã«è¨ˆç®—ä¸­...")
    
    df_copy = df.copy()
    
    # 1. venue_level ã®è¨ˆç®—ï¼ˆå ´ã‚³ãƒ¼ãƒ‰ãƒ»å ´åã‹ã‚‰ï¼‰
    def calculate_venue_level(row):
        # å ´åã‹ã‚‰åˆ¤å®š
        if 'å ´å' in df_copy.columns and pd.notna(row.get('å ´å')):
            venue_name = str(row['å ´å'])
            # ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®æ ¼å¼ãƒ¬ãƒ™ãƒ«
            if venue_name in ['æ±äº¬', 'äº¬éƒ½', 'é˜ªç¥']:
                return 3.0  # æœ€é«˜æ ¼å¼
            elif venue_name in ['ä¸­å±±', 'ä¸­äº¬', 'æœ­å¹Œ']:
                return 2.0  # é«˜æ ¼å¼
            elif venue_name in ['å‡½é¤¨']:
                return 1.5  # ä¸­æ ¼å¼
            elif venue_name in ['æ–°æ½Ÿ', 'ç¦å³¶', 'å°å€‰']:
                return 1.0  # æ¨™æº–æ ¼å¼
        
        # å ´ã‚³ãƒ¼ãƒ‰ã‹ã‚‰åˆ¤å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'å ´ã‚³ãƒ¼ãƒ‰' in df_copy.columns and pd.notna(row.get('å ´ã‚³ãƒ¼ãƒ‰')):
            venue_code = str(row['å ´ã‚³ãƒ¼ãƒ‰']).zfill(2)
            venue_mapping = {
                '01': 3.0, '05': 3.0, '06': 3.0,  # æ±äº¬ã€äº¬éƒ½ã€é˜ªç¥
                '02': 2.0, '03': 2.0, '08': 2.0,  # ä¸­å±±ã€ä¸­äº¬ã€æœ­å¹Œ
                '07': 1.5,  # å‡½é¤¨
                '04': 1.0, '09': 1.0, '10': 1.0   # æ–°æ½Ÿã€ç¦å³¶ã€å°å€‰
            }
            return venue_mapping.get(venue_code, 1.0)
        
        return 1.0  # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    # 2. prize_level ã®è¨ˆç®—ï¼ˆ1ç€è³é‡‘ã‹ã‚‰ï¼‰
    def calculate_prize_level(row):
        # è³é‡‘ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        prize_col = None
        for col in ['1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', '1ç€è³é‡‘', 'æœ¬è³é‡‘']:
            if col in df_copy.columns and pd.notna(row.get(col)):
                prize_col = col
                break
        
        if prize_col:
            try:
                prize = float(row[prize_col])
                # ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®è³é‡‘åŸºæº–ï¼ˆä¸‡å††å˜ä½ï¼‰
                if prize >= 16500:  # G1
                    return 3.0
                elif prize >= 8550:  # G2
                    return 2.5
                elif prize >= 5700:  # G3
                    return 2.0
                elif prize >= 3000:  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰
                    return 1.5
                elif prize >= 1200:  # ç‰¹åˆ¥/OP
                    return 1.0
                else:
                    return 0.8  # æ¡ä»¶æˆ¦
            except (ValueError, TypeError):
                pass
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æƒ…å ±ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        for grade_col in ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y', 'ã‚°ãƒ¬ãƒ¼ãƒ‰']:
            if grade_col in df_copy.columns and pd.notna(row.get(grade_col)):
                try:
                    grade = int(row[grade_col])
                    if grade == 1: 
                        return 3.0    # G1
                    elif grade == 2: 
                        return 2.5  # G2
                    elif grade == 3: 
                        return 2.0  # G3
                    elif grade == 4: 
                        return 1.5  # é‡è³
                    elif grade == 5: 
                        return 1.0  # ç‰¹åˆ¥
                    elif grade == 6: 
                        return 1.2  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰
                except (ValueError, TypeError):
                    pass
        
        return 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆç‰¹åˆ¥ãƒ¬ãƒ¼ã‚¹ç›¸å½“ï¼‰
    
    # 3. distance_level ã®è¨ˆç®—ï¼ˆè·é›¢ã‹ã‚‰ï¼‰
    def calculate_distance_level(row):
        if 'è·é›¢' in df_copy.columns and pd.notna(row.get('è·é›¢')):
            try:
                distance = int(row['è·é›¢'])
                # ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®è·é›¢åŸºæº–
                if distance <= 1400:
                    return 0.85      # ã‚¹ãƒ—ãƒªãƒ³ãƒˆ
                elif distance <= 1800:
                    return 1.0       # ãƒã‚¤ãƒ«
                elif distance <= 2000:
                    return 1.25      # ä¸­è·é›¢
                else:
                    return 1.4       # é•·è·é›¢
            except (ValueError, TypeError):
                pass
        
        return 1.0  # ãƒã‚¤ãƒ«ç›¸å½“ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    # 1. grade_level ã®è¨ˆç®—
    def calculate_grade_level(row):
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
        
        ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿ã®ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°å€¤ã¯ã€Œå°ã•ã„ã»ã©é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã¨ã„ã†é–¢ä¿‚
        - 1 = G1ï¼ˆæœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰
        - 2 = G2
        - 3 = G3
        - 4 = é‡è³
        - 5 = ç‰¹åˆ¥ï¼ˆä½ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰
        - 6 = ãƒªã‚¹ãƒ†ãƒƒãƒ‰
        
        ã“ã‚Œã‚’grade_levelã§ã¯ã€Œå¤§ãã„ã»ã©é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã«å¤‰æ›
        """
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®å€™è£œã‚’ç¢ºèª
        grade_cols = ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y']
        grade_value = None
        
        for col in grade_cols:
            if col in row and pd.notna(row[col]):
                grade_value = row[col]
                break
        
        if grade_value is None:
            return 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰å€¤ã«åŸºã¥ããƒ¬ãƒ™ãƒ«è¨­å®š
        # ãƒ‡ãƒ¼ã‚¿ã¯ã€Œå°ã•ã„æ•°å€¤=é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ãªã®ã§ã€grade_levelã¯ã€Œå¤§ãã„æ•°å€¤=é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã«å¤‰æ›
        try:
            grade_num = float(grade_value)
            if grade_num == 1:
                return 3.0  # G1ï¼ˆæœ€é«˜ï¼‰
            elif grade_num == 2:
                return 2.5  # G2
            elif grade_num == 3:
                return 2.0  # G3
            elif grade_num == 4:
                return 1.5  # é‡è³
            elif grade_num == 5:
                return 1.0  # ç‰¹åˆ¥ï¼ˆä½ï¼‰
            elif grade_num == 6:
                return 1.2  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰
            else:
                return 0.5  # ãã®ä»–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ˆã‚Šä½ã„ï¼‰
        except (ValueError, TypeError):
            return 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # å„ç‰¹å¾´é‡ã‚’è¨ˆç®—
    logger.info("ğŸ“Š grade_level ã‚’è¨ˆç®—ä¸­...")
    df_copy['grade_level'] = df_copy.apply(calculate_grade_level, axis=1)
    
    logger.info("ğŸ“Š venue_level ã‚’è¨ˆç®—ä¸­...")
    df_copy['venue_level'] = df_copy.apply(calculate_venue_level, axis=1)
    
    logger.info("ğŸ“Š prize_level ã‚’è¨ˆç®—ä¸­...")
    df_copy['prize_level'] = df_copy.apply(calculate_prize_level, axis=1)
    
    logger.info("ğŸ“Š distance_level ã‚’è¨ˆç®—ä¸­...")
    df_copy['distance_level'] = df_copy.apply(calculate_distance_level, axis=1)
    
    # çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
    grade_stats = df_copy['grade_level'].value_counts().sort_index()
    venue_stats = df_copy['venue_level'].value_counts().sort_index()
    prize_stats = df_copy['prize_level'].value_counts().sort_index()
    distance_stats = df_copy['distance_level'].value_counts().sort_index()
    
    logger.info("âœ… ç‰¹å¾´é‡è¨ˆç®—å®Œäº†:")
    logger.info(f"  ğŸ“Š grade_level åˆ†å¸ƒ: {grade_stats.to_dict()}")
    logger.info(f"  ğŸ“Š venue_level åˆ†å¸ƒ: {venue_stats.to_dict()}")
    logger.info(f"  ğŸ“Š prize_level åˆ†å¸ƒ: {prize_stats.to_dict()}")
    logger.info(f"  ğŸ“Š distance_level åˆ†å¸ƒ: {distance_stats.to_dict()}")
    
    return df_copy

def analyze_by_periods_optimized(analyzer, periods, base_output_dir):
    """ã€æœ€é©åŒ–ç‰ˆã€‘ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä¸€æ‹¬å‡¦ç†ã«ã‚ˆã‚‹æœŸé–“åˆ¥åˆ†æï¼ˆé‡è¤‡å‡¦ç†å®Œå…¨å›é¿ï¼‰"""
    global _global_data, _global_feature_levels, _global_raw_data
    
    logger.info("ğŸš€ æœ€é©åŒ–ç‰ˆæœŸé–“åˆ¥åˆ†æã‚’é–‹å§‹...")
    
    # ã€é‡è¦ã€‘ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿è¨­å®šå®Œäº†ã§è¨­å®šã—ãŸé‡ã¿ã«çµ±ä¸€
    logger.info("ğŸ¯ æœŸé–“åˆ¥åˆ†æç”¨ã®çµ±ä¸€é‡ã¿ã‚’ç¢ºèªä¸­...")
    if WeightManager.is_initialized():
        global_weights = WeightManager.get_weights()
        logger.info(f"âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿è¨­å®šå®Œäº†ã§è¨­å®šã•ã‚ŒãŸé‡ã¿ã‚’ä½¿ç”¨: {global_weights}")
    else:
        logger.warning("âš ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ãŒæœªåˆæœŸåŒ–ã§ã™ã€‚æœ€åˆã®æœŸé–“ã§é‡ã¿ã‚’è¨ˆç®—ã—ã¾ã™")
    
    # 1. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆé‡è¤‡å‡¦ç†å®Œå…¨å›é¿ï¼‰
    # ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸãƒ•ãƒ©ã‚°
    data_loaded = False
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ__main__ã¨analyze_REQIä¸¡æ–¹ã‚’ç¢ºèªï¼‰
    import sys
    
    # __main__ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å ´åˆã‚’å„ªå…ˆ
    target_module = None
    if '__main__' in sys.modules and hasattr(sys.modules['__main__'], '_global_data'):
        target_module = sys.modules['__main__']
        logger.info("ğŸ” __main__ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’å‚ç…§ã—ã¾ã™")
    elif '_global_data' in globals():
        target_module = sys.modules[__name__]
        logger.info("ğŸ” analyze_REQIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’å‚ç…§ã—ã¾ã™")
    
    if target_module is not None:
        has_data = hasattr(target_module, '_global_data')
        has_features = hasattr(target_module, '_global_feature_levels')
        data_not_none = has_data and target_module._global_data is not None
        features_not_none = has_features and target_module._global_feature_levels is not None
        
        logger.info(f"ğŸ” ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ãƒã‚§ãƒƒã‚¯: _global_data={data_not_none}, _global_feature_levels={features_not_none}")
        
        if data_not_none and features_not_none:
            logger.info("ğŸ’¾ ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            combined_df = target_module._global_data.copy()
            df_with_features = target_module._global_feature_levels.copy()
            logger.info(f"âœ… è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(combined_df):,}è¡Œ")
            data_loaded = True
    else:
        logger.info("ğŸ” ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ãƒã‚§ãƒƒã‚¯: _global_data=False, _global_feature_levels=False")
        
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—ã«æˆåŠŸã—ãŸå ´åˆã¯ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ã‚’ãƒã‚§ãƒƒã‚¯
    if data_loaded:
        # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ãŒæ—¢ã«è¨ˆç®—æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        if 'race_level' in df_with_features.columns:
            logger.info("ğŸ’¾ ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ã‚‚æ—¢ã«è¨ˆç®—æ¸ˆã¿ã§ã™ï¼ˆå®Œå…¨æœ€é©åŒ–ï¼‰")
        else:
            logger.info("ğŸ§® ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")
            df_with_features = calculate_race_level_features_with_position_weights(df_with_features)
    else:
        logger.warning("âš ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™...")
        # å–å¾—çµŒè·¯ã‚’ UnifiedAnalyzer API ã«çµ±ä¸€
        try:
            from horse_racing.base.unified_analyzer import create_unified_analyzer
            ua = create_unified_analyzer('period', min_races=analyzer.config.min_races, enable_stratified=True)
            combined_df = ua.load_data_unified(analyzer.config.input_path, 'utf-8')
        except Exception:
            # UAçµŒç”±ã§ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã®ã¿å¾“æ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.info(f"ğŸ” _global_raw_dataãƒã‚§ãƒƒã‚¯: {_global_raw_data is not None}")
            if _global_raw_data is not None:
                logger.info("ğŸ’¾ æ—¢å­˜ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å†åˆ©ç”¨ä¸­...")
                combined_df = _global_raw_data.copy()
            else:
                logger.warning("âš ï¸ _global_raw_dataã‚‚åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æ–°è¦èª­ã¿è¾¼ã¿ã‚’å®Ÿè¡Œã—ã¾ã™...")
                combined_df = load_all_data_once(analyzer.config.input_path, 'utf-8')
                if combined_df.empty:
                    return {}
        
        # ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«åˆ—ã‚’è¨ˆç®—
        logger.info("ğŸ§® å®Ÿéš›ã®CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æ­£ç¢ºã«è¨ˆç®—ä¸­...")
        df_with_features = calculate_accurate_feature_levels(combined_df)
        
        # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ä¸€æ‹¬è¨ˆç®—
        logger.info("ğŸ§® ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ç‰¹å¾´é‡ä¸€æ‹¬è¨ˆç®—ä¸­...")
        df_with_features = calculate_race_level_features_with_position_weights(df_with_features)
        
        logger.info(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†: {len(df_with_features):,}ãƒ¬ãƒ¼ã‚¹")
    
    all_results = {}
    
    # 3. æœŸé–“ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã—ã¦åˆ†æ
    for period_name, start_year, end_year in periods:
        logger.info(f"ğŸ“Š æœŸé–“ {period_name} ã®åˆ†æé–‹å§‹...")
        
        try:
            # æœŸé–“åˆ¥å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
            period_output_dir = base_output_dir / period_name
            period_output_dir.mkdir(parents=True, exist_ok=True)
            
            # ã€æœ€é©åŒ–ã€‘ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚³ãƒ”ãƒ¼ä¸è¦ï¼‰
            period_mask = (df_with_features['å¹´'] >= start_year) & (df_with_features['å¹´'] <= end_year)
            period_df = df_with_features[period_mask].copy()  # å¿…è¦ãªéƒ¨åˆ†ã®ã¿ã‚³ãƒ”ãƒ¼
            
            logger.info(f"  ğŸ“… æœŸé–“è¨­å®š: {start_year}å¹´ - {end_year}å¹´")
            logger.info(f"  ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(period_df):,}è¡Œ")
            logger.info(f"  ğŸ å¯¾è±¡é¦¬æ•°: {len(period_df['é¦¬å'].unique()):,}é ­")
            
            # æœŸé–“å†…ã®å®Ÿéš›ã®å¹´ç¯„å›²ã‚’ç¢ºèª
            if len(period_df) > 0:
                actual_min_year = int(period_df['å¹´'].min())
                actual_max_year = int(period_df['å¹´'].max())
                logger.info(f"  ğŸ“Š å®Ÿéš›ã®å¹´ç¯„å›²: {actual_min_year}å¹´ - {actual_max_year}å¹´")
            
            # ãƒ‡ãƒ¼ã‚¿å……è¶³æ€§ãƒã‚§ãƒƒã‚¯
            if len(period_df) < analyzer.config.min_races:
                logger.warning(f"æœŸé–“ {period_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— ({len(period_df)}è¡Œ)")
                continue
            
            # ã€é‡è¦ã€‘ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿è¨­å®šå®Œäº†ã§è¨­å®šã—ãŸé‡ã¿ã«çµ±ä¸€ï¼ˆå†è¨ˆç®—ã‚’é˜²ãï¼‰
            if WeightManager.is_initialized():
                logger.info(f"â™»ï¸ æœŸé–“ {period_name} ã§ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿è¨­å®šå®Œäº†ã§è¨­å®šã•ã‚ŒãŸé‡ã¿ã‚’å†åˆ©ç”¨ã—ã¾ã™")
                # é‡ã¿ã®å†è¨ˆç®—ã‚’é˜²ã
                WeightManager.prevent_recalculation()
            else:
                logger.warning(f"âš ï¸ æœŸé–“ {period_name} ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ãŒæœªåˆæœŸåŒ–ã§ã™ã€‚é‡ã¿ã‚’è¨ˆç®—ã—ã¾ã™")
                # æœ€åˆã®æœŸé–“ã§ã®ã¿é‡ã¿ã‚’è¨ˆç®—
                weights = WeightManager.initialize_from_training_data(df_with_features)
                logger.info(f"âœ… æœŸé–“ {period_name} ã§é‡ã¿è¨­å®šå®Œäº†: {weights}")
            
            # ã€é‡è¦ä¿®æ­£ã€‘æœŸé–“åˆ¥ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆã—ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å®šæœŸé–“ã‚’ç›´æ¥è¨­å®š
            period_config = AnalysisConfig(
                input_path=analyzer.config.input_path,
                min_races=analyzer.config.min_races,
                output_dir=str(period_output_dir),
                date_str=analyzer.config.date_str,
                start_date=None,  # é‡è¤‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é˜²æ­¢
                end_date=None     # é‡è¤‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é˜²æ­¢
            )
            
            period_analyzer = REQIAnalyzer(period_config, 
                                              enable_stratified_analysis=analyzer.enable_stratified_analysis)
            
            # ã€é‡è¦ä¿®æ­£ã€‘ç‰¹å¾´é‡è¨ˆç®—æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥è¨­å®šï¼ˆé‡è¤‡è¨ˆç®—å›é¿ï¼‰
            period_analyzer.df = period_df.copy()
            
            # ã€ä¿®æ­£ã€‘æœŸé–“æƒ…å ±ã‚’æ˜ç¤ºçš„ã«è¨­å®šã—ã¦æ™‚ç³»åˆ—åˆ†å‰²ã®å•é¡Œã‚’å›é¿
            period_analyzer._override_period_info = {
                'start_year': start_year,
                'end_year': end_year,
                'period_name': period_name,
                'total_years': end_year - start_year + 1
            }
            
            # åˆ†æå®Ÿè¡Œ
            logger.info(f"  ğŸ“ˆ åˆ†æå®Ÿè¡Œä¸­...")
            results = period_analyzer.analyze()
            
            # çµæœã®å¯è¦–åŒ–
            logger.info(f"  ğŸ“Š å¯è¦–åŒ–ç”Ÿæˆä¸­...")
            period_analyzer.stats = results
            period_analyzer.visualize()
            
            # æœŸé–“æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            results['period_info'] = {
                'name': period_name,
                'start_year': start_year,
                'end_year': end_year,
                'total_races': len(period_df),
                'total_horses': len(period_df['é¦¬å'].unique())
            }
            
            all_results[period_name] = results
            logger.info(f"âœ… æœŸé–“ {period_name} å®Œäº†: {results['period_info']['total_races']:,}ãƒ¬ãƒ¼ã‚¹, {results['period_info']['total_horses']:,}é ­")
            
        except Exception as e:
            logger.error(f"âŒ æœŸé–“ {period_name} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
            continue
    
    logger.info("ğŸ‰ æœ€é©åŒ–ç‰ˆæœŸé–“åˆ¥åˆ†æå®Œäº†")
    return all_results

def analyze_by_periods(analyzer, periods, base_output_dir):
    """æœŸé–“åˆ¥ã«åˆ†æã‚’å®Ÿè¡Œï¼ˆæœ€é©åŒ–ç‰ˆã‚’ä½¿ç”¨ï¼‰"""
    return analyze_by_periods_optimized(analyzer, periods, base_output_dir)


def generate_period_summary_report(all_results, output_dir):
    """æœŸé–“åˆ¥åˆ†æã®ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report_path = output_dir / 'ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ_æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## ğŸ“Š åˆ†ææœŸé–“ä¸€è¦§\n\n")
        f.write("| æœŸé–“ | å¯¾è±¡é¦¬æ•° | ç·ãƒ¬ãƒ¼ã‚¹æ•° | å¹³å‡ãƒ¬ãƒ™ãƒ«ç›¸é–¢ | æœ€é«˜ãƒ¬ãƒ™ãƒ«ç›¸é–¢ |\n")
        f.write("|------|----------|-----------|---------------|---------------|\n")
        
        for period_name, results in all_results.items():
            period_info = results.get('period_info', {})
            correlation_stats = results.get('correlation_stats', {})
            
            total_horses = period_info.get('total_horses', 0)
            total_races = period_info.get('total_races', 0)
            
            # ç›¸é–¢ä¿‚æ•°ã®å–å¾—
            corr_avg = correlation_stats.get('correlation_place_avg', 0.0)
            corr_max = correlation_stats.get('correlation_place_max', 0.0)
            
            f.write(f"| {period_name} | {total_horses:,}é ­ | {total_races:,}ãƒ¬ãƒ¼ã‚¹ | {corr_avg:.3f} | {corr_max:.3f} |\n")
        
        # å„æœŸé–“ã®è©³ç´°
        for period_name, results in all_results.items():
            f.write(f"\n## ğŸ“ˆ æœŸé–“: {period_name}\n\n")
            
            period_info = results.get('period_info', {})
            correlation_stats = results.get('correlation_stats', {})
            
            f.write(f"### åŸºæœ¬æƒ…å ±\n")
            f.write(f"- **åˆ†ææœŸé–“**: {period_info.get('start_year', 'ä¸æ˜')}å¹´ - {period_info.get('end_year', 'ä¸æ˜')}å¹´\n")
            f.write(f"- **å¯¾è±¡é¦¬æ•°**: {period_info.get('total_horses', 0):,}é ­\n")
            f.write(f"- **ç·ãƒ¬ãƒ¼ã‚¹æ•°**: {period_info.get('total_races', 0):,}ãƒ¬ãƒ¼ã‚¹\n\n")
            
            f.write(f"### ç›¸é–¢åˆ†æçµæœ\n")
            if correlation_stats:
                # å¹³å‡ãƒ¬ãƒ™ãƒ«åˆ†æ
                corr_place_avg = correlation_stats.get('correlation_place_avg', 0.0)
                r2_place_avg = correlation_stats.get('r2_place_avg', 0.0)
                
                # æœ€é«˜ãƒ¬ãƒ™ãƒ«åˆ†æ
                corr_place_max = correlation_stats.get('correlation_place_max', 0.0)
                r2_place_max = correlation_stats.get('r2_place_max', 0.0)
                
                f.write(f"**å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ vs è¤‡å‹ç‡**\n")
                f.write(f"- ç›¸é–¢ä¿‚æ•°: {corr_place_avg:.3f}\n")
                f.write(f"- æ±ºå®šä¿‚æ•° (RÂ²): {r2_place_avg:.3f}\n\n")
                
                f.write(f"**æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ vs è¤‡å‹ç‡**\n")
                f.write(f"- ç›¸é–¢ä¿‚æ•°: {corr_place_max:.3f}\n")
                f.write(f"- æ±ºå®šä¿‚æ•° (RÂ²): {r2_place_max:.3f}\n\n")
            else:
                f.write("- ç›¸é–¢åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—\n\n")
        
        f.write("\n## ğŸ’¡ ç·åˆçš„ãªå‚¾å‘ã¨çŸ¥è¦‹\n\n")
        
        # æœŸé–“åˆ¥ã®ç›¸é–¢ä¿‚æ•°å¤‰åŒ–
        if len(all_results) > 1:
            f.write("### æ™‚ç³»åˆ—å¤‰åŒ–\n")
            f.write("å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨è¤‡å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã®å¤‰åŒ–ï¼š\n")
            
            correlations_by_period = []
            for period_name, results in all_results.items():
                correlation_stats = results.get('correlation_stats', {})
                corr = correlation_stats.get('correlation_place_avg', 0.0)
                correlations_by_period.append((period_name, corr))
            
            for i, (period, corr) in enumerate(correlations_by_period):
                if i > 0:
                    prev_corr = correlations_by_period[i-1][1]
                    change = corr - prev_corr
                    trend = "ä¸Šæ˜‡" if change > 0.05 else "ä¸‹é™" if change < -0.05 else "æ¨ªã°ã„"
                    f.write(f"- {period}: {corr:.3f} ({trend})\n")
                else:
                    f.write(f"- {period}: {corr:.3f} (åŸºæº–)\n")
        
        f.write("\n### ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æã®ç‰¹å¾´\n")
        f.write("- ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¯ç«¶é¦¬å ´ã®æ ¼å¼åº¦ã¨å®ŸåŠ›ã®é–¢ä¿‚ã‚’æ•°å€¤åŒ–\n")
        f.write("- å¹³å‡ãƒ¬ãƒ™ãƒ«ï¼šé¦¬ã®ç¶™ç¶šçš„ãªå®ŸåŠ›ã‚’è¡¨ã™æŒ‡æ¨™\n")
        f.write("- æœ€é«˜ãƒ¬ãƒ™ãƒ«ï¼šé¦¬ã®ãƒ”ãƒ¼ã‚¯æ™‚ã®å®ŸåŠ›ã‚’è¡¨ã™æŒ‡æ¨™\n")
        f.write("- æ™‚ç³»åˆ—åˆ†æã«ã‚ˆã‚Šã€ç«¶é¦¬ç•Œã®æ ¼å¼ä½“ç³»ã®å¤‰åŒ–ã‚’æŠŠæ¡å¯èƒ½\n")
    
    logger.info(f"æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

@log_performance("åŒ…æ‹¬çš„ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æ")
def perform_comprehensive_odds_analysis(data_dir: str, output_dir: str, sample_size: int = None, min_races: int = 6) -> Dict[str, Any]:
    """åŒ…æ‹¬çš„ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã®å®Ÿè¡Œ"""
    logger.info("ğŸ¯ åŒ…æ‹¬çš„ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’é–‹å§‹...")
    
    try:
        # OddsComparisonAnalyzerã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        analyzer = OddsComparisonAnalyzer(min_races=min_races)
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        combined_df = load_all_data_once(data_dir, 'utf-8')
        if combined_df.empty:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
        dataset_files = get_all_dataset_files(data_dir)
        file_count = len(dataset_files)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ãŒã‚ã‚‹å ´åˆã¯é©ç”¨
        if sample_size is not None and len(combined_df) > sample_size * 1000:  # æ¦‚ç®—ã§åˆ¶é™
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã‚’é©ç”¨: {sample_size * 1000}è¡Œ")
            combined_df = combined_df.sample(n=sample_size * 1000, random_state=42)
        
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df):,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
        log_dataframe_info(combined_df, "çµ±åˆã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿")
        
        # HorseREQIè¨ˆç®—
        horse_stats_df = analyzer.calculate_horse_race_level(combined_df)
        logger.info(f"HorseREQIè¨ˆç®—å®Œäº†: {len(horse_stats_df):,}é ­")
        
        # ç›¸é–¢åˆ†æ
        correlation_results = analyzer.perform_correlation_analysis(horse_stats_df)
        
        # å›å¸°åˆ†æ
        regression_results = analyzer.perform_regression_analysis(horse_stats_df)
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        analysis_results = {
            'data_summary': {
                'total_records': len(combined_df),
                'horse_count': len(horse_stats_df),
                'file_count': file_count
            },
            'correlations': correlation_results,
            'regression': regression_results
        }
        
        # ã€ä¿®æ­£ã€‘å¯è¦–åŒ–ã®ä½œæˆ
        logger.info("ğŸ“Š å¯è¦–åŒ–ï¼ˆæ•£å¸ƒå›³ãƒ»ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼‰ã‚’ä½œæˆä¸­...")
        try:
            # ç›¸é–¢åˆ†æã¨å›å¸°åˆ†æã®çµæœã‚’çµ±åˆ
            visualization_results = {
                'correlations': correlation_results['correlations'],
                'h2_verification': regression_results.get('h2_verification', {})
            }
            analyzer.create_visualizations(horse_stats_df, visualization_results, Path(output_dir))
            logger.info("âœ… å¯è¦–åŒ–ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ å¯è¦–åŒ–ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        analyzer.generate_comprehensive_report(horse_stats_df, correlation_results, regression_results, Path(output_dir))
        
        return analysis_results
        
    except ImportError:
        # OddsComparisonAnalyzerãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ç°¡æ˜“ç‰ˆ
        logger.warning("OddsComparisonAnalyzerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ç°¡æ˜“ç‰ˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        return perform_simple_odds_analysis(data_dir, output_dir, sample_size, min_races)

def perform_simple_odds_analysis(data_dir: str, output_dir: str, sample_size: int = None, min_races: int = 6) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æ"""
    logger.info("ğŸ“Š ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ...")
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    combined_df = load_all_data_once(data_dir, 'utf-8')
    if combined_df.empty:
        raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
    dataset_files = get_all_dataset_files(data_dir)
    file_count = len(dataset_files)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ãŒã‚ã‚‹å ´åˆã¯é©ç”¨
    if sample_size is not None and len(combined_df) > sample_size * 1000:  # æ¦‚ç®—ã§åˆ¶é™
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã‚’é©ç”¨: {sample_size * 1000}è¡Œ")
        combined_df = combined_df.sample(n=sample_size * 1000, random_state=42)
    
    logger.info("ğŸ”— ç°¡æ˜“ç‰ˆãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
    logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df):,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
    log_dataframe_info(combined_df, "ç°¡æ˜“ç‰ˆçµ±åˆãƒ‡ãƒ¼ã‚¿")
    
    # åŸºæœ¬çš„ãªé¦¬çµ±è¨ˆè¨ˆç®—
    horse_stats = calculate_simple_horse_statistics(combined_df, min_races)
    logger.info(f"é¦¬çµ±è¨ˆè¨ˆç®—å®Œäº†: {len(horse_stats):,}é ­")
    
    # ç›¸é–¢åˆ†æ
    correlations = perform_simple_correlation_analysis(horse_stats)
    
    # å›å¸°åˆ†æ
    regression = perform_simple_regression_analysis(horse_stats)
    
    # çµæœ
    analysis_results = {
        'data_summary': {
            'total_records': len(combined_df),
            'horse_count': len(horse_stats),
            'file_count': file_count
        },
        'correlations': correlations,
        'regression': regression
    }
    
    # ã€è¿½åŠ ã€‘ç°¡æ˜“ç‰ˆã§ã‚‚å¯è¦–åŒ–ã‚’ä½œæˆ
    logger.info("ğŸ“Š ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
    try:
        create_simple_visualizations(horse_stats, correlations, regression, Path(output_dir))
        logger.info("âœ… ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_simple_report(analysis_results, Path(output_dir))
    
    return analysis_results

@log_performance("ç°¡æ˜“é¦¬çµ±è¨ˆè¨ˆç®—")
def calculate_simple_horse_statistics(df: pd.DataFrame, min_races: int = 6) -> pd.DataFrame:
    """ç°¡æ˜“ç‰ˆé¦¬çµ±è¨ˆè¨ˆç®—ï¼ˆå±¤åˆ¥åˆ†æã¨çµ±ä¸€ã—ãŸREQIè¨ˆç®—æ–¹æ³•ã‚’é©ç”¨ï¼‰"""
    # å¿…è¦ã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['é¦¬å', 'ç€é †']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
    
    # ã€çµ±ä¸€ã€‘èŠãƒ¬ãƒ¼ã‚¹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå±¤åˆ¥åˆ†æã¨çµ±ä¸€ï¼‰
    if 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰' in df.columns:
        original_count = len(df)
        df = df[df['èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰'] == 'èŠ']
        logger.info(f"ğŸ“Š èŠãƒ¬ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿: {original_count:,} â†’ {len(df):,}è¡Œ")
    
    # æ•°å€¤å¤‰æ›
    df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
    df = df[df['ç€é †'] > 0]
    
    # ã‚ªãƒƒã‚ºæƒ…å ±ã®å‡¦ç†
    if 'ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º' in df.columns:
        df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'] = pd.to_numeric(df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'], errors='coerce')
        df = df[df['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'] > 0]
    
    if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in df.columns:
        df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'] = pd.to_numeric(df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'], errors='coerce')
        df = df[df['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'] > 0]
    
    # ã€çµ±ä¸€ã€‘å±¤åˆ¥åˆ†æã¨åŒã˜ç€é †é‡ã¿ä»˜ãREQIè¨ˆç®—ã‚’é©ç”¨
    logger.info("ğŸ“Š å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã—ãŸç€é †é‡ã¿ä»˜ãREQIè¨ˆç®—ã‚’é©ç”¨ä¸­...")
    df_with_reqi = calculate_race_level_features_with_position_weights(df)
    
    # ã€é«˜é€ŸåŒ–ã€‘pandas groupbyã‚’ä½¿ç”¨ã—ã¦O(nÂ²)ã‚’O(n)ã«æ”¹å–„
    logger.info("ğŸš€ é«˜é€ŸåŒ–ç‰ˆé¦¬çµ±è¨ˆè¨ˆç®—ã‚’å®Ÿè¡Œä¸­ï¼ˆpandas groupbyä½¿ç”¨ï¼‰...")
    stats_calc_start = time.time()
    
    # é¦¬ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦çµ±è¨ˆã‚’ä¸€æ‹¬è¨ˆç®—
    horse_groups = df_with_reqi.groupby('é¦¬å')
    
    # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
    basic_stats = horse_groups.agg({
        'ç€é †': ['count', lambda x: (x == 1).mean(), lambda x: (x <= 3).mean()],
        'race_level': ['mean', 'max']
    }).round(6)
    
    # åˆ—åã‚’æ•´ç†
    basic_stats.columns = ['total_races', 'win_rate', 'place_rate', 'avg_race_level', 'max_race_level']
    
    # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹äºˆæ¸¬ç¢ºç‡ã®è¨ˆç®—ï¼ˆåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    odds_stats = pd.DataFrame(index=basic_stats.index)
    
    if 'ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º' in df_with_reqi.columns:
        odds_stats['avg_win_prob_from_odds'] = horse_groups['ç¢ºå®šå˜å‹ã‚ªãƒƒã‚º'].apply(
            lambda x: (1 / x).mean() if len(x) > 0 else 0
        )
    else:
        odds_stats['avg_win_prob_from_odds'] = 0
    
    if 'ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹' in df_with_reqi.columns:
        odds_stats['avg_place_prob_from_odds'] = horse_groups['ç¢ºå®šè¤‡å‹ã‚ªãƒƒã‚ºä¸‹'].apply(
            lambda x: (1 / x).mean() if len(x) > 0 else 0
        )
    else:
        odds_stats['avg_place_prob_from_odds'] = 0
    
    # çµ±è¨ˆã‚’çµåˆ
    horse_stats_df = pd.concat([basic_stats, odds_stats], axis=1)
    
    # æœ€ä½å‡ºèµ°æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿
    horse_stats_df = horse_stats_df[horse_stats_df['total_races'] >= min_races]
    
    # é¦¬åã‚’åˆ—ã«è¿½åŠ 
    horse_stats_df['horse_name'] = horse_stats_df.index
    
    # åˆ—ã®é †åºã‚’æ•´ç†
    horse_stats_df = horse_stats_df[['horse_name', 'total_races', 'win_rate', 'place_rate', 
                                   'avg_win_prob_from_odds', 'avg_place_prob_from_odds',
                                   'avg_race_level', 'max_race_level']]
    
    stats_time = time.time() - stats_calc_start
    logger.info(f"âœ… é«˜é€ŸåŒ–ç‰ˆé¦¬çµ±è¨ˆè¨ˆç®—å®Œäº†: {len(horse_stats_df):,}é ­ ({stats_time:.2f}ç§’)")
    
    return horse_stats_df.set_index('horse_name')

def perform_simple_correlation_analysis(horse_stats: pd.DataFrame) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆç›¸é–¢åˆ†æï¼ˆå±¤åˆ¥åˆ†æã¨çµ±ä¸€ã—ãŸREQIæŒ‡æ¨™ã‚’ä½¿ç”¨ï¼‰"""
    correlations = {}
    target = 'place_rate'
    
    # ã€çµ±ä¸€ã€‘å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã—ãŸREQIæŒ‡æ¨™ã‚’ä½¿ç”¨
    variables = {
        'å¹³å‡REQI': 'avg_race_level',  # å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã®æŒ‡æ¨™
        'æœ€é«˜REQI': 'max_race_level',  # å±¤åˆ¥åˆ†æã¨çµ±ä¸€ã®æŒ‡æ¨™
        'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬': 'avg_place_prob_from_odds',
        'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬': 'avg_win_prob_from_odds'
    }
    
    for name, var in variables.items():
        if var in horse_stats.columns:
            corr, p_value = pearsonr(horse_stats[var].fillna(0), horse_stats[target].fillna(0))
            correlations[name] = {
                'correlation': corr,
                'r_squared': corr ** 2,
                'p_value': p_value
            }
            logger.info(f"ğŸ“Š ç›¸é–¢åˆ†æ: {name} r={corr:.3f}, RÂ²={corr**2:.3f}, p={p_value:.3e}")
    
    return correlations

def perform_simple_regression_analysis(horse_stats: pd.DataFrame) -> Dict[str, Any]:
    """ç°¡æ˜“ç‰ˆå›å¸°åˆ†æ"""
    data = horse_stats.dropna().copy()
    if len(data) < 30:
        logger.warning("å›å¸°åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
        return {}
    
    y = data['place_rate'].values
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    split_idx = int(len(data) * 0.7)
    
    results = {}
    
    # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
    if 'avg_place_prob_from_odds' in data.columns:
        X_odds = data[['avg_place_prob_from_odds']].fillna(0).values
        X_odds_train, X_odds_test = X_odds[:split_idx], X_odds[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        model_odds = LinearRegression()
        model_odds.fit(X_odds_train, y_train)
        y_pred_odds = model_odds.predict(X_odds_test)
        
        results['odds_baseline'] = {
            'train_r2': model_odds.score(X_odds_train, y_train),
            'test_r2': r2_score(y_test, y_pred_odds),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_odds))
        }
    
    # ã€ä¿®æ­£ã€‘REQIï¼ˆå¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ï¼‰
    if 'avg_race_level' in data.columns:
        X_level = data[['avg_race_level']].fillna(0).values
        X_level_train, X_level_test = X_level[:split_idx], X_level[split_idx:]
        
        model_level = LinearRegression()
        model_level.fit(X_level_train, y_train)
        y_pred_level = model_level.predict(X_level_test)
        
        results['reqi_model'] = {
            'train_r2': model_level.score(X_level_train, y_train),
            'test_r2': r2_score(y_test, y_pred_level),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_level))
        }
    
    # ã€ä¿®æ­£ã€‘çµ±è¨ˆçš„æ¤œå®šã‚’å«ã‚€H2ä»®èª¬æ¤œè¨¼
    if 'odds_baseline' in results and 'reqi_model' in results:
        # åŸºæœ¬çš„ãªæ•°å€¤æ¯”è¼ƒ
        h2_supported = results['reqi_model']['test_r2'] > results['odds_baseline']['test_r2']
        improvement = results['reqi_model']['test_r2'] - results['odds_baseline']['test_r2']
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“è©•ä¾¡ï¼ˆæ”¹å–„å¹…ãŒ0.01ä»¥ä¸Šã‹ã¤æ­£ã®å€¤ï¼‰
        statistically_meaningful = improvement > 0.01 and h2_supported
        
        results['h2_verification'] = {
            'hypothesis_supported': h2_supported,
            'improvement': improvement,
            'statistically_meaningful': statistically_meaningful,
            'warning': 'æœ¬åˆ†æã¯ç°¡æ˜“ç‰ˆã§ã™ã€‚å³å¯†ãªçµ±è¨ˆçš„æ¤œå®šã«ã¯OddsComparisonAnalyzerã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚'
        }
    
    return results

def create_simple_visualizations(horse_stats: pd.DataFrame, correlations: Dict[str, Any], 
                                regression: Dict[str, Any], output_dir: Path):
    """ç°¡æ˜“ç‰ˆã‚ªãƒƒã‚ºåˆ†æã®å¯è¦–åŒ–ä½œæˆ"""
    try:
        # matplotlibã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®š
        import matplotlib
        matplotlib.use('Agg')  # GUIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’é¿ã‘ã‚‹
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # çµ±ä¸€ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’é©ç”¨
        from horse_racing.utils.font_config import setup_japanese_fonts
        setup_japanese_fonts(suppress_warnings=True)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        viz_dir = output_dir / "odds_comparison"
        viz_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ ç°¡æ˜“ç‰ˆå¯è¦–åŒ–å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {viz_dir}")
        
        # 1. ç›¸é–¢æ•£å¸ƒå›³
        logger.info("ğŸ“Š ç°¡æ˜“ç‰ˆç›¸é–¢æ•£å¸ƒå›³ã‚’ä½œæˆä¸­...")
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®ç›¸é–¢åˆ†æ', fontsize=16, fontweight='bold')
        
        # ã€ä¿®æ­£ã€‘å¹³å‡REQI vs è¤‡å‹ç‡
        if 'avg_race_level' in horse_stats.columns and 'place_rate' in horse_stats.columns:
            axes[0, 0].scatter(horse_stats['avg_race_level'], horse_stats['place_rate'], alpha=0.6, s=20, color='blue')
            axes[0, 0].set_xlabel('å¹³å‡REQI')
            axes[0, 0].set_ylabel('è¤‡å‹ç‡')
            
            # ç›¸é–¢ä¿‚æ•°ã‚’å–å¾—
            reqi_corr = correlations.get('å¹³å‡REQI', {}).get('correlation', 0)
            axes[0, 0].set_title(f'å¹³å‡REQI vs è¤‡å‹ç‡ (r={reqi_corr:.3f})')
        
        # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬ vs è¤‡å‹ç‡
        if 'avg_place_prob_from_odds' in horse_stats.columns and 'place_rate' in horse_stats.columns:
            axes[0, 1].scatter(horse_stats['avg_place_prob_from_odds'], horse_stats['place_rate'], alpha=0.6, s=20, color='green')
            axes[0, 1].set_xlabel('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬')
            axes[0, 1].set_ylabel('è¤‡å‹ç‡')
            
            odds_place_corr = correlations.get('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬', {}).get('correlation', 0)
            axes[0, 1].set_title(f'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹è¤‡å‹äºˆæ¸¬ vs è¤‡å‹ç‡ (r={odds_place_corr:.3f})')
        
        # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬ vs è¤‡å‹ç‡
        if 'avg_win_prob_from_odds' in horse_stats.columns and 'place_rate' in horse_stats.columns:
            axes[1, 0].scatter(horse_stats['avg_win_prob_from_odds'], horse_stats['place_rate'], alpha=0.6, s=20, color='orange')
            axes[1, 0].set_xlabel('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬')
            axes[1, 0].set_ylabel('è¤‡å‹ç‡')
            
            odds_win_corr = correlations.get('ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬', {}).get('correlation', 0)
            axes[1, 0].set_title(f'ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹å‹ç‡äºˆæ¸¬ vs è¤‡å‹ç‡ (r={odds_win_corr:.3f})')
        
        # ç©ºã®4ç•ªç›®ã®ãƒ—ãƒ­ãƒƒãƒˆ
        axes[1, 1].text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«\nçµ±è¨ˆæƒ…å ±', ha='center', va='center', fontsize=14)
        axes[1, 1].text(0.5, 0.3, f'åˆ†æå¯¾è±¡: {len(horse_stats):,}é ­', ha='center', va='center', fontsize=12)
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].set_title('åˆ†ææ¦‚è¦')
        
        plt.tight_layout()
        scatter_plot_path = viz_dir / 'correlation_scatter_plots.png'
        plt.savefig(scatter_plot_path, dpi=300, bbox_inches='tight',
                   facecolor='white', edgecolor='none',
                   format='png', pad_inches=0.1)
        plt.close()
        logger.info(f"âœ… ç›¸é–¢æ•£å¸ƒå›³ã‚’ä¿å­˜: {scatter_plot_path}")
        
        # 2. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆH2ä»®èª¬æ¤œè¨¼ï¼‰
        if regression and 'h2_verification' in regression:
            logger.info("ğŸ“Š H2ä»®èª¬æ¤œè¨¼ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆä¸­...")
            h2_results = regression['h2_verification']
            
            model_names = ['ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³', 'å¹³å‡REQI']
            r2_scores = [
                regression.get('odds_baseline', {}).get('test_r2', 0),
                regression.get('reqi_model', {}).get('test_r2', 0)
            ]
            
            plt.figure(figsize=(10, 6))
            bars = plt.bar(model_names, r2_scores, color=['#ff7f0e', '#2ca02c'])
            plt.ylabel('RÂ² (æ±ºå®šä¿‚æ•°)')
            plt.title('H2ä»®èª¬æ¤œè¨¼: å¹³å‡REQI ã®äºˆæ¸¬æ€§èƒ½')
            plt.ylim(0, max(r2_scores) * 1.2 if max(r2_scores) > 0 else 1)
            
            # æ•°å€¤ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
            for bar, score in zip(bars, r2_scores):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(r2_scores)*0.01,
                        f'{score:.4f}', ha='center', va='bottom', fontweight='bold')
            
            # H2ä»®èª¬çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º
            if h2_results.get('hypothesis_supported', False):
                result_text = f"âœ… H2ä»®èª¬ã‚µãƒãƒ¼ãƒˆ\næ”¹å–„: {h2_results.get('improvement', 0):+.4f}"
                plt.text(0.7, max(r2_scores) * 0.8, result_text, fontsize=12, 
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
            else:
                result_text = f"âŒ H2ä»®èª¬éã‚µãƒãƒ¼ãƒˆ\næ”¹å–„: {h2_results.get('improvement', 0):+.4f}"
                plt.text(0.7, max(r2_scores) * 0.8, result_text, fontsize=12,
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral"))
            
            plt.tight_layout()
            performance_plot_path = viz_dir / 'model_performance_comparison.png'
            plt.savefig(performance_plot_path, dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none',
                       format='png', pad_inches=0.1)
            plt.close()
            logger.info(f"âœ… H2ä»®èª¬æ¤œè¨¼ãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜: {performance_plot_path}")
        
        # ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        created_files = list(viz_dir.glob("*.png"))
        if created_files:
            logger.info("ğŸ“ ä½œæˆã•ã‚ŒãŸç°¡æ˜“ç‰ˆå¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«:")
            for file_path in created_files:
                logger.info(f"   - {file_path.name}")
        
    except ImportError as e:
        logger.error(f"âŒ matplotlib/seabornã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.info("å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    except Exception as e:
        logger.error(f"âŒ ç°¡æ˜“ç‰ˆå¯è¦–åŒ–ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚‚ç¢ºå®Ÿã«figureã‚’é–‰ã˜ã‚‹
        try:
            plt.close('all')
        except:
            pass

def generate_simple_report(results: Dict[str, Any], output_dir: Path):
    """ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    output_dir.mkdir(parents=True, exist_ok=True)
    report_path = output_dir / "horse_REQI_odds_analysis_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: analyze_horse_REQI.py\n\n")
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
        if 'data_summary' in results:
            f.write("## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
            summary = results['data_summary']
            f.write(f"- **ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: {summary.get('total_records', 'N/A'):,}\n")
            f.write(f"- **åˆ†æå¯¾è±¡é¦¬æ•°**: {summary.get('horse_count', 'N/A'):,}\n")
            f.write(f"- **å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {summary.get('file_count', 'N/A')}\n\n")
        
        # ç›¸é–¢åˆ†æçµæœ
        if 'correlations' in results:
            f.write("## ç›¸é–¢åˆ†æçµæœ\n\n")
            f.write("| å¤‰æ•° | ç›¸é–¢ä¿‚æ•° | RÂ² | på€¤ |\n")
            f.write("|------|----------|----|---------|\n")
            
            for name, corr in results['correlations'].items():
                f.write(f"| {name} | {corr['correlation']:.3f} | {corr['r_squared']:.3f} | {corr['p_value']:.3e} |\n")
            f.write("\n")
        
        # å›å¸°åˆ†æçµæœ
        if 'regression' in results:
            f.write("## å›å¸°åˆ†æçµæœï¼ˆH2ä»®èª¬æ¤œè¨¼ï¼‰\n\n")
            regression = results['regression']
            
            f.write("| ãƒ¢ãƒ‡ãƒ« | è¨“ç·´RÂ² | æ¤œè¨¼RÂ² | RMSE |\n")
            f.write("|--------|---------|---------|-------|\n")
            
            if 'odds_baseline' in regression:
                model = regression['odds_baseline']
                f.write(f"| ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | {model.get('train_r2', 0):.4f} | {model.get('test_r2', 0):.4f} | {model.get('test_rmse', 0):.4f} |\n")
            
            if 'reqi_model' in regression:
                model = regression['reqi_model']
                f.write(f"| å¹³å‡REQI | {model.get('train_r2', 0):.4f} | {model.get('test_r2', 0):.4f} | {model.get('test_rmse', 0):.4f} |\n")
            
            # H2ä»®èª¬çµæœ
            if 'h2_verification' in regression:
                h2 = regression['h2_verification']
                f.write(f"\n### H2ä»®èª¬æ¤œè¨¼çµæœï¼ˆç°¡æ˜“ç‰ˆï¼‰\n\n")
                f.write(f"- **ä»®èª¬ã‚µãƒãƒ¼ãƒˆ**: {'âœ“ YES' if h2['hypothesis_supported'] else 'âœ— NO'}\n")
                f.write(f"- **æ€§èƒ½æ”¹å–„**: {h2['improvement']:+.4f}\n")
                f.write(f"- **çµ±è¨ˆçš„æ„å‘³**: {'âœ“ æœ‰æ„' if h2.get('statistically_meaningful', False) else 'âœ— é™å®šçš„'}\n")
                if 'warning' in h2:
                    f.write(f"- **æ³¨æ„**: {h2['warning']}\n")
                f.write("\n")
        
        f.write("## çµè«–\n\n")
        f.write("å¹³å‡REQIï¼ˆç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼‰ã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®æ¯”è¼ƒåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n")
        f.write("ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å›ºå®šé‡ã¿æ³•ã‚’é©ç”¨ã—ãŸæ­£ç¢ºãªREQIè¨ˆç®—ã«ã‚ˆã‚Šã€çµ±è¨ˆçš„å¦¥å½“æ€§ã‚’ç¢ºä¿ã—ã¾ã—ãŸã€‚\n")
    
    logger.info(f"ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ: {report_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆçµ±åˆç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æ
  python analyze_horse_REQI.py --odds-analysis export/dataset --output-dir results/reqi_odds

  # å¾“æ¥ã®ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ
  python analyze_horse_REQI.py export/with_bias --output-dir results/race_level_analysis

  # å±¤åˆ¥åˆ†æã®ã¿å®Ÿè¡Œ
  python analyze_horse_REQI.py --stratified-only --output-dir results/stratified_analysis

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä¸»è¦æ©Ÿèƒ½:
  1. ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºæƒ…å ±ã®åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æ
  2. H2ä»®èª¬ã€ŒREQIãŒã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€ã®æ¤œè¨¼
  3. ç›¸é–¢åˆ†æã¨å›å¸°åˆ†æã«ã‚ˆã‚‹çµ±è¨ˆçš„è©•ä¾¡
  4. å±¤åˆ¥åˆ†æï¼ˆå¹´é½¢å±¤ãƒ»çµŒé¨“æ•°ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰
  5. æœŸé–“åˆ¥åˆ†æï¼ˆ3å¹´é–“éš”ã§ã®æ™‚ç³»åˆ—åˆ†æï¼‰
        """
    )
    parser.add_argument('input_path', nargs='?', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ (ä¾‹: export/with_bias)')
    parser.add_argument('--output-dir', default='results/race_level_analysis', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹')
    parser.add_argument('--min-races', type=int, default=6, help='åˆ†æå¯¾è±¡ã¨ã™ã‚‹æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°')
    parser.add_argument('--encoding', default='utf-8', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°')
    parser.add_argument('--start-date', help='åˆ†æé–‹å§‹æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰')
    parser.add_argument('--end-date', help='åˆ†æçµ‚äº†æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰')
    
    # æ–°æ©Ÿèƒ½ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--odds-analysis', metavar='DATA_DIR', help='ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šï¼‰')
    parser.add_argument('--sample-size', type=int, default=None, help='ã‚ªãƒƒã‚ºåˆ†æã§ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰')
    
    # å¾“æ¥ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆç¶™ç¶šï¼‰
    parser.add_argument('--three-year-periods', action='store_true',
                       help='3å¹´é–“éš”ã§ã®æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨æœŸé–“åˆ†æï¼‰')
    parser.add_argument('--enable-stratified-analysis', action='store_true', default=True,
                       help='å±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œï¼ˆå¹´é½¢å±¤åˆ¥ã€çµŒé¨“æ•°åˆ¥ã€è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹')
    parser.add_argument('--disable-stratified-analysis', action='store_true',
                       help='å±¤åˆ¥åˆ†æã‚’ç„¡åŠ¹åŒ–ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ç”¨ï¼‰')
    parser.add_argument('--stratified-only', action='store_true',
                       help='å±¤åˆ¥åˆ†æã®ã¿ã‚’å®Ÿè¡Œï¼ˆexport/datasetã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ï¼‰')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š')
    parser.add_argument('--log-file', help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰')
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ•°ã®åˆæœŸåŒ–
    log_file = None
    
    try:
        args = parser.parse_args()
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ç”Ÿæˆï¼ˆargså–å¾—å¾Œã€validate_argså‰ã«å®Ÿè¡Œï¼‰
        log_file = args.log_file
        if log_file is None:
            # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆoutput_dir/logsé…ä¸‹ã«çµ±ä¸€ï¼‰
            # argsã¯æ—¢ã«å–å¾—æ¸ˆã¿ãªã®ã§ã€output_diré…ä¸‹ã«å‡ºåŠ›
            out_dir = Path(getattr(args, 'output_dir', 'results'))
            log_dir = out_dir / 'logs'
            log_dir.mkdir(parents=True, exist_ok=True)
            log_file = str(log_dir / f'analyze_horse_REQI_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        
        # ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–
        setup_logging(log_level=args.log_level, log_file=log_file)
        
        # å¼•æ•°æ¤œè¨¼ï¼ˆãƒ­ã‚°è¨­å®šå¾Œã«å®Ÿè¡Œã€ã‚ªãƒƒã‚ºåˆ†æãƒ»å±¤åˆ¥åˆ†æã®ã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if not args.odds_analysis and not args.stratified_only:
            args = validate_args(args)

        # ğŸ“‹ race_level_analysis_report.mdæº–æ‹ ã®å‡¦ç†é–‹å§‹è¡¨ç¤º
        print("\n" + "="*80)
        print("ğŸ ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹: race_level_analysis_report.mdæº–æ‹ ")
        print("="*80)
        print("ğŸ“– å‚ç…§ãƒ¬ãƒãƒ¼ãƒˆ: race_level_analysis_report.md")
        print("ğŸ¯ REQIè¨ˆç®—æ–¹å¼: å‹•çš„é‡ã¿è¨ˆç®—æ³•ï¼ˆæ¯å›ç›¸é–¢åˆ†æã§ç®—å‡ºï¼‰")
        print("ğŸ“Š é‡ã¿ç®—å‡º: w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)")
        print("ğŸ”¬ çµ±è¨ˆçš„æ ¹æ‹ : å®Ÿæ¸¬ç›¸é–¢ä¿‚æ•°ã®2ä¹—å€¤æ­£è¦åŒ–")
        print("â³ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ä¸­...")
        print("="*80 + "\n")
        
        # ğŸ¯ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ï¼ˆã‚ªãƒƒã‚ºåˆ†ææ™‚ã®ã¿å®Ÿè¡Œï¼‰
        if args.odds_analysis:
            try:
                weights_initialized = initialize_global_weights(args)
                if weights_initialized:
                    logger.info("âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–å®Œäº†")
                else:
                    logger.warning("âš ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ã«å¤±æ•—ã€å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å€‹åˆ¥è¨ˆç®—")
            except Exception as e:
                logger.error(f"âŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.warning("âš ï¸ å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å€‹åˆ¥é‡ã¿è¨ˆç®—ã‚’å®Ÿè¡Œã—ã¾ã™")
        else:
            logger.info("ğŸ“Š æœŸé–“åˆ¥åˆ†æãƒ¢ãƒ¼ãƒ‰: é‡ã¿åˆæœŸåŒ–ã¯å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å®Ÿè¡Œ")

        # ãƒ­ã‚°è¨­å®šå®Œäº†å¾Œã«é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›
        logger.info("ğŸ‡ ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        logger.info(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
        logger.info(f"ğŸ–¥ï¸ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {args.log_level}")
        logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        
        # åˆæœŸã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
        log_system_resources()

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚å«ã‚ã¦ç¢ºå®Ÿã«ä½œæˆï¼‰
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæ›¸ãè¾¼ã¿å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if not output_dir.exists() or not output_dir.is_dir():
            raise FileNotFoundError(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {output_dir}")
        
        logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªæ¸ˆã¿: {output_dir.absolute()}")

        logger.info(f"ğŸ“ å…¥åŠ›ãƒ‘ã‚¹: {args.input_path}")
        logger.info(f"ğŸ“Š å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
        logger.info(f"ğŸ¯ æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°: {args.min_races}")
        if args.start_date:
            logger.info(f"ğŸ“… åˆ†æé–‹å§‹æ—¥: {args.start_date}")
        if args.end_date:
            logger.info(f"ğŸ“… åˆ†æçµ‚äº†æ—¥: {args.end_date}")
        
        # å±¤åˆ¥åˆ†æè¨­å®šã®å‡¦ç†
        enable_stratified = args.enable_stratified_analysis and not args.disable_stratified_analysis
        if enable_stratified:
            logger.info(f"ğŸ“Š å±¤åˆ¥åˆ†æ: æœ‰åŠ¹ï¼ˆå¹´é½¢å±¤åˆ¥ãƒ»çµŒé¨“æ•°åˆ¥ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰")
        else:
            logger.info(f"ğŸ“Š å±¤åˆ¥åˆ†æ: ç„¡åŠ¹ï¼ˆ--disable-stratified-analysisã§ç„¡åŠ¹åŒ–ï¼‰")
        
        # ã‚ªãƒƒã‚ºåˆ†æã®å ´åˆ
        if args.odds_analysis:
            logger.info("ğŸ¯ ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã¨ã‚ªãƒƒã‚ºã®æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                # çµ±ä¸€åˆ†æå™¨ã‚’ä½¿ç”¨
                from horse_racing.base.unified_analyzer import create_unified_analyzer
                analyzer = create_unified_analyzer('odds', args.min_races, enable_stratified)
                
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                df = analyzer.load_data_unified(args.odds_analysis, args.encoding)
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ï¼ˆã‚ªãƒƒã‚ºåˆ†ææ™‚ã®ã¿ï¼‰
                if not WeightManager.is_initialized():
                    analyzer.initialize_global_weights(df)
                else:
                    logger.info("âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã¯æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã§ã™")
                
                # å‰å‡¦ç†
                df = analyzer.preprocess_data_unified(df)
                
                # åˆ†æå®Ÿè¡Œ
                results = analyzer.analyze(df)
                
                logger.info("âœ… ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡: {results['data_summary']['total_records']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰, {results['data_summary']['horse_count']:,}é ­")
                logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {args.output_dir}")
                
                # H2ä»®èª¬çµæœã®è¡¨ç¤º
                if 'regression' in results and 'h2_verification' in results['regression']:
                    h2 = results['regression']['h2_verification']
                    result_text = "ã‚µãƒãƒ¼ãƒˆ" if h2.get('h2_hypothesis_supported', h2.get('hypothesis_supported', False)) else "éã‚µãƒãƒ¼ãƒˆ"
                    logger.info(f"ğŸ¯ H2ä»®èª¬ã€Œç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ãŒã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€: {result_text}")
                    improvement = h2.get('r2_improvement', h2.get('improvement', 0))
                    logger.info(f"   æ€§èƒ½æ”¹å–„: {improvement:+.4f}")
                
                # ã€å¼·åˆ¶å‡ºåŠ›ã€‘ã‚ªãƒƒã‚ºæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’å¿…ãšç”Ÿæˆï¼ˆåŒ…æ‹¬ç‰ˆãŒå¤±æ•—ã—ã¦ã‚‚ç°¡æ˜“ç‰ˆã‚’å‡ºåŠ›ï¼‰
                try:
                    logger.info("ğŸ“‹ ã‚ªãƒƒã‚ºæ¯”è¼ƒã®ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼‰...")
                    _ = perform_simple_odds_analysis(args.odds_analysis, args.output_dir, sample_size=None, min_races=args.min_races)
                    logger.info("âœ… ç°¡æ˜“ã‚ªãƒƒã‚ºæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: horse_REQI_odds_analysis_report.md")
                except Exception as e:
                    logger.error(f"âŒ ç°¡æ˜“ã‚ªãƒƒã‚ºæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)

                # ã€è¿½åŠ ã€‘ã‚ªãƒƒã‚ºåˆ†æãƒ¢ãƒ¼ãƒ‰ã§ã‚‚å±¤åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
                try:
                    logger.info("ğŸ“‹ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
                    stratified_dataset = create_stratified_dataset_from_export('export/dataset')
                    stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
                    _ = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
                    logger.info("âœ… çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
                except Exception as e:
                    logger.error(f"âŒ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                    logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                
                return 0
            except Exception as e:
                logger.error(f"âŒ ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1
        
        # å±¤åˆ¥åˆ†æã®ã¿ã®å ´åˆ
        if args.stratified_only:
            logger.info("ğŸ“Š å±¤åˆ¥åˆ†æã®ã¿ã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                stratified_dataset = create_stratified_dataset_from_export('export/dataset')
                stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
                _ = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
                logger.info("âœ… å±¤åˆ¥åˆ†æã®ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(stratified_dataset):,}é ­")
                logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
                return 0
            except Exception as e:
                logger.error(f"âŒ å±¤åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1

        if args.three_year_periods:
            logger.info("ğŸ“Š 3å¹´é–“éš”ã§ã®æœŸé–“åˆ¥åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            try:
                # çµ±ä¸€åˆ†æå™¨ã‚’ä½¿ç”¨
                from horse_racing.base.unified_analyzer import create_unified_analyzer
                analyzer = create_unified_analyzer('period', args.min_races, enable_stratified)
                
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                df = analyzer.load_data_unified(args.input_path, args.encoding)
                
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿åˆæœŸåŒ–ï¼ˆæœŸé–“åˆ¥åˆ†ææ™‚ã¯é‡è¤‡å®Ÿè¡Œã‚’å›é¿ï¼‰
                if not WeightManager.is_initialized():
                    analyzer.initialize_global_weights(df)
                else:
                    logger.info("âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã¯æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã§ã™")
                
                # å‰å‡¦ç†
                df = analyzer.preprocess_data_unified(df)
                
                logger.info(f"ğŸ“Š èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶")
            
            # å¹´ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if 'å¹´' in df.columns and df['å¹´'].notna().any():
                    min_year = int(df['å¹´'].min())
                    max_year = int(df['å¹´'].max())
                    logger.info(f"ğŸ“Š å¹´ãƒ‡ãƒ¼ã‚¿ç¯„å›²: {min_year}å¹´ - {max_year}å¹´")
                
                    # æœŸé–“åˆ¥åˆ†æå®Ÿè¡Œ
                    results = analyzer.analyze(df)
                    
                    if results:
                        logger.info(f"ğŸ“Š æœŸé–“åˆ¥åˆ†æå®Œäº†: {len(results)}æœŸé–“")
                        
                        # æœŸé–“åˆ¥åˆ†æã®ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
                        logger.info("ğŸ“‹ æœŸé–“åˆ¥åˆ†æã®ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
                        try:
                            generate_period_summary_report(results, Path(args.output_dir))
                            logger.info("âœ… æœŸé–“åˆ¥åˆ†æç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
                        except Exception as e:
                            logger.error(f"âŒ ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                        
                        # ã€è¿½åŠ ã€‘çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚‚ç”Ÿæˆ
                        try:
                            logger.info("ğŸ“‹ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
                            stratified_dataset = create_stratified_dataset_from_export('export/dataset')
                            stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
                            _ = generate_stratified_report(stratified_results, stratified_dataset, Path(args.output_dir))
                            logger.info("âœ… çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
                        except Exception as e:
                            logger.error(f"âŒ çµ±åˆå±¤åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                    
                        # çµæœã®ä¿å­˜å…ˆã‚’è¡¨ç¤º
                        logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {args.output_dir}")
                        logger.info(f"ğŸ“‹ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ: {args.output_dir}/ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰åˆ†æ_æœŸé–“åˆ¥ç·åˆãƒ¬ãƒãƒ¼ãƒˆ.md")
                        logger.info(f"ğŸ“‹ å±¤åˆ¥ãƒ¬ãƒãƒ¼ãƒˆ: {args.output_dir}/stratified_analysis_integrated_report.md")
                        
                        return 0
                    else:
                        logger.warning("âš ï¸ æœ‰åŠ¹ãªæœŸé–“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        return 1
                else:
                    logger.warning("âš ï¸ å¹´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return 1
                    
            except Exception as e:
                logger.error(f"âŒ æœŸé–“åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
                return 1
        
        if not args.three_year_periods:
            logger.info("ğŸ“Š ã€ä¿®æ­£ç‰ˆã€‘å³å¯†ãªæ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            
            # è¨­å®šã®ä½œæˆ
            date_str = datetime.now().strftime('%Y%m%d')
            config = AnalysisConfig(
                input_path=args.input_path,
                min_races=args.min_races,
                output_dir=str(output_dir),
                date_str=date_str,
                start_date=args.start_date,
                end_date=args.end_date
            )

            # 1. REQIAnalyzerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
            analyzer = REQIAnalyzer(config, enable_stratified)

            # 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            logger.info("ğŸ“– å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            analyzer.df = analyzer.load_data()
            log_dataframe_info(analyzer.df, "èª­ã¿è¾¼ã¿å®Œäº†ãƒ‡ãƒ¼ã‚¿")

            # å‰å‡¦ç†ã‚’è¿½åŠ 
            logger.info("ğŸ”§ å‰å‡¦ç†ä¸­...")
            analyzer.df = analyzer.preprocess_data()
            log_dataframe_info(analyzer.df, "å‰å‡¦ç†å®Œäº†ãƒ‡ãƒ¼ã‚¿")
            
            # 3. ç‰¹å¾´é‡è¨ˆç®—
            logger.info("ğŸ§® ç‰¹å¾´é‡è¨ˆç®—ä¸­...")
            analyzer.df = analyzer.calculate_feature()
            log_dataframe_info(analyzer.df, "ç‰¹å¾´é‡è¨ˆç®—å®Œäº†ãƒ‡ãƒ¼ã‚¿")

            # 4. ã€é‡è¦ã€‘ä¿®æ­£ç‰ˆåˆ†æã®å®Ÿè¡Œ
            logger.info("ğŸ”¬ ã€ä¿®æ­£ç‰ˆã€‘å³å¯†ãªæ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹åˆ†æã‚’å®Ÿè¡Œä¸­...")
            log_system_resources()
            analyzer.stats = analyzer.analyze()
            
            # çµæœã®å¯è¦–åŒ–
            logger.info("ğŸ“Š å¯è¦–åŒ–ç”Ÿæˆä¸­...")
            analyzer.visualize()

            # ã€è¿½åŠ ã€‘ãƒ¬ãƒãƒ¼ãƒˆæ•´åˆæ€§ã®ç¢ºèª
            logger.info("ğŸ” ãƒ¬ãƒãƒ¼ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")
            oot_results = analyzer.stats.get('out_of_time_validation', {})
            test_performance = oot_results.get('test_performance', {})
            
            if test_performance:
                test_r2 = test_performance.get('r_squared', 0)
                test_corr = test_performance.get('correlation', 0)
                test_size = test_performance.get('sample_size', 0)
                
                logger.info(f"   ğŸ“Š æ¤œè¨¼æœŸé–“(2013-2014å¹´)ã‚µãƒ³ãƒ—ãƒ«æ•°: {test_size}é ­")
                logger.info(f"   ğŸ“Š æ¤œè¨¼æœŸé–“RÂ²: {test_r2:.3f}")
                logger.info(f"   ğŸ“Š æ¤œè¨¼æœŸé–“ç›¸é–¢ä¿‚æ•°: {test_corr:.3f}")
                
                # å®Ÿæ¸¬çµæœã®çµ±è¨ˆçš„è©•ä¾¡
                if test_r2 > 0.01:
                    logger.info("âœ… çµ±è¨ˆçš„ã«æœ‰æ„ãªèª¬æ˜åŠ›ã‚’ç¢ºèª")
                else:
                    logger.warning("âš ï¸ èª¬æ˜åŠ›ãŒé™å®šçš„ã§ã™")
                    
                if abs(test_corr) > 0.1:
                    logger.info("âœ… å®Ÿç”¨çš„ãªç›¸é–¢é–¢ä¿‚ã‚’ç¢ºèª")
                else:
                    logger.warning("âš ï¸ ç›¸é–¢é–¢ä¿‚ãŒå¼±ã„ã§ã™")

            # å±¤åˆ¥åˆ†æã®å®Ÿè¡Œ
            logger.info("ğŸ“Š çµ±åˆå±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œä¸­...")
            try:
                stratified_dataset = create_stratified_dataset_from_export('export/dataset')
                stratified_results = perform_integrated_stratified_analysis(stratified_dataset)
                _ = generate_stratified_report(stratified_results, stratified_dataset, output_dir)
                logger.info("âœ… çµ±åˆå±¤åˆ¥åˆ†æå®Œäº†")
            except Exception as e:
                logger.error(f"âŒ å±¤åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
            
            logger.info(f"âœ… ã€ä¿®æ­£ç‰ˆã€‘åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
            logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
            logger.info("ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ã¨æ™‚ç³»åˆ—åˆ†å‰²ãŒæ­£ã—ãå®Ÿè£…ã•ã‚Œã¾ã—ãŸã€‚")
            logger.info("ğŸ“Š çµ±åˆå±¤åˆ¥åˆ†æã«ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªæ¤œè¨¼ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚")

        return 0

    except FileNotFoundError as e:
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        logger.error("   â€¢ å…¥åŠ›ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯è‹±æ•°å­—ã«å¤‰æ›´ã—ã¦ãã ã•ã„")
        logger.error("   â€¢ 'export/with_bias' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except ValueError as e:
        error_msg = str(e)
        logger.error(f"âŒ å…¥åŠ›å€¤ã‚¨ãƒ©ãƒ¼: {error_msg}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        
        if "æ¡ä»¶ã‚’æº€ãŸã™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" in error_msg:
            logger.error("   â€¢ --min-races ã®å€¤ã‚’å°ã•ãã—ã¦ã¿ã¦ãã ã•ã„ï¼ˆä¾‹: --min-races 3ï¼‰")
            logger.error("   â€¢ æœŸé–“æŒ‡å®šãŒç‹­ã™ãã‚‹å ´åˆã¯ç¯„å›²ã‚’åºƒã’ã¦ãã ã•ã„")
            logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹æœŸé–“ã‹ã©ã†ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        elif "æ—¥ä»˜å½¢å¼" in error_msg:
            logger.error("   â€¢ æ—¥ä»˜ã¯YYYYMMDDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 20220101ï¼‰")
            logger.error("   â€¢ --start-date ã¨ --end-date ã®ä¸¡æ–¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        else:
            logger.error("   â€¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ --help ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’ç¢ºèªã§ãã¾ã™")
        
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except IndexError as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒçŸ­ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        logger.error("   â€¢ æ™‚ç³»åˆ—åˆ†å‰²ã«å¿…è¦ãªæœ€ä½3å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        logger.error("   â€¢ æœŸé–“æŒ‡å®šã‚’åºƒã’ã¦å†å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except KeyboardInterrupt:
        logger.warning("â¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        logger.info("ğŸ’¡ å‡¦ç†æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ã«ã¯:")
        logger.info("   â€¢ --min-races ã‚’å¤§ããã—ã¦ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ¸›ã‚‰ã™")
        logger.info("   â€¢ æœŸé–“ã‚’çŸ­ãã—ã¦å‡¦ç†ç¯„å›²ã‚’çµã‚‹")
        logger.info("   â€¢ --disable-stratified-analysis ã§å±¤åˆ¥åˆ†æã‚’ç„¡åŠ¹åŒ–")
        if log_file:
            logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        return 1
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}")
        logger.error("ğŸ’¡ è§£æ±ºæ–¹æ³•:")
        
        if "encoding" in error_msg.lower() or "unicode" in error_msg.lower():
            logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ã¾ãŸã¯Shift-JISã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        elif "memory" in error_msg.lower():
            logger.error("   â€¢ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ --min-races ã‚’å¤§ããã—ã¦ãƒ‡ãƒ¼ã‚¿é‡ã‚’æ¸›ã‚‰ã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ä¸è¦ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¦ãã ã•ã„")
        elif "permission" in error_msg.lower():
            logger.error("   â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®å•é¡ŒãŒã‚ã‚Šã¾ã™")
            logger.error("   â€¢ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ç®¡ç†è€…æ¨©é™ã§å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„")
        else:
            logger.error("   â€¢ --log-level DEBUG ã§è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("   â€¢ Pythonã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        logger.error("ğŸ” è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
        logger.error(f"   ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥: {type(e).__name__}")
        logger.error(f"   ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_msg}")
        if log_file:
            logger.error(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        logger.error("è©³ç´°ãªã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:", exc_info=True)
        return 1

if __name__ == '__main__':
    sys.exit(main())