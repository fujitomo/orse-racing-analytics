"""
é‡ã¿è¨ˆç®—æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
race_level_analysis_report.md ã®5.1.3ç¯€ã«åŸºã¥ãé‡ã¿è¨ˆç®—ã®æ¤œè¨¼

å•é¡Œ:
- ãƒ­ã‚°ã®é‡ã¿è¨ˆç®—çµæœãŒãƒ¬ãƒãƒ¼ãƒˆã¨å¤§ããç•°ãªã‚‹
- å‹•çš„è¨ˆç®—: ã‚°ãƒ¬ãƒ¼ãƒ‰6.6%, å ´æ‰€5.4%, è·é›¢88.0%
- ãƒ¬ãƒãƒ¼ãƒˆå€¤: ã‚°ãƒ¬ãƒ¼ãƒ‰63.6%, å ´æ‰€32.3%, è·é›¢4.1%

æ¤œè¨¼é …ç›®:
1. ç›¸é–¢è¨ˆç®—ã®æ­£ç¢ºæ€§
2. é‡ã¿è¨ˆç®—å¼ã®é©ç”¨
3. ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§
4. ãƒ¬ãƒãƒ¼ãƒˆå€¤ã¨ã®æ¯”è¼ƒ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from scipy.stats import pearsonr
import sys
import os

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_training_data():
    """è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿ï¼ˆ2010-2020å¹´ï¼‰ã‚’èª­ã¿è¾¼ã¿"""
    logger.info("ğŸ“– è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿ï¼ˆ2010-2020å¹´ï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    dataset_dir = Path("export/dataset")
    csv_files = list(dataset_dir.glob("*.csv"))
    
    if not csv_files:
        logger.error("âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    logger.info(f"ğŸ“Š {len(csv_files)}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    all_dfs = []
    for i, csv_file in enumerate(csv_files):
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            all_dfs.append(df)
            
            if (i + 1) % 100 == 0:
                logger.info(f"   é€²æ—: {i + 1}/{len(csv_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {csv_file.name} - {str(e)}")
            continue
    
    if not all_dfs:
        logger.error("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    combined_df = pd.concat(all_dfs, ignore_index=True)
    logger.info(f"âœ… çµ±åˆå®Œäº†: {len(combined_df):,}è¡Œã®ãƒ‡ãƒ¼ã‚¿")
    
    # å¹´åˆ—ã®ç¢ºèªã¨ä½œæˆ
    if 'å¹´' not in combined_df.columns:
        if 'å¹´æœˆæ—¥' in combined_df.columns:
            combined_df['å¹´'] = pd.to_datetime(combined_df['å¹´æœˆæ—¥'], errors='coerce').dt.year
        else:
            logger.error("âŒ å¹´ã¾ãŸã¯å¹´æœˆæ—¥åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
    
    # è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    train_data = combined_df[(combined_df['å¹´'] >= 2010) & (combined_df['å¹´'] <= 2020)].copy()
    logger.info(f"ğŸ“Š è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿: {len(train_data):,}è¡Œ (2010-2020å¹´)")
    
    return train_data

def calculate_feature_levels(df):
    """ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
    logger.info("ğŸ§® ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ä¸­...")
    
    df_copy = df.copy()
    
    # 1. grade_level ã®è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰
    def calculate_grade_level(row):
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜æº–æ‹ ï¼‰"""
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ—ã®å€™è£œã‚’ç¢ºèª
        grade_cols = ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x', 'ã‚°ãƒ¬ãƒ¼ãƒ‰_y', 'ã‚°ãƒ¬ãƒ¼ãƒ‰', 'grade']
        grade_value = None
        
        for col in grade_cols:
            if col in row and pd.notna(row[col]):
                grade_value = row[col]
                break
        
        if grade_value is None:
            return 2.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆç‰¹åˆ¥ãƒ¬ãƒ™ãƒ«ï¼‰
        
        # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«è¨­å®š
        try:
            grade_num = float(grade_value)
            if grade_num == 1:
                return 9.0  # G1ï¼ˆæœ€é«˜ï¼‰
            elif grade_num == 2:
                return 7.5  # G2
            elif grade_num == 3:
                return 6.0  # G3
            elif grade_num == 4:
                return 4.5  # é‡è³
            elif grade_num == 5:
                return 2.0  # ç‰¹åˆ¥
            elif grade_num == 6:
                return 3.0  # ãƒªã‚¹ãƒ†ãƒƒãƒ‰
            else:
                return 1.0  # ãã®ä»–
        except (ValueError, TypeError):
            return 2.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    df_copy['grade_level'] = df_copy.apply(calculate_grade_level, axis=1)
    logger.info(f"ğŸ“Š grade_level è¨ˆç®—å®Œäº†: ç¯„å›² {df_copy['grade_level'].min():.2f} - {df_copy['grade_level'].max():.2f}")
    
    # 2. venue_level ã®è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰
    def calculate_venue_level(row):
        """å ´æ‰€ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜æº–æ‹ ï¼‰"""
        # ç«¶é¦¬å ´åã«åŸºã¥ãæ ¼å¼ãƒ¬ãƒ™ãƒ«
        venue_cols = ['å ´å', 'ç«¶é¦¬å ´', 'venue']
        venue_name = None
        
        for col in venue_cols:
            if col in row and pd.notna(row[col]):
                venue_name = str(row[col])
                break
        
        if venue_name is None:
            return 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®ç«¶é¦¬å ´æ ¼å¼ãƒ¬ãƒ™ãƒ«
        if venue_name in ['æ±äº¬', 'é˜ªç¥']:
            return 3.0  # æœ€é«˜æ ¼å¼
        elif venue_name in ['äº¬éƒ½', 'ä¸­å±±']:
            return 2.5  # é«˜æ ¼å¼
        elif venue_name in ['æ–°æ½Ÿ', 'ä¸­äº¬', 'å°å€‰']:
            return 2.0  # ä¸­æ ¼å¼
        elif venue_name in ['æœ­å¹Œ', 'å‡½é¤¨', 'ç¦å³¶']:
            return 1.5  # ä½æ ¼å¼
        else:
            return 1.0  # ãã®ä»–
    
    df_copy['venue_level'] = df_copy.apply(calculate_venue_level, axis=1)
    logger.info(f"ğŸ“Š venue_level è¨ˆç®—å®Œäº†: ç¯„å›² {df_copy['venue_level'].min():.2f} - {df_copy['venue_level'].max():.2f}")
    
    # 3. distance_level ã®è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰
    def calculate_distance_level(row):
        """è·é›¢ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜æº–æ‹ ï¼‰"""
        if 'è·é›¢' in row and pd.notna(row['è·é›¢']):
            try:
                distance = float(row['è·é›¢'])
                # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®è·é›¢ãƒ¬ãƒ™ãƒ«è¨­å®š
                if distance >= 2000:
                    return 1.25  # é•·è·é›¢
                elif distance >= 1600:
                    return 1.35  # ä¸­é•·è·é›¢
                elif distance >= 1200:
                    return 1.0   # ä¸­è·é›¢
                else:
                    return 0.85  # çŸ­è·é›¢
            except (ValueError, TypeError):
                pass
        
        return 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    df_copy['distance_level'] = df_copy.apply(calculate_distance_level, axis=1)
    logger.info(f"ğŸ“Š distance_level è¨ˆç®—å®Œäº†: ç¯„å›² {df_copy['distance_level'].min():.2f} - {df_copy['distance_level'].max():.2f}")
    
    return df_copy

def create_horse_statistics(df):
    """é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    logger.info("ğŸ é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...")
    
    # è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆ
    if 'ç€é †' in df.columns:
        df['is_placed'] = (pd.to_numeric(df['ç€é †'], errors='coerce') <= 3).astype(int)
        logger.info("ğŸ“Š ç€é †åˆ—ã‹ã‚‰è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆï¼ˆç€é †<=3ï¼‰")
    elif 'è¤‡å‹' in df.columns:
        df['is_placed'] = pd.to_numeric(df['è¤‡å‹'], errors='coerce').fillna(0)
        logger.info("ğŸ“Š è¤‡å‹åˆ—ã‹ã‚‰è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆ")
    else:
        logger.error("âŒ è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆã§ãã¾ã›ã‚“")
        return None
    
    # è¤‡å‹ãƒ•ãƒ©ã‚°ã®çµ±è¨ˆã‚’ç¢ºèª
    placed_count = df['is_placed'].sum()
    total_count = len(df)
    placed_rate = placed_count / total_count if total_count > 0 else 0
    logger.info(f"ğŸ“Š è¤‡å‹ãƒ•ãƒ©ã‚°çµ±è¨ˆ: {placed_count:,}/{total_count:,} ({placed_rate:.1%})")
    
    # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆæœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šï¼‰
    horse_stats = df.groupby('é¦¬å').agg({
        'is_placed': 'mean',  # è¤‡å‹ç‡
        'å¹´': 'count'  # å‡ºèµ°å›æ•°
    }).reset_index()
    
    # åˆ—åã‚’æ¨™æº–åŒ–
    horse_stats.columns = ['é¦¬å', 'place_rate', 'race_count']
    
    # æœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜æº–æ‹ ï¼‰
    horse_stats = horse_stats[horse_stats['race_count'] >= 6].copy()
    logger.info(f"ğŸ“Š æœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šã§ãƒ•ã‚£ãƒ«ã‚¿: {len(horse_stats):,}é ­")
    
    # ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã®å¹³å‡ã‚’è¨ˆç®—
    feature_cols = ['grade_level', 'venue_level', 'distance_level']
    for col in feature_cols:
        if col in df.columns:
            avg_feature = df.groupby('é¦¬å')[col].mean().reset_index()
            avg_feature.columns = ['é¦¬å', f'avg_{col}']
            horse_stats = horse_stats.merge(avg_feature, on='é¦¬å', how='left')
    
    logger.info(f"ğŸ“Š é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(horse_stats):,}é ­")
    
    return horse_stats

def calculate_correlations(horse_stats):
    """ç›¸é–¢ã‚’è¨ˆç®—ï¼ˆé¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰"""
    logger.info("ğŸ“ˆ é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã§ç›¸é–¢ã‚’è¨ˆç®—ä¸­...")
    
    # å¿…è¦ãªåˆ—ã®ç¢ºèª
    required_cols = ['place_rate', 'avg_grade_level', 'avg_venue_level', 'avg_distance_level']
    missing_cols = [col for col in required_cols if col not in horse_stats.columns]
    
    if missing_cols:
        logger.error(f"âŒ å¿…è¦ãªåˆ—ãŒä¸è¶³: {missing_cols}")
        logger.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(horse_stats.columns)}")
        return None
    
    # æ¬ æå€¤ã‚’é™¤å»
    clean_data = horse_stats[required_cols].dropna()
    logger.info(f"ğŸ“Š ç›¸é–¢è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿: {len(clean_data):,}é ­")
    
    if len(clean_data) < 100:
        logger.error(f"âŒ ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³: {len(clean_data)}é ­ï¼ˆæœ€ä½100é ­å¿…è¦ï¼‰")
        return None
    
    # ç›¸é–¢è¨ˆç®—
    correlations = {}
    target = clean_data['place_rate']
    
    # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®ç›¸é–¢è¨ˆç®—
    feature_mapping = {
        'avg_grade_level': 'grade',
        'avg_venue_level': 'venue', 
        'avg_distance_level': 'distance'
    }
    
    for feature_col, feature_name in feature_mapping.items():
        if feature_col in clean_data.columns:
            corr, p_value = pearsonr(clean_data[feature_col], target)
            correlations[feature_name] = {
                'correlation': corr,
                'p_value': p_value,
                'squared': corr ** 2
            }
            logger.info(f"   ğŸ“ˆ {feature_name}_level: r = {corr:.3f}, rÂ² = {corr**2:.3f}, p = {p_value:.3f}")
    
    return correlations

def calculate_weights(correlations):
    """é‡ã¿ã‚’è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰"""
    logger.info("âš–ï¸ é‡ã¿ã‚’è¨ˆç®—ä¸­...")
    logger.info("ğŸ“‹ è¨ˆç®—å¼: w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)")
    
    # ç›¸é–¢ã®äºŒä¹—ã‚’è¨ˆç®—
    squared_correlations = {}
    total_squared = 0
    
    for feature, stats in correlations.items():
        squared = stats['squared']
        squared_correlations[feature] = squared
        total_squared += squared
        logger.info(f"   ğŸ“Š {feature}: rÂ² = {squared:.3f}")
    
    logger.info(f"ğŸ“Š ç·å¯„ä¸åº¦: {total_squared:.3f}")
    
    if total_squared == 0:
        logger.warning("âš ï¸ ç·å¯„ä¸åº¦ãŒ0ã§ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return get_fallback_weights()
    
    # é‡ã¿ã‚’æ­£è¦åŒ–
    weights = {}
    for feature, squared in squared_correlations.items():
        weight = squared / total_squared
        weights[feature] = weight
        logger.info(f"   âš–ï¸ {feature}: w = {weight:.3f} ({weight*100:.1f}%)")
    
    return weights

def get_fallback_weights():
    """ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€ã®å›ºå®šé‡ã¿"""
    return {
        'grade': 0.636,   # 63.6%
        'venue': 0.323,   # 32.3%
        'distance': 0.041 # 4.1%
    }

def compare_with_report(weights):
    """ãƒ¬ãƒãƒ¼ãƒˆå€¤ã¨æ¯”è¼ƒ"""
    logger.info("ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆå€¤ã¨æ¯”è¼ƒä¸­...")
    
    # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€ã®å€¤
    report_weights = {
        'grade': 0.636,   # 63.6%
        'venue': 0.323,   # 32.3%
        'distance': 0.041 # 4.1%
    }
    
    logger.info("ğŸ“Š æ¯”è¼ƒçµæœ:")
    for feature in weights.keys():
        calculated = weights[feature]
        reported = report_weights.get(feature, 0)
        diff = calculated - reported
        diff_pct = (diff / reported * 100) if reported > 0 else 0
        
        logger.info(f"   {feature}:")
        logger.info(f"     è¨ˆç®—å€¤: {calculated:.3f} ({calculated*100:.1f}%)")
        logger.info(f"     ãƒ¬ãƒãƒ¼ãƒˆå€¤: {reported:.3f} ({reported*100:.1f}%)")
        logger.info(f"     å·®ç•°: {diff:+.3f} ({diff_pct:+.1f}%)")
    
    return report_weights

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸ” é‡ã¿è¨ˆç®—æ¤œè¨¼ã‚’é–‹å§‹...")
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_data = load_training_data()
    if train_data is None:
        return
    
    # 2. ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«è¨ˆç®—
    df_with_features = calculate_feature_levels(train_data)
    
    # 3. é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    horse_stats = create_horse_statistics(df_with_features)
    if horse_stats is None:
        return
    
    # 4. ç›¸é–¢è¨ˆç®—
    correlations = calculate_correlations(horse_stats)
    if correlations is None:
        return
    
    # 5. é‡ã¿è¨ˆç®—
    weights = calculate_weights(correlations)
    
    # 6. ãƒ¬ãƒãƒ¼ãƒˆå€¤ã¨æ¯”è¼ƒ
    report_weights = compare_with_report(weights)
    
    # 7. çµæœã‚µãƒãƒªãƒ¼
    logger.info("ğŸ“‹ æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼:")
    logger.info("=" * 50)
    logger.info("è¨ˆç®—ã•ã‚ŒãŸé‡ã¿:")
    for feature, weight in weights.items():
        logger.info(f"  {feature}: {weight:.3f} ({weight*100:.1f}%)")
    
    logger.info("\nãƒ¬ãƒãƒ¼ãƒˆå€¤:")
    for feature, weight in report_weights.items():
        logger.info(f"  {feature}: {weight:.3f} ({weight*100:.1f}%)")
    
    # 8. å•é¡Œè¨ºæ–­
    logger.info("\nğŸ” å•é¡Œè¨ºæ–­:")
    max_diff = max(abs(weights.get(f, 0) - report_weights.get(f, 0)) for f in weights.keys())
    if max_diff > 0.1:
        logger.warning(f"âš ï¸ é‡ã¿è¨ˆç®—ã«å¤§ããªå·®ç•°ãŒã‚ã‚Šã¾ã™ (æœ€å¤§å·®ç•°: {max_diff:.3f})")
        logger.info("   åŸå› å€™è£œ:")
        logger.info("   1. ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—æ–¹æ³•ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰")
        logger.info("   2. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰")
        logger.info("   3. ç›¸é–¢è¨ˆç®—ã®å‰ææ¡ä»¶")
        logger.info("   4. ãƒ¬ãƒãƒ¼ãƒˆå€¤ã®ç®—å‡ºæ–¹æ³•")
        
        # è©³ç´°åˆ†æ
        logger.info("\nğŸ“Š è©³ç´°åˆ†æ:")
        for feature in weights.keys():
            calc_val = weights[feature]
            report_val = report_weights[feature]
            diff = abs(calc_val - report_val)
            if diff > 0.1:
                logger.info(f"   {feature}: å·®ç•° {diff:.3f} (è¨ˆç®—å€¤: {calc_val:.3f}, ãƒ¬ãƒãƒ¼ãƒˆå€¤: {report_val:.3f})")
    else:
        logger.info("âœ… é‡ã¿è¨ˆç®—ã¯æ­£å¸¸ã§ã™")
    
    logger.info("âœ… é‡ã¿è¨ˆç®—æ¤œè¨¼å®Œäº†")

if __name__ == "__main__":
    main()
