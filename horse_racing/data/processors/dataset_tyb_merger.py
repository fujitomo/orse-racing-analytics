"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»TYBçµ±åˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
SEDãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«TYBã®ç›´å‰æƒ…å ±ã‚’çµåˆã™ã‚‹
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import List, Dict, Any, Optional, Tuple
import re
from datetime import datetime

logger = logging.getLogger(__name__)

class DatasetTYBMerger:
    """
    SEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨TYBãƒ‡ãƒ¼ã‚¿ã®çµ±åˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
    """
    
    def __init__(self):
        self.processing_stats = {
            'total_datasets': 0,
            'successful_merges': 0,
            'failed_merges': 0,
            'processing_errors': []
        }
        
    def create_race_key_from_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        SEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            df: SEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            
        Returns:
            ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼ä»˜ãã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        try:
            # å¹´ã€æœˆã€æ—¥ã€å ´ã‚³ãƒ¼ãƒ‰ã€Rã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            # TYBã®ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼å½¢å¼: YYMMDDCCR (YY=å¹´, MM=æœˆ, DD=æ—¥, CC=å ´ã‚³ãƒ¼ãƒ‰, R=ãƒ¬ãƒ¼ã‚¹No)
            
            df = df.copy()
            
            # æ—¥ä»˜æƒ…å ±ã®å–å¾—ï¼ˆå¹´æœˆæ—¥åˆ—ã‹ã‚‰ï¼‰
            if 'å¹´æœˆæ—¥' in df.columns:
                # å¹´æœˆæ—¥ã‹ã‚‰æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡º
                df['å¹´æœˆæ—¥_str'] = df['å¹´æœˆæ—¥'].astype(str)
                df['å¹´_from_date'] = df['å¹´æœˆæ—¥_str'].str[:4].astype(int)
                df['æœˆ_from_date'] = df['å¹´æœˆæ—¥_str'].str[4:6].astype(int)
                df['æ—¥_from_date'] = df['å¹´æœˆæ—¥_str'].str[6:8].astype(int)
            elif all(col in df.columns for col in ['å¹´', 'æœˆ', 'æ—¥']):
                # æ—¢ã«å¹´æœˆæ—¥åˆ—ãŒã‚ã‚‹å ´åˆ
                df['å¹´_from_date'] = df['å¹´']
                df['æœˆ_from_date'] = df['æœˆ']
                df['æ—¥_from_date'] = df['æ—¥']
            else:
                logger.warning("æ—¥ä»˜æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return df
            
            # ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼ã®ç”Ÿæˆ
            # YY: å¹´ã®ä¸‹2æ¡
            df['å¹´_2æ¡'] = (df['å¹´_from_date'] % 100).astype(str).str.zfill(2)
            
            # MM: æœˆï¼ˆ16é€²æ•°ï¼‰
            df['æœˆ_16é€²'] = df['æœˆ_from_date'].apply(lambda x: f"{x:02X}")
            
            # DD: æ—¥ï¼ˆ16é€²æ•°ï¼‰
            df['æ—¥_16é€²'] = df['æ—¥_from_date'].apply(lambda x: f"{x:02X}")
            
            # CC: å ´ã‚³ãƒ¼ãƒ‰ï¼ˆ2æ¡ï¼‰
            df['å ´ã‚³ãƒ¼ãƒ‰_2æ¡'] = df['å ´ã‚³ãƒ¼ãƒ‰'].astype(str).str.zfill(2)
            
            # R: ãƒ¬ãƒ¼ã‚¹Noï¼ˆ2æ¡ï¼‰
            df['ãƒ¬ãƒ¼ã‚¹No_2æ¡'] = df['R'].astype(str).str.zfill(2)
            
            # ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼ã®çµåˆ
            df['ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼'] = (df['å¹´_2æ¡'] + df['æœˆ_16é€²'] + df['æ—¥_16é€²'] + 
                              df['å ´ã‚³ãƒ¼ãƒ‰_2æ¡'] + df['ãƒ¬ãƒ¼ã‚¹No_2æ¡'])
            
            # ä¸­é–“åˆ—ã‚’å‰Šé™¤
            columns_to_drop = ['å¹´_2æ¡', 'æœˆ_16é€²', 'æ—¥_16é€²', 'å ´ã‚³ãƒ¼ãƒ‰_2æ¡', 'ãƒ¬ãƒ¼ã‚¹No_2æ¡',
                              'å¹´_from_date', 'æœˆ_from_date', 'æ—¥_from_date']
            if 'å¹´æœˆæ—¥_str' in df.columns:
                columns_to_drop.append('å¹´æœˆæ—¥_str')
                
            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])
            
            logger.debug(f"ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼ç”Ÿæˆå®Œäº†: {len(df)}ä»¶")
            return df
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒ¼ã‚¹ã‚­ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return df
    
    def merge_dataset_with_tyb(self, dataset_path: Path, tyb_path: Path) -> Optional[pd.DataFrame]:
        """
        SEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨TYBãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            dataset_path: SEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
            tyb_path: TYBãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
            
        Returns:
            çµåˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        try:
            logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆ: {dataset_path.name} + {tyb_path.name}")
            
            # SEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
            sed_df = pd.read_csv(dataset_path, encoding='utf-8')
            logger.debug(f"SEDãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(sed_df)}ä»¶")
            
            # TYBãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            tyb_df = pd.read_csv(tyb_path, encoding='utf-8')
            logger.debug(f"TYBãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(tyb_df)}ä»¶")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            target_date = self.extract_date_from_filename(dataset_path)
            if target_date:
                # SEDãƒ‡ãƒ¼ã‚¿ã‚’å¯¾è±¡æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿
                sed_df = sed_df[sed_df['å¹´æœˆæ—¥'].astype(str) == target_date].copy()
                logger.debug(f"æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(sed_df)}ä»¶")
            
            # TYBåˆ—åã®æ¥é ­è¾ã‚’è¿½åŠ ï¼ˆé‡è¤‡å›é¿ï¼‰
            tyb_columns_rename = {}
            for col in tyb_df.columns:
                if col not in ['å ´ã‚³ãƒ¼ãƒ‰', 'ãƒ¬ãƒ¼ã‚¹No', 'é¦¬ç•ª']:
                    tyb_columns_rename[col] = f'TYB_{col}'
            
            tyb_df = tyb_df.rename(columns=tyb_columns_rename)
            
            # ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã§ã®çµ±åˆï¼ˆå ´ã‚³ãƒ¼ãƒ‰ + ãƒ¬ãƒ¼ã‚¹Noï¼‰
            merge_columns = []
            
            # å ´ã‚³ãƒ¼ãƒ‰
            if 'å ´ã‚³ãƒ¼ãƒ‰' in sed_df.columns and 'å ´ã‚³ãƒ¼ãƒ‰' in tyb_df.columns:
                merge_columns.append('å ´ã‚³ãƒ¼ãƒ‰')
                
            # ãƒ¬ãƒ¼ã‚¹Noï¼ˆSEDã®'R'åˆ— vs TYBã®'ãƒ¬ãƒ¼ã‚¹No'åˆ—ï¼‰
            if 'R' in sed_df.columns and 'ãƒ¬ãƒ¼ã‚¹No' in tyb_df.columns:
                sed_df['ãƒ¬ãƒ¼ã‚¹No'] = sed_df['R']
                merge_columns.append('ãƒ¬ãƒ¼ã‚¹No')
            
            if merge_columns:
                # TYBãƒ‡ãƒ¼ã‚¿ã‚’ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã§é›†ç´„
                tyb_race_agg = self.aggregate_tyb_by_race(tyb_df)
                
                # ãƒ‡ãƒ¼ã‚¿çµåˆ
                merged_df = pd.merge(
                    sed_df, tyb_race_agg,
                    on=merge_columns,
                    how='left'
                )
                
                # TYBçµåˆç‡ã®è¨ˆç®—
                tyb_rate = 0
                if 'TYB_ãƒ¬ãƒ¼ã‚¹é ­æ•°' in merged_df.columns:
                    tyb_rate = merged_df['TYB_ãƒ¬ãƒ¼ã‚¹é ­æ•°'].notna().mean()
                elif 'TYB_IDM_å¹³å‡' in merged_df.columns:
                    tyb_rate = merged_df['TYB_IDM_å¹³å‡'].notna().mean()
                
                logger.info(f"   âœ… çµåˆå®Œäº†: {len(merged_df)}ä»¶ (TYBãƒ¬ãƒ¼ã‚¹çµåˆç‡: {tyb_rate:.1%})")
                
                return merged_df
                
            else:
                logger.warning("   âŒ çµåˆå¯èƒ½ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return sed_df
                
        except Exception as e:
            logger.error(f"   âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def extract_date_from_filename(self, file_path: Path) -> Optional[str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            YYYYMMDDå½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—
        """
        try:
            # SEDãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜æŠ½å‡º (ä¾‹: SED250420_formatted_dataset.csv -> 250420)
            match = re.search(r'SED(\d{6})', file_path.name)
            if match:
                date_str = match.group(1)  # 250420
                
                # YYMMDD -> YYYYMMDDå¤‰æ›
                if len(date_str) == 6:
                    å¹´_2æ¡ = date_str[:2]  # 25
                    æœˆ_2æ¡ = date_str[2:4]  # 04  
                    æ—¥_2æ¡ = date_str[4:6]  # 20
                    
                    å¹´_æ•°å€¤ = int(å¹´_2æ¡)
                    è¥¿æš¦å¹´ = 2000 + å¹´_æ•°å€¤ if å¹´_æ•°å€¤ < 50 else 1900 + å¹´_æ•°å€¤
                    
                    return f"{è¥¿æš¦å¹´:04d}{æœˆ_2æ¡}{æ—¥_2æ¡}"
                    
        except Exception as e:
            logger.debug(f"æ—¥ä»˜æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        return None
    
    def aggregate_tyb_by_race(self, tyb_df: pd.DataFrame) -> pd.DataFrame:
        """
        TYBãƒ‡ãƒ¼ã‚¿ã‚’ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã§é›†ç´„
        
        Args:
            tyb_df: TYBãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            
        Returns:
            ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰ã§é›†ç´„ã•ã‚ŒãŸTYBãƒ‡ãƒ¼ã‚¿
        """
        try:
            # ãƒ¬ãƒ¼ã‚¹å˜ä½ã§ã®é›†ç´„
            race_groups = tyb_df.groupby(['å ´ã‚³ãƒ¼ãƒ‰', 'ãƒ¬ãƒ¼ã‚¹No'])
            
            # é›†ç´„è¾æ›¸
            agg_dict = {}
            
            # æ•°å€¤åˆ—ã®é›†ç´„
            numeric_columns = ['TYB_IDM', 'TYB_é¨æ‰‹æŒ‡æ•°', 'TYB_æƒ…å ±æŒ‡æ•°', 'TYB_ç·åˆæŒ‡æ•°', 
                              'TYB_äººæ°—æŒ‡æ•°', 'TYB_èª¿æ•™æŒ‡æ•°', 'TYB_å©èˆæŒ‡æ•°', 'TYB_é¦¬ä½“é‡']
            
            for col in numeric_columns:
                if col in tyb_df.columns:
                    agg_dict[f'{col}_å¹³å‡'] = (col, lambda x: x.mean() if x.notna().any() else None)
                    agg_dict[f'{col}_æœ€å¤§'] = (col, lambda x: x.max() if x.notna().any() else None)
                    agg_dict[f'{col}_æœ€å°'] = (col, lambda x: x.min() if x.notna().any() else None)
            
            # ã‚ªãƒƒã‚ºé–¢é€£ã®é›†ç´„
            odds_columns = ['TYB_å˜å‹ã‚ªãƒƒã‚º', 'TYB_è¤‡å‹ã‚ªãƒƒã‚º']
            for col in odds_columns:
                if col in tyb_df.columns:
                    agg_dict[f'{col}_å¹³å‡'] = (col, lambda x: x.mean() if x.notna().any() else None)
                    agg_dict[f'{col}_æœ€ä½'] = (col, lambda x: x.min() if x.notna().any() else None)
            
            # é¦¬ä½“é‡é–¢é€£
            if 'TYB_é¦¬ä½“é‡' in tyb_df.columns:
                agg_dict['TYB_é¦¬ä½“é‡_å¹³å‡'] = ('TYB_é¦¬ä½“é‡', lambda x: x.mean() if x.notna().any() else None)
                
            if 'TYB_é¦¬ä½“é‡å¢—æ¸›' in tyb_df.columns:
                agg_dict['TYB_é¦¬ä½“é‡å¢—æ¸›_å¹³å‡'] = ('TYB_é¦¬ä½“é‡å¢—æ¸›', lambda x: x.mean() if x.notna().any() else None)
            
            # ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±
            agg_dict['TYB_ãƒ¬ãƒ¼ã‚¹é ­æ•°'] = ('TYB_IDM', 'count')
            agg_dict['TYB_ãƒ‡ãƒ¼ã‚¿æ•°'] = ('TYB_IDM', lambda x: x.notna().sum())
            
            # å°è©•ä¾¡ã®æœ€é »å€¤
            if 'TYB_ç›´å‰ç·åˆå°' in tyb_df.columns:
                agg_dict['TYB_ç›´å‰ç·åˆå°_æœ€é »'] = ('TYB_ç›´å‰ç·åˆå°', lambda x: x.mode().iloc[0] if not x.mode().empty else None)
            
            # é›†ç´„å®Ÿè¡Œ
            if agg_dict:
                race_agg = race_groups.agg(**agg_dict).reset_index()
                
                # ã‚«ãƒ©ãƒ åã‚’æ•´ç†
                race_agg.columns = ['å ´ã‚³ãƒ¼ãƒ‰', 'ãƒ¬ãƒ¼ã‚¹No'] + [col for col in race_agg.columns if col not in ['å ´ã‚³ãƒ¼ãƒ‰', 'ãƒ¬ãƒ¼ã‚¹No']]
                
                logger.debug(f"TYBãƒ¬ãƒ¼ã‚¹é›†ç´„å®Œäº†: {len(race_agg)}ãƒ¬ãƒ¼ã‚¹")
                
                return race_agg
            else:
                # æœ€ä½é™ã®é›†ç´„
                race_agg = race_groups.size().reset_index(name='TYB_ãƒ¬ãƒ¼ã‚¹é ­æ•°')
                return race_agg
                
        except Exception as e:
            logger.error(f"TYBãƒ¬ãƒ¼ã‚¹é›†ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ç©ºã®DataFrameã‚’è¿”ã™
            return pd.DataFrame(columns=['å ´ã‚³ãƒ¼ãƒ‰', 'ãƒ¬ãƒ¼ã‚¹No', 'TYB_ãƒ¬ãƒ¼ã‚¹é ­æ•°'])
    
    def find_matching_tyb_file(self, dataset_file: Path, tyb_dir: Path) -> Optional[Path]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹TYBãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        
        Args:
            dataset_file: SEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«
            tyb_dir: TYBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            å¯¾å¿œã™ã‚‹TYBãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            # SEDãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º (ä¾‹: SED250420_formatted_dataset.csv -> 250420)
            match = re.search(r'SED(\d{6})_formatted_dataset\.csv', dataset_file.name)
            if not match:
                return None
                
            date_str = match.group(1)
            
            # å¯¾å¿œã™ã‚‹TYBãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ (ä¾‹: TYB250420_formatted.csv)
            tyb_filename = f"TYB{date_str}_formatted.csv"
            tyb_path = tyb_dir / tyb_filename
            
            if tyb_path.exists():
                return tyb_path
            else:
                logger.debug(f"å¯¾å¿œã™ã‚‹TYBãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tyb_filename}")
                return None
                
        except Exception as e:
            logger.debug(f"TYBãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def process_all_datasets(self, dataset_dir: Path = None, tyb_dir: Path = None, output_dir: Path = None):
        """
        å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«TYBãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
        
        Args:
            dataset_dir: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            tyb_dir: TYBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª  
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã®è¨­å®š
        if dataset_dir is None:
            dataset_dir = Path('export/dataset')
        if tyb_dir is None:
            tyb_dir = Path('export/TYB/formatted')
        if output_dir is None:
            output_dir = Path('export/dataset_with_tyb')
            
        logger.info("ğŸ”— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»TYBçµ±åˆå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        logger.info(f"   ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_dir}")
        logger.info(f"   ğŸ“‚ TYBãƒ‡ãƒ¼ã‚¿: {tyb_dir}")
        logger.info(f"   ğŸ“‚ å‡ºåŠ›å…ˆ: {output_dir}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        dataset_files = list(dataset_dir.glob('SED*_formatted_dataset.csv'))
        
        if not dataset_files:
            logger.error("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
            
        logger.info(f"   ğŸ“„ å‡¦ç†å¯¾è±¡: {len(dataset_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        self.processing_stats['total_datasets'] = len(dataset_files)
        
        # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†
        for i, dataset_file in enumerate(dataset_files, 1):
            try:
                # å¯¾å¿œã™ã‚‹TYBãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
                tyb_file = self.find_matching_tyb_file(dataset_file, tyb_dir)
                
                if tyb_file is None:
                    logger.debug(f"   [{i:3d}/{len(dataset_files)}] TYBãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {dataset_file.name}")
                    # TYBãƒ‡ãƒ¼ã‚¿ãªã—ã§ã‚‚ã‚³ãƒ”ãƒ¼
                    sed_df = pd.read_csv(dataset_file, encoding='utf-8')
                    output_path = output_dir / f"{dataset_file.stem}_with_tyb.csv"
                    sed_df.to_csv(output_path, index=False, encoding='utf-8')
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆ
                merged_df = self.merge_dataset_with_tyb(dataset_file, tyb_file)
                
                if merged_df is not None:
                    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
                    output_filename = f"{dataset_file.stem}_with_tyb.csv"
                    output_path = output_dir / output_filename
                    
                    # CSVå‡ºåŠ›
                    merged_df.to_csv(output_path, index=False, encoding='utf-8')
                    
                    self.processing_stats['successful_merges'] += 1
                    
                    if i % 50 == 0:
                        logger.info(f"   [{i:3d}/{len(dataset_files)}] å‡¦ç†å®Œäº†: {self.processing_stats['successful_merges']}ä»¶æˆåŠŸ")
                        
                else:
                    self.processing_stats['failed_merges'] += 1
                    logger.warning(f"   [{i:3d}/{len(dataset_files)}] çµ±åˆå¤±æ•—: {dataset_file.name}")
                    
            except Exception as e:
                self.processing_stats['failed_merges'] += 1
                self.processing_stats['processing_errors'].append(f"{dataset_file.name}: {str(e)}")
                logger.error(f"   [{i:3d}/{len(dataset_files)}] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {dataset_file.name} - {str(e)}")
        
        # å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»TYBçµ±åˆçµæœ")
        logger.info("="*60)
        logger.info(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.processing_stats['total_datasets']}")
        logger.info(f"   æˆåŠŸ: {self.processing_stats['successful_merges']}")
        logger.info(f"   å¤±æ•—: {self.processing_stats['failed_merges']}")
        logger.info(f"   æˆåŠŸç‡: {self.processing_stats['successful_merges']/self.processing_stats['total_datasets']:.1%}")
        
        if self.processing_stats['processing_errors']:
            logger.info(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {len(self.processing_stats['processing_errors'])}ä»¶")
            for error in self.processing_stats['processing_errors'][:5]:
                logger.info(f"     - {error}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import logging
    logging.basicConfig(level=logging.INFO)
    
    merger = DatasetTYBMerger()
    merger.process_all_datasets()

if __name__ == "__main__":
    main()
