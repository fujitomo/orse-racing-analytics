"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å¯¾å¿œã—ãŸå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®æœ€é©åŒ–æ©Ÿèƒ½

ä¸»è¦æ©Ÿèƒ½:
1. ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²å‡¦ç†
2. ä¸¦åˆ—å‡¦ç†: ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æ´»ç”¨
3. ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›
4. ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ»åˆ¶å¾¡: è‡ªå‹•ãƒ¡ãƒ¢ãƒªç®¡ç†
"""

import pandas as pd
import numpy as np
import logging
import psutil
import time
import gc
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable, Iterator, Tuple
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import warnings

logger = logging.getLogger(__name__)

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®é–¢æ•°å®šç¾©ï¼ˆpickleå¯¾å¿œï¼‰
def _process_single_file_for_parallel(args_tuple):
    """ä¸¦åˆ—å‡¦ç†ç”¨ã®å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•°ï¼ˆå¼•æ•°ã‚’ã‚¿ãƒ—ãƒ«ã§å—ã‘å–ã‚‹ï¼‰"""
    file_path, process_func, output_dir, use_chunking, optimize_dtypes, memory_manager, dtype_optimizer, chunk_processor = args_tuple
    
    try:
        output_path = output_dir / file_path.name
        
        if use_chunking and chunk_processor:
            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®å ´åˆã€enhanced_process_funcã‚’ç›´æ¥å®šç¾©
            def enhanced_process_func(df: pd.DataFrame) -> pd.DataFrame:
                processed_df = process_func(df)
                if optimize_dtypes and dtype_optimizer:
                    optimized_df, _ = dtype_optimizer.optimize_dtypes(processed_df)
                    return optimized_df
                return processed_df
            
            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            return chunk_processor.process_csv_in_chunks(
                file_path, enhanced_process_func, output_path
            )
        else:
            # é€šå¸¸å‡¦ç†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–¢æ•°ã‚’ä½¿ã‚ãšã«ç›´æ¥å‡¦ç†ï¼‰
            df = pd.read_csv(file_path, encoding='utf-8')
            processed_df = process_func(df)
            
            # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
            if optimize_dtypes and dtype_optimizer:
                processed_df, _ = dtype_optimizer.optimize_dtypes(processed_df)
            
            processed_df.to_csv(output_path, index=False, encoding='utf-8')
            
            return {
                'success': True,
                'total_rows_processed': len(df),
                'total_rows_output': len(processed_df),
                'output_file': str(output_path)
            }
            
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file_path}: {str(e)}")
        return {'success': False, 'error': str(e), 'file': str(file_path)}


class MemoryManager:
    """
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–ãƒ»åˆ¶å¾¡ã‚¯ãƒ©ã‚¹
    å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ¡ãƒ¢ãƒªç®¡ç†æ©Ÿèƒ½
    """
    
    def __init__(self, 
                 memory_threshold_gb: float = 4.0,
                 warning_threshold_gb: float = 6.0,
                 emergency_threshold_gb: float = 8.0):
        """
        Args:
            memory_threshold_gb: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å¥¨é–¾å€¤ï¼ˆGBï¼‰
            warning_threshold_gb: è­¦å‘Šãƒ¬ãƒ™ãƒ«ã®é–¾å€¤ï¼ˆGBï¼‰
            emergency_threshold_gb: ç·Šæ€¥åœæ­¢ãƒ¬ãƒ™ãƒ«ã®é–¾å€¤ï¼ˆGBï¼‰
        """
        self.memory_threshold = memory_threshold_gb
        self.warning_threshold = warning_threshold_gb
        self.emergency_threshold = emergency_threshold_gb
        self.initial_memory = self._get_memory_usage()
        
    def _get_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆGBï¼‰ã‚’å–å¾—"""
        return psutil.virtual_memory().used / 1024 / 1024 / 1024
    
    def _get_available_memory(self) -> float:
        """åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªé‡ï¼ˆGBï¼‰ã‚’å–å¾—"""
        return psutil.virtual_memory().available / 1024 / 1024 / 1024
    
    def check_memory_status(self, stage_name: str = "") -> Dict[str, Any]:
        """
        ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
        
        Returns:
            ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹æƒ…å ±
        """
        current_memory = self._get_memory_usage()
        available_memory = self._get_available_memory()
        memory_diff = current_memory - self.initial_memory
        
        status = {
            'current_memory_gb': current_memory,
            'available_memory_gb': available_memory,
            'memory_diff_gb': memory_diff,
            'status': 'OK',
            'action_required': False,
            'recommendations': []
        }
        
        # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®åˆ¤å®š
        if current_memory >= self.emergency_threshold:
            status['status'] = 'EMERGENCY'
            status['action_required'] = True
            status['recommendations'].append('å³åº§ã«å‡¦ç†ã‚’åœæ­¢ã—ã¦ãã ã•ã„')
            logger.critical(f"ğŸš¨ [{stage_name}] ç·Šæ€¥: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ {current_memory:.1f}GB > {self.emergency_threshold:.1f}GB")
            
        elif current_memory >= self.warning_threshold:
            status['status'] = 'WARNING'
            status['action_required'] = True
            status['recommendations'].extend([
                'ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹',
                'ä¸¦åˆ—å‡¦ç†æ•°ã‚’æ¸›ã‚‰ã™',
                'ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹'
            ])
            logger.warning(f"âš ï¸ [{stage_name}] è­¦å‘Š: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ {current_memory:.1f}GB > {self.warning_threshold:.1f}GB")
            
        elif current_memory >= self.memory_threshold:
            status['status'] = 'CAUTION'
            status['recommendations'].append('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ã¦ãã ã•ã„')
            logger.info(f"ğŸ’¡ [{stage_name}] æ³¨æ„: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ {current_memory:.1f}GB > {self.memory_threshold:.1f}GB")
        
        else:
            logger.info(f"âœ… [{stage_name}] ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹è‰¯å¥½: {current_memory:.1f}GB (åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB)")
        
        return status
    
    def auto_cleanup(self, force_gc: bool = True) -> float:
        """
        è‡ªå‹•ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            force_gc: å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            
        Returns:
            è§£æ”¾ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªé‡ï¼ˆGBï¼‰
        """
        before_memory = self._get_memory_usage()
        
        if force_gc:
            logger.info("ğŸ§¹ ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
            gc.collect()
            
        after_memory = self._get_memory_usage()
        freed_memory = before_memory - after_memory
        
        if freed_memory > 0.1:  # 100MBä»¥ä¸Šè§£æ”¾ã•ã‚ŒãŸå ´åˆ
            logger.info(f"âœ… ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {freed_memory:.2f}GBè§£æ”¾")
        
        return freed_memory
    
    def suggest_chunk_size(self, 
                          data_size_mb: float, 
                          target_memory_gb: float = None) -> int:
        """
        æœ€é©ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’ææ¡ˆ
        
        Args:
            data_size_mb: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆMBï¼‰
            target_memory_gb: ç›®æ¨™ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆGBï¼‰
            
        Returns:
            æ¨å¥¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆè¡Œæ•°ï¼‰
        """
        if target_memory_gb is None:
            target_memory_gb = self.memory_threshold * 0.8  # 80%ã‚’ç›®å®‰
        
        available_memory = self._get_available_memory()
        safe_memory = min(target_memory_gb, available_memory * 0.7)  # 70%ã‚’å®‰å…¨åŸŸã¨ã™ã‚‹
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ããƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—
        chunk_memory_gb = safe_memory / 4  # å‡¦ç†ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’è€ƒæ…®
        chunk_size_mb = chunk_memory_gb * 1024
        
        # æ¨å®šè¡Œæ•°ï¼ˆ1è¡Œã‚ãŸã‚Šå¹³å‡1KBã¨ä»®å®šï¼‰
        estimated_chunk_rows = int(chunk_size_mb * 1000)
        
        # æœ€å°ãƒ»æœ€å¤§å€¤ã®è¨­å®š
        min_chunk_size = 1000
        max_chunk_size = 50000
        
        chunk_size = max(min_chunk_size, min(estimated_chunk_rows, max_chunk_size))
        
        logger.info(f"ğŸ’¡ æ¨å¥¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size:,}è¡Œ (ç›®æ¨™ãƒ¡ãƒ¢ãƒª: {safe_memory:.1f}GB)")
        
        return chunk_size

class ChunkProcessor:
    """
    ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¯ãƒ©ã‚¹
    å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦åŠ¹ç‡çš„ã«å‡¦ç†
    """
    
    def __init__(self, 
                 chunk_size: int = 10000,
                 memory_manager: MemoryManager = None):
        """
        Args:
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆè¡Œæ•°ï¼‰
            memory_manager: ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¯ãƒ©ã‚¹
        """
        self.chunk_size = chunk_size
        self.memory_manager = memory_manager or MemoryManager()
        
    def process_csv_in_chunks(self, 
                             file_path: Path,
                             process_func: Callable[[pd.DataFrame], pd.DataFrame],
                             output_path: Path,
                             **read_csv_kwargs) -> Dict[str, Any]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†
        
        Args:
            file_path: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            process_func: å‡¦ç†é–¢æ•°
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            **read_csv_kwargs: pandas.read_csv()ã®è¿½åŠ å¼•æ•°
            
        Returns:
            å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"ğŸ”„ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–‹å§‹: {file_path.name}")
        
        start_time = time.time()
        total_rows_processed = 0
        total_rows_output = 0
        chunk_count = 0
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®read_csvå¼•æ•°
        default_kwargs = {
            'encoding': 'utf-8',
            'chunksize': self.chunk_size
        }
        default_kwargs.update(read_csv_kwargs)
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆæœŸåŒ–ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æ›¸ãè¾¼ã¿ç”¨ï¼‰
        first_chunk = True
        
        try:
            # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã®èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†
            for chunk_df in pd.read_csv(file_path, **default_kwargs):
                chunk_count += 1
                chunk_start_time = time.time()
                
                logger.info(f"   ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯ {chunk_count}: {len(chunk_df):,}è¡Œå‡¦ç†ä¸­...")
                
                # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                memory_status = self.memory_manager.check_memory_status(f"ãƒãƒ£ãƒ³ã‚¯{chunk_count}")
                
                if memory_status['status'] == 'EMERGENCY':
                    logger.error("âŒ ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
                    break
                
                if memory_status['status'] == 'WARNING':
                    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
                    self.memory_manager.auto_cleanup()
                
                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè¡Œ
                try:
                    processed_chunk = process_func(chunk_df)
                    
                    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿
                    if first_chunk:
                        processed_chunk.to_csv(output_path, index=False, encoding='utf-8')
                        first_chunk = False
                    else:
                        processed_chunk.to_csv(output_path, mode='a', header=False, index=False, encoding='utf-8')
                    
                    total_rows_processed += len(chunk_df)
                    total_rows_output += len(processed_chunk)
                    
                    chunk_time = time.time() - chunk_start_time
                    logger.info(f"   âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk_count} å®Œäº†: {len(processed_chunk):,}è¡Œå‡ºåŠ› ({chunk_time:.1f}ç§’)")
                    
                except Exception as e:
                    logger.error(f"   âŒ ãƒãƒ£ãƒ³ã‚¯ {chunk_count} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå®šæœŸå®Ÿè¡Œï¼‰
                if chunk_count % 5 == 0:
                    self.memory_manager.auto_cleanup()
            
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'total_chunks': chunk_count,
                'total_rows_processed': total_rows_processed,
                'total_rows_output': total_rows_output,
                'processing_time_seconds': processing_time,
                'output_file': str(output_path)
            }
            
            logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†: {file_path.name}")
            logger.info(f"   ğŸ“Š å‡¦ç†ã‚µãƒãƒªãƒ¼: {chunk_count}ãƒãƒ£ãƒ³ã‚¯, {total_rows_processed:,}è¡Œ â†’ {total_rows_output:,}è¡Œ")
            logger.info(f"   â±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'total_chunks': chunk_count,
                'total_rows_processed': total_rows_processed
            }

class ParallelProcessor:
    """
    ä¸¦åˆ—å‡¦ç†ã‚¯ãƒ©ã‚¹
    ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ»ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®åŠ¹ç‡çš„å‡¦ç†
    """
    
    def __init__(self, 
                 max_workers: int = None,
                 memory_manager: MemoryManager = None):
        """
        Args:
            max_workers: æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•è¨­å®šï¼‰
            memory_manager: ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¯ãƒ©ã‚¹
        """
        if max_workers is None:
            # CPUã‚³ã‚¢æ•°ã¨ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’è€ƒæ…®ã—ã¦è‡ªå‹•è¨­å®š
            cpu_cores = cpu_count()
            available_memory = psutil.virtual_memory().available / 1024 / 1024 / 1024
            
            # ãƒ¡ãƒ¢ãƒª1GBã‚ãŸã‚Š1ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’åŸºæº–ã¨ã—ã€CPUã‚³ã‚¢æ•°ã‚‚è€ƒæ…®
            memory_based_workers = max(1, int(available_memory / 2))  # 2GBã‚ãŸã‚Š1ãƒ¯ãƒ¼ã‚«ãƒ¼
            max_workers = min(cpu_cores, memory_based_workers, 8)  # æœ€å¤§8ãƒ¯ãƒ¼ã‚«ãƒ¼
        
        self.max_workers = max_workers
        self.memory_manager = memory_manager or MemoryManager()
        
        logger.info(f"ğŸ”§ ä¸¦åˆ—å‡¦ç†è¨­å®š: {self.max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼ (CPU: {cpu_count()}ã‚³ã‚¢)")
    
    def process_files_parallel(self,
                              items: List[Any],
                              process_func: Callable[[Any], Any],
                              use_threading: bool = False) -> List[Any]:
        """
        è¤‡æ•°ã‚¢ã‚¤ãƒ†ãƒ ã®ä¸¦åˆ—å‡¦ç†ï¼ˆæ±ç”¨ç‰ˆï¼‰
        
        Args:
            items: å‡¦ç†å¯¾è±¡ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ä»»æ„ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
            process_func: å‡¦ç†é–¢æ•°
            use_threading: ThreadPoolExecutorã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆIOå‡¦ç†å‘ã‘ï¼‰
            
        Returns:
            å‡¦ç†çµæœãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸš€ ä¸¦åˆ—å‡¦ç†é–‹å§‹: {len(items)}ã‚¢ã‚¤ãƒ†ãƒ , {self.max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼")
        
        start_time = time.time()
        results = []
        
        # ä¸¦åˆ—å‡¦ç†ã®å®Ÿè¡Œ
        executor_class = ThreadPoolExecutor if use_threading else ProcessPoolExecutor
        
        try:
            with executor_class(max_workers=self.max_workers) as executor:
                # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã®æŠ•å…¥
                future_to_item = {
                    executor.submit(process_func, item): item 
                    for item in items
                }
                
                # çµæœã®å›å
                completed_count = 0
                for future in as_completed(future_to_item):
                    item = future_to_item[future]
                    completed_count += 1
                    
                    try:
                        result = future.result()
                        results.append(result)
                        
                        # ã‚¢ã‚¤ãƒ†ãƒ ã®è¡¨ç¤ºç”¨åå‰ã‚’å–å¾—
                        item_name = self._get_item_name(item)
                        logger.info(f"   âœ… å®Œäº† ({completed_count}/{len(items)}): {item_name}")
                        
                    except Exception as e:
                        item_name = self._get_item_name(item)
                        logger.error(f"   âŒ ã‚¨ãƒ©ãƒ¼ ({completed_count}/{len(items)}): {item_name} - {str(e)}")
                        results.append({'error': str(e), 'item': str(item)})
                    
                    # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ãƒã‚§ãƒƒã‚¯
                    if completed_count % max(1, len(items) // 4) == 0:  # 25%ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
                        memory_status = self.memory_manager.check_memory_status(f"ä¸¦åˆ—å‡¦ç† {completed_count}/{len(items)}")
                        if memory_status['status'] in ['WARNING', 'EMERGENCY']:
                            self.memory_manager.auto_cleanup()
            
            processing_time = time.time() - start_time
            
            logger.info(f"âœ… ä¸¦åˆ—å‡¦ç†å®Œäº†: {len(results)}ä»¶å‡¦ç†, {processing_time:.1f}ç§’")
            logger.info(f"   ğŸ“Š å¹³å‡å‡¦ç†æ™‚é–“: {processing_time/len(items):.2f}ç§’/ãƒ•ã‚¡ã‚¤ãƒ«")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _get_item_name(self, item) -> str:
        """ã‚¢ã‚¤ãƒ†ãƒ ã®è¡¨ç¤ºç”¨åå‰ã‚’å–å¾—"""
        if hasattr(item, 'name'):
            return str(item.name)
        elif isinstance(item, tuple) and len(item) > 0:
            # ã‚¿ãƒ—ãƒ«ã®å ´åˆã€æœ€åˆã®è¦ç´ ãŒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ä»®å®š
            first_item = item[0]
            if hasattr(first_item, 'name'):
                return str(first_item.name)
            else:
                return str(first_item)
        else:
            return str(item)

class DataTypeOptimizer:
    """
    ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
    """
    
    def __init__(self):
        # æœ€é©åŒ–ãƒ«ãƒ¼ãƒ«å®šç¾©
        self.optimization_rules = {
            'integer_columns': {
                'small': {'min': -128, 'max': 127, 'dtype': 'int8'},
                'medium': {'min': -32768, 'max': 32767, 'dtype': 'int16'},
                'large': {'min': -2147483648, 'max': 2147483647, 'dtype': 'int32'},
                'default': 'int64'
            },
            'float_columns': {
                'precision_threshold': 6,  # æœ‰åŠ¹æ¡æ•°6æ¡ä»¥ä¸‹ãªã‚‰float32
                'default_float': 'float32',
                'high_precision': 'float64'
            },
            'categorical_threshold': 0.5,  # 50%ä»¥ä¸‹ã®ä¸€æ„å€¤ç‡ã§ã‚«ãƒ†ã‚´ãƒªåŒ–
            'boolean_keywords': ['is_', 'has_', 'flag_', '_flg']
        }
    
    def optimize_dtypes(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–
        
        Args:
            df: æœ€é©åŒ–å¯¾è±¡ã®DataFrame
            
        Returns:
            æœ€é©åŒ–å¾Œã®DataFrame, æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
        """
        logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–é–‹å§‹...")
        
        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        optimization_report = {
            'original_memory_mb': original_memory,
            'optimizations': {},
            'errors': []
        }
        
        df_optimized = df.copy()
        
        try:
            # 1. æ•´æ•°åˆ—ã®æœ€é©åŒ–
            df_optimized, int_report = self._optimize_integer_columns(df_optimized)
            optimization_report['optimizations']['integers'] = int_report
            
            # 2. æµ®å‹•å°æ•°ç‚¹åˆ—ã®æœ€é©åŒ–
            df_optimized, float_report = self._optimize_float_columns(df_optimized)
            optimization_report['optimizations']['floats'] = float_report
            
            # 3. ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æœ€é©åŒ–
            df_optimized, cat_report = self._optimize_categorical_columns(df_optimized)
            optimization_report['optimizations']['categoricals'] = cat_report
            
            # 4. ãƒ–ãƒ¼ãƒ«åˆ—ã®æœ€é©åŒ–
            df_optimized, bool_report = self._optimize_boolean_columns(df_optimized)
            optimization_report['optimizations']['booleans'] = bool_report
            
            # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            memory_reduction = original_memory - optimized_memory
            reduction_rate = (memory_reduction / original_memory) * 100 if original_memory > 0 else 0
            
            optimization_report.update({
                'optimized_memory_mb': optimized_memory,
                'memory_reduction_mb': memory_reduction,
                'reduction_rate_percent': reduction_rate
            })
            
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–å®Œäº†:")
            logger.info(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {original_memory:.1f}MB â†’ {optimized_memory:.1f}MB")
            logger.info(f"   ğŸ“‰ å‰Šæ¸›é‡: {memory_reduction:.1f}MB ({reduction_rate:.1f}%å‰Šæ¸›)")
            
            return df_optimized, optimization_report
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            optimization_report['errors'].append(str(e))
            return df, optimization_report
    
    def _optimize_integer_columns(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:
        """æ•´æ•°åˆ—ã®æœ€é©åŒ–"""
        int_columns = df.select_dtypes(include=['int']).columns
        optimizations = {}
        
        for col in int_columns:
            if df[col].notna().sum() == 0:  # å…¨ã¦æ¬ æå€¤ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            try:
                col_min = df[col].min()
                col_max = df[col].max()
                
                # é©åˆ‡ãªintegerå‹ã‚’é¸æŠ
                rules = self.optimization_rules['integer_columns']
                new_dtype = rules['default']
                
                for size, rule in rules.items():
                    if size == 'default':
                        continue
                    if col_min >= rule['min'] and col_max <= rule['max']:
                        new_dtype = rule['dtype']
                        break
                
                if new_dtype != str(df[col].dtype):
                    df[col] = df[col].astype(new_dtype)
                    optimizations[col] = f"{df[col].dtype} â†’ {new_dtype}"
                    
            except Exception as e:
                logger.warning(f"æ•´æ•°åˆ— {col} ã®æœ€é©åŒ–ã«å¤±æ•—: {str(e)}")
        
        return df, optimizations
    
    def _optimize_float_columns(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:
        """æµ®å‹•å°æ•°ç‚¹åˆ—ã®æœ€é©åŒ–"""
        float_columns = df.select_dtypes(include=['float']).columns
        optimizations = {}
        
        for col in float_columns:
            if df[col].notna().sum() == 0:  # å…¨ã¦æ¬ æå€¤ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            try:
                # float32ã§ååˆ†ãªç²¾åº¦ã‹åˆ¤å®š
                original_dtype = str(df[col].dtype)
                
                if original_dtype == 'float64':
                    # float32ã«å¤‰æ›ã—ã¦ç²¾åº¦ã‚’ãƒã‚§ãƒƒã‚¯
                    test_series = df[col].astype('float32')
                    
                    # å…ƒã®å€¤ã¨ã®å·®ãŒè¨±å®¹ç¯„å›²å†…ã‹ç¢ºèª
                    if df[col].notna().sum() > 0:
                        max_diff = abs(df[col] - test_series).max()
                        relative_error = max_diff / abs(df[col]).max() if abs(df[col]).max() > 0 else 0
                        
                        if relative_error < 1e-6:  # ç›¸å¯¾èª¤å·®ãŒååˆ†å°ã•ã„
                            df[col] = test_series
                            optimizations[col] = f"float64 â†’ float32"
                            
            except Exception as e:
                logger.warning(f"æµ®å‹•å°æ•°ç‚¹åˆ— {col} ã®æœ€é©åŒ–ã«å¤±æ•—: {str(e)}")
        
        return df, optimizations
    
    def _optimize_categorical_columns(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æœ€é©åŒ–"""
        object_columns = df.select_dtypes(include=['object']).columns
        optimizations = {}
        
        for col in object_columns:
            if df[col].notna().sum() == 0:  # å…¨ã¦æ¬ æå€¤ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            try:
                # ä¸€æ„å€¤ã®å‰²åˆã‚’è¨ˆç®—
                unique_ratio = df[col].nunique() / len(df)
                
                if unique_ratio <= self.optimization_rules['categorical_threshold']:
                    df[col] = df[col].astype('category')
                    optimizations[col] = f"object â†’ category (ä¸€æ„å€¤ç‡: {unique_ratio:.2%})"
                    
            except Exception as e:
                logger.warning(f"ã‚«ãƒ†ã‚´ãƒªåˆ— {col} ã®æœ€é©åŒ–ã«å¤±æ•—: {str(e)}")
        
        return df, optimizations
    
    def _optimize_boolean_columns(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:
        """ãƒ–ãƒ¼ãƒ«åˆ—ã®æœ€é©åŒ–"""
        optimizations = {}
        
        # ãƒ–ãƒ¼ãƒ«ã£ã½ã„åˆ—åã‚’æ¤œå‡º
        boolean_candidates = []
        for col in df.columns:
            for keyword in self.optimization_rules['boolean_keywords']:
                if keyword in col.lower():
                    boolean_candidates.append(col)
                    break
        
        # 0,1ã®ã¿ã®æ•°å€¤åˆ—ã‚‚ãƒ–ãƒ¼ãƒ«å€™è£œ
        numeric_columns = df.select_dtypes(include=['int', 'float']).columns
        for col in numeric_columns:
            if df[col].notna().sum() > 0:
                unique_values = set(df[col].dropna().unique())
                if unique_values.issubset({0, 1, 0.0, 1.0}):
                    boolean_candidates.append(col)
        
        # ãƒ–ãƒ¼ãƒ«å‹ã«å¤‰æ›
        for col in boolean_candidates:
            if col in df.columns:
                try:
                    original_dtype = str(df[col].dtype)
                    df[col] = df[col].astype('bool')
                    optimizations[col] = f"{original_dtype} â†’ bool"
                    
                except Exception as e:
                    logger.warning(f"ãƒ–ãƒ¼ãƒ«åˆ— {col} ã®æœ€é©åŒ–ã«å¤±æ•—: {str(e)}")
        
        return df, optimizations

class PerformanceOptimizer:
    """
    çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹
    ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ãƒ»ä¸¦åˆ—å‡¦ç†ãƒ»ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã®çµ±åˆç®¡ç†
    """
    
    def __init__(self, 
                 auto_tune: bool = True,
                 memory_limit_gb: float = 6.0,
                 max_workers: int = None):
        """
        Args:
            auto_tune: è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æœ‰åŠ¹åŒ–
            memory_limit_gb: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ä¸Šé™
            max_workers: æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        self.auto_tune = auto_tune
        self.memory_manager = MemoryManager(
            memory_threshold_gb=memory_limit_gb * 0.6,
            warning_threshold_gb=memory_limit_gb * 0.8,
            emergency_threshold_gb=memory_limit_gb
        )
        self.chunk_processor = ChunkProcessor(memory_manager=self.memory_manager)
        self.parallel_processor = ParallelProcessor(max_workers=max_workers, memory_manager=self.memory_manager)
        self.dtype_optimizer = DataTypeOptimizer()
        
        logger.info("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def optimize_data_processing(self,
                                file_paths: List[Path],
                                process_func: Callable[[pd.DataFrame], pd.DataFrame],
                                output_dir: Path,
                                use_chunking: bool = None,
                                use_parallel: bool = None,
                                optimize_dtypes: bool = True) -> Dict[str, Any]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè¡Œ
        
        Args:
            file_paths: å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
            process_func: ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            use_chunking: ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ä½¿ç”¨ãƒ•ãƒ©ã‚°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
            use_parallel: ä¸¦åˆ—å‡¦ç†ä½¿ç”¨ãƒ•ãƒ©ã‚°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
            optimize_dtypes: ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–å®Ÿè¡Œãƒ•ãƒ©ã‚°
            
        Returns:
            å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
        """
        logger.info("ğŸ¯ æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
        
        start_time = time.time()
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
        if self.auto_tune:
            use_chunking, use_parallel = self._auto_tune_processing_strategy(file_paths)
        
        # å‡¦ç†æˆ¦ç•¥ã®æ±ºå®š
        if use_parallel and len(file_paths) > 1:
            logger.info("ğŸ”„ ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
            results = self._process_files_parallel(file_paths, process_func, output_dir, use_chunking, optimize_dtypes)
        else:
            logger.info("ğŸ“ é †æ¬¡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
            results = self._process_files_sequential(file_paths, process_func, output_dir, use_chunking, optimize_dtypes)
        
        # å‡¦ç†ã‚µãƒãƒªãƒ¼
        processing_time = time.time() - start_time
        
        summary = {
            'total_files': len(file_paths),
            'successful_files': sum(1 for r in results if r.get('success', False)),
            'failed_files': sum(1 for r in results if not r.get('success', False)),
            'total_processing_time': processing_time,
            'strategy': {
                'chunking': use_chunking,
                'parallel': use_parallel,
                'dtype_optimization': optimize_dtypes
            },
            'results': results
        }
        
        logger.info(f"âœ… æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†:")
        logger.info(f"   ğŸ“Š å‡¦ç†çµæœ: {summary['successful_files']}/{summary['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«æˆåŠŸ")
        logger.info(f"   â±ï¸ ç·å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
        
        return summary
    
    def _auto_tune_processing_strategy(self, file_paths: List[Path]) -> Tuple[bool, bool]:
        """å‡¦ç†æˆ¦ç•¥ã®è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‹ã‚‰æœ€é©æˆ¦ç•¥ã‚’æ±ºå®š
        total_size_mb = sum(f.stat().st_size for f in file_paths if f.exists()) / 1024 / 1024
        available_memory_gb = psutil.virtual_memory().available / 1024 / 1024 / 1024
        
        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®åˆ¤å®š
        use_chunking = total_size_mb > (available_memory_gb * 1024 * 0.3)  # åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã®30%ä»¥ä¸Š
        
        # ä¸¦åˆ—å‡¦ç†ã®åˆ¤å®š
        use_parallel = len(file_paths) > 1 and available_memory_gb > 2.0  # 2GBä»¥ä¸Šã®å ´åˆ
        
        logger.info(f"ğŸ”§ è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ:")
        logger.info(f"   ğŸ“Š ç·ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {total_size_mb:.1f}MB")
        logger.info(f"   ğŸ’¾ åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {available_memory_gb:.1f}GB")
        logger.info(f"   ğŸ”„ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {'æœ‰åŠ¹' if use_chunking else 'ç„¡åŠ¹'}")
        logger.info(f"   ğŸš€ ä¸¦åˆ—å‡¦ç†: {'æœ‰åŠ¹' if use_parallel else 'ç„¡åŠ¹'}")
        
        return use_chunking, use_parallel
    
    def _process_files_parallel(self, file_paths, process_func, output_dir, use_chunking, optimize_dtypes):
        """ä¸¦åˆ—å‡¦ç†ã§ã®å®Ÿè¡Œï¼ˆThreadPoolExecutorå¼·åˆ¶ä½¿ç”¨ã§pickleã‚¨ãƒ©ãƒ¼å›é¿ï¼‰"""
        # å¼•æ•°ã‚¿ãƒ—ãƒ«ã‚’æº–å‚™
        args_tuples = [
            (
                file_path,
                process_func,
                output_dir,
                use_chunking,
                optimize_dtypes,
                self.memory_manager,
                self.dtype_optimizer,
                self.chunk_processor
            )
            for file_path in file_paths
        ]
        
        # å¼·åˆ¶çš„ã«ThreadPoolExecutorã‚’ä½¿ç”¨ã—ã¦pickleã‚¨ãƒ©ãƒ¼ã‚’å›é¿
        return self.parallel_processor.process_files_parallel(
            args_tuples, 
            _process_single_file_for_parallel,
            use_threading=True  # å¼·åˆ¶çš„ã«Threadingã‚’ä½¿ç”¨
        )
    
    def _process_files_sequential(self, file_paths, process_func, output_dir, use_chunking, optimize_dtypes):
        """é †æ¬¡å‡¦ç†ã§ã®å®Ÿè¡Œ"""
        results = []
        for file_path in file_paths:
            result = self._process_single_file(file_path, process_func, output_dir, use_chunking, optimize_dtypes)
            results.append(result)
        return results
    
    def _process_single_file(self, file_path: Path, process_func, output_dir: Path, use_chunking: bool, optimize_dtypes: bool) -> Dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""
        try:
            output_path = output_dir / file_path.name
            
            # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã‚’å«ã‚€å‡¦ç†é–¢æ•°ã‚’ä½œæˆ
            def enhanced_process_func(df: pd.DataFrame) -> pd.DataFrame:
                processed_df = process_func(df)
                
                if optimize_dtypes:
                    optimized_df, _ = self.dtype_optimizer.optimize_dtypes(processed_df)
                    return optimized_df
                
                return processed_df
            
            if use_chunking:
                # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
                return self.chunk_processor.process_csv_in_chunks(
                    file_path, enhanced_process_func, output_path
                )
            else:
                # é€šå¸¸å‡¦ç†
                df = pd.read_csv(file_path, encoding='utf-8')
                processed_df = enhanced_process_func(df)
                processed_df.to_csv(output_path, index=False, encoding='utf-8')
                
                return {
                    'success': True,
                    'total_rows_processed': len(df),
                    'total_rows_output': len(processed_df),
                    'output_file': str(output_path)
                }
                
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file_path}: {str(e)}")
            return {'success': False, 'error': str(e), 'file': str(file_path)} 