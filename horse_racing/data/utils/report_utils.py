"""
ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
"""
import logging
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
from collections import defaultdict

logger = logging.getLogger(__name__)


def display_deletion_statistics():
    """ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æã«ã‚ˆã‚‹å‰Šé™¤çµ±è¨ˆã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    logger = logging.getLogger(__name__)
    
    try:
        def _count_csv_rows(file_path: Path) -> int:
            buffer_size = 1024 * 1024
            newline_count = 0
            last_char = b'\n'

            with file_path.open('rb') as f:
                while True:
                    chunk = f.read(buffer_size)
                    if not chunk:
                        break
                    newline_count += chunk.count(b'\n')
                    last_char = chunk[-1:]

            line_count = newline_count
            if last_char not in (b'\n', b''):
                line_count += 1

            return max(line_count - 1, 0)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        sed_dir = Path('export/SED/formatted')
        bias_dir = Path('export/dataset')
        
        if not sed_dir.exists() or not bias_dir.exists():
            logger.warning("âš ï¸ æ¯”è¼ƒç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        sed_files = list(sed_dir.glob('*.csv'))
        bias_files = list(bias_dir.glob('*.csv'))
        
        if not sed_files or not bias_files:
            logger.warning("âš ï¸ æ¯”è¼ƒç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # çµ±è¨ˆã‚’åé›†
        total_sed = 0
        total_bias = 0
        total_deleted = 0
        deletion_files = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒãƒƒãƒ”ãƒ³ã‚°
        sed_files_dict = {f.stem.replace('_formatted', ''): f for f in sed_files}
        
        for bias_file in bias_files:
            base_name = bias_file.stem.replace('_formatted_dataset', '')
            
            if base_name in sed_files_dict:
                sed_file = sed_files_dict[base_name]
                
                try:
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’æ•°ãˆã‚‹ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰
                    sed_count = _count_csv_rows(sed_file)
                    bias_count = _count_csv_rows(bias_file)
                    
                    deleted = sed_count - bias_count
                    total_sed += sed_count
                    total_bias += bias_count
                    total_deleted += deleted
                    
                    if deleted > 0:
                        deletion_rate = (deleted / sed_count * 100) if sed_count > 0 else 0
                        deletion_files.append({
                            'file': base_name,
                            'deleted': deleted,
                            'deletion_rate': deletion_rate
                        })
                
                except Exception:
                    continue
        
        # çµ±è¨ˆè¡¨ç¤º
        logger.info("ğŸ“ˆ å…¨ä½“å‰Šé™¤çµ±è¨ˆ:")
        logger.info(f"   ğŸ“¥ å‡¦ç†å‰ç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {total_sed:,}ä»¶")
        logger.info(f"   ğŸ“¤ å‡¦ç†å¾Œç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {total_bias:,}ä»¶")
        logger.info(f"   âŒ å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_deleted:,}ä»¶")
        logger.info(f"   ğŸ“‰ å…¨ä½“å‰Šé™¤ç‡: {(total_deleted/total_sed*100 if total_sed > 0 else 0):.2f}%")
        logger.info(f"   ğŸ—‚ï¸ å‰Šé™¤ç™ºç”Ÿãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(deletion_files)}")
        logger.info(f"   ğŸ“Š å‰Šé™¤ç™ºç”Ÿç‡: {(len(deletion_files)/len(sed_files_dict)*100 if sed_files_dict else 0):.1f}%")
        
        if deletion_files:
            logger.info("\nğŸ“‹ å‰Šé™¤ã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸Šä½10ä»¶ï¼‰:")
            deletion_files.sort(key=lambda x: x['deleted'], reverse=True)
            for i, item in enumerate(deletion_files[:10], 1):
                logger.info(f"   {i:2d}. {item['file']}: -{item['deleted']:,}ä»¶ (-{item['deletion_rate']:.1f}%)")
        else:
            logger.info("âœ… ã‚°ãƒ¬ãƒ¼ãƒ‰æ¬ æã«ã‚ˆã‚‹å‰Šé™¤ã¯ç™ºç”Ÿã—ã¦ã„ã¾ã›ã‚“")
    
    except Exception as e:
        logger.warning(f"âš ï¸ å‰Šé™¤çµ±è¨ˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(e)}")


def summarize_processing_log():
    """æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    logger = logging.getLogger(__name__)
    
    log_file = Path('export/missing_value_processing_log.txt')
    backup_file = Path('export/missing_value_processing_log_original.txt')
    summary_file = Path('export/missing_value_processing_summary.txt')
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if not log_file.exists():
        logger.info("ğŸ“ æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return
    
    logger.info("ğŸ“Š æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã‚’ã‚µãƒãƒªãƒ¼å½¢å¼ã«æ•´ç†ä¸­...")
    
    try:
        # ãƒ­ã‚°è§£æ
        stats = _parse_processing_log(log_file)
        
        if not stats:
            logger.warning("âš ï¸ ãƒ­ã‚°è§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        _generate_summary_report(stats, summary_file)
        
        # å…ƒãƒ­ã‚°ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if backup_file.exists():
            backup_file.unlink()  # æ—¢å­˜ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤
        log_file.rename(backup_file)
        
        # ã‚µãƒãƒªãƒ¼ã‚’æ–°ã—ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«
        summary_file.rename(log_file)
        
        logger.info("âœ… æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã®æ•´ç†å®Œäº†")
        logger.info(f"   ğŸ“‹ ã‚µãƒãƒªãƒ¼: {log_file}")
        logger.info(f"   ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
        logger.info(f"   ğŸ“Š å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
        if stats['idm_deletions']:
            total_idm = sum(stats['idm_deletions'])
            logger.info(f"   ğŸ¯ IDMå‰Šé™¤: {total_idm:,}è¡Œ ({len(stats['idm_deletions'])}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
        if stats['grade_estimations']:
            total_grade = sum(stats['grade_estimations'])
            logger.info(f"   ğŸ† ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š: {total_grade:,}ä»¶ ({len(stats['grade_estimations'])}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ­ã‚°ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")


def _parse_processing_log(log_file: Path) -> Optional[Dict[str, Any]]:
    """æ¬ æå€¤å‡¦ç†ãƒ­ã‚°ã‚’è§£æã—ã¦çµ±è¨ˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        log_file (Path): è§£æå¯¾è±¡ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚

    Returns:
        Optional[Dict[str, Any]]: ãƒ­ã‚°è§£æçµæœã®çµ±è¨ˆæƒ…å ±ã€‚
    """
    logger = logging.getLogger(__name__)
    
    # çµ±è¨ˆæƒ…å ±æ ¼ç´ç”¨
    stats = {
        'idm_deletions': [],
        'grade_estimations': [],
        'median_imputations': defaultdict(list),
        'dropped_columns': set(),
        'categorical_imputations': defaultdict(list),
        'other_imputations': defaultdict(list),
        'total_files': 0,
        'final_shapes': []
    }
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        logger.error(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}
    
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('==') or line.startswith('æ¬ æå€¤å‡¦ç†ãƒ­ã‚°'):
            continue
            
        # IDMå‰Šé™¤
        if 'IDM:' in line and 'è¡Œã‚’å‰Šé™¤ï¼ˆé‡è¦åˆ—ï¼‰' in line:
            match = re.search(r'IDM: (\d+)è¡Œã‚’å‰Šé™¤', line)
            if match:
                stats['idm_deletions'].append(int(match.group(1)))
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®š
        elif 'ã‚°ãƒ¬ãƒ¼ãƒ‰:' in line and 'æ¨å®šâ†’ã‚°ãƒ¬ãƒ¼ãƒ‰ååˆ—è¿½åŠ ' in line:
            match = re.search(r'ã‚°ãƒ¬ãƒ¼ãƒ‰: è³é‡‘ãƒ»ãƒ¬ãƒ¼ã‚¹åã‹ã‚‰(\d+)ä»¶æ¨å®š', line)
            if match:
                stats['grade_estimations'].append(int(match.group(1)))
        
        # ä¸­å¤®å€¤è£œå®Œ
        elif 'medianã§' in line and 'ä»¶è£œå®Œ' in line:
            match = re.search(r'â€¢ ([^:]+): medianã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                count = int(match.group(2))
                stats['median_imputations'][column_name].append(count)
        
        # é«˜æ¬ æç‡ã«ã‚ˆã‚‹åˆ—å‰Šé™¤
        elif 'é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤' in line:
            match = re.search(r'â€¢ ([^:]+): é«˜æ¬ æç‡ã«ã‚ˆã‚Šåˆ—å‰Šé™¤', line)
            if match:
                stats['dropped_columns'].add(match.group(1))
        
        # ã‚«ãƒ†ã‚´ãƒªè£œå®Œï¼ˆãƒ¬ãƒ¼ã‚¹åã€é¦¬ä½“é‡å¢—æ¸›ï¼‰
        elif line.startswith('â€¢ ãƒ¬ãƒ¼ã‚¹å:') or line.startswith('â€¢ ãƒ¬ãƒ¼ã‚¹åç•¥ç§°:') or line.startswith('â€¢ é¦¬ä½“é‡å¢—æ¸›:'):
            match = re.search(r'â€¢ ([^:]+): (.+)ã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                value = match.group(2)
                count = int(match.group(3))
                stats['categorical_imputations'][column_name].append((value, count))
        
        # ãã®ä»–ã®è£œå®Œå‡¦ç†
        elif 'ä»¶è£œå®Œ' in line and 'median' not in line:
            match = re.search(r'â€¢ ([^:]+): (.+)ã§(\d+)ä»¶è£œå®Œ', line)
            if match:
                column_name = match.group(1)
                value = match.group(2)
                count = int(match.group(3))
                stats['other_imputations'][column_name].append((value, count))
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
        elif 'æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:' in line:
            match = re.search(r'æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: \((\d+), (\d+)\)', line)
            if match:
                rows = int(match.group(1))
                cols = int(match.group(2))
                stats['final_shapes'].append((rows, cols))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ¨å®šï¼ˆIDMå‰Šé™¤ã®å›æ•°ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šã®å›æ•°ã®åˆè¨ˆï¼‰
    stats['total_files'] = len(stats['idm_deletions']) + len(stats['grade_estimations'])
    
    return stats


def _generate_summary_report(stats: Dict[str, Any], output_file: Path):
    """çµ±è¨ˆæƒ…å ±ã‹ã‚‰ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        stats (Dict[str, Any]): ãƒ­ã‚°è§£æã«ã‚ˆã£ã¦å¾—ã‚‰ã‚ŒãŸçµ±è¨ˆæƒ…å ±ã€‚
        output_file (Path): å‡ºåŠ›å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
    """
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ğŸ“Š æ¬ æå€¤å‡¦ç†ãƒ­ã‚° ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆï¼ˆå®Ÿå‹™ãƒ¬ãƒ™ãƒ«ï¼‰\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        f.write(f"ğŸ“ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        
        # IDMå‰Šé™¤çµ±è¨ˆ
        if stats['idm_deletions']:
            total_idm = sum(stats['idm_deletions'])
            f.write("ğŸ¯ IDMæ¬ æå€¤å‰Šé™¤å‡¦ç†:\n")
            f.write(f"   â€¢ å‡¦ç†å›æ•°: {len(stats['idm_deletions'])}å›\n")
            f.write(f"   â€¢ ç·å‰Šé™¤è¡Œæ•°: {total_idm:,}è¡Œ\n")
            f.write(f"   â€¢ å¹³å‡å‰Šé™¤è¡Œæ•°: {total_idm/len(stats['idm_deletions']):.1f}è¡Œ\n\n")
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šçµ±è¨ˆ
        if stats['grade_estimations']:
            total_grade = sum(stats['grade_estimations'])
            f.write("ğŸ† ã‚°ãƒ¬ãƒ¼ãƒ‰æ¨å®šå‡¦ç†:\n")
            f.write(f"   â€¢ å‡¦ç†å›æ•°: {len(stats['grade_estimations'])}å›\n")
            f.write(f"   â€¢ ç·æ¨å®šä»¶æ•°: {total_grade:,}ä»¶\n")
            f.write(f"   â€¢ å¹³å‡æ¨å®šä»¶æ•°: {total_grade/len(stats['grade_estimations']):.1f}ä»¶\n\n")
        
        # ä¸­å¤®å€¤è£œå®Œçµ±è¨ˆ
        if stats['median_imputations']:
            f.write("ğŸ”¢ ä¸­å¤®å€¤è£œå®Œå‡¦ç†:\n")
            for column, counts in stats['median_imputations'].items():
                total_count = sum(counts)
                f.write(f"   â€¢ {column}: {len(counts)}å›, ç·è£œå®Œ{total_count:,}ä»¶ (å¹³å‡{total_count/len(counts):.1f}ä»¶)\n")
            f.write("\n")
        
        # é«˜æ¬ æç‡åˆ—å‰Šé™¤
        if stats['dropped_columns']:
            f.write("âŒ é«˜æ¬ æç‡ã«ã‚ˆã‚Šå‰Šé™¤ã•ã‚ŒãŸåˆ—:\n")
            sorted_columns = sorted(stats['dropped_columns'])
            for i, column in enumerate(sorted_columns, 1):
                f.write(f"   {i:2d}. {column}\n")
            f.write(f"\n   ğŸ“Š å‰Šé™¤åˆ—æ•°: {len(sorted_columns)}åˆ—\n\n")
        
        # ã‚«ãƒ†ã‚´ãƒªè£œå®Œçµ±è¨ˆ
        if stats['categorical_imputations']:
            f.write("ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªè£œå®Œå‡¦ç†:\n")
            for column, values in stats['categorical_imputations'].items():
                total_count = sum(count for _, count in values)
                unique_values = len(set(value for value, _ in values))
                f.write(f"   â€¢ {column}: {len(values)}å›, ç·è£œå®Œ{total_count:,}ä»¶, {unique_values}ç¨®é¡ã®å€¤\n")
            f.write("\n")
        
        # ãã®ä»–è£œå®Œçµ±è¨ˆ
        if stats['other_imputations']:
            f.write("ğŸ”§ ãã®ä»–è£œå®Œå‡¦ç†:\n")
            for column, values in stats['other_imputations'].items():
                total_count = sum(count for _, count in values)
                f.write(f"   â€¢ {column}: {len(values)}å›, ç·è£œå®Œ{total_count:,}ä»¶\n")
            f.write("\n")
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        if stats['final_shapes']:
            total_rows = sum(rows for rows, _ in stats['final_shapes'])
            total_cols = sum(cols for _, cols in stats['final_shapes'])
            avg_rows = total_rows / len(stats['final_shapes']) if stats['final_shapes'] else 0
            avg_cols = total_cols / len(stats['final_shapes']) if stats['final_shapes'] else 0
            
            f.write("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\n")
            f.write(f"   â€¢ ç·è¡Œæ•°: {total_rows:,}è¡Œ\n")
            f.write(f"   â€¢ å¹³å‡è¡Œæ•°: {avg_rows:.1f}è¡Œ/ãƒ•ã‚¡ã‚¤ãƒ«\n")
            f.write(f"   â€¢ å¹³å‡åˆ—æ•°: {avg_cols:.1f}åˆ—/ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ‰ å®Ÿå‹™ãƒ¬ãƒ™ãƒ«æ¬ æå€¤å‡¦ç† å®Œäº†ã‚µãƒãƒªãƒ¼\n")
        f.write("=" * 80 + "\n")

