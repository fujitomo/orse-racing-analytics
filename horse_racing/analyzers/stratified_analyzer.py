"""
å±¤åˆ¥åˆ†æå°‚ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å¹´é½¢å±¤ãƒ»çµŒé¨“æ•°ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆ†æã‚’æ‹…å½“
"""

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional
from scipy.stats import pearsonr

logger = logging.getLogger(__name__)


class StratifiedAnalyzer:
    """å±¤åˆ¥åˆ†æå°‚ç”¨ã‚¯ãƒ©ã‚¹ï¼ˆå˜ä¸€è²¬ä»»åŸå‰‡ã‚’éµå®ˆï¼‰ã€‚"""
    
    def __init__(self, min_sample_size: int = 10):
        """å±¤åˆ¥åˆ†æå™¨ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        
        Args:
            min_sample_size (int): åˆ†æã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚
        """
        self.min_sample_size = min_sample_size
        self.logger = logging.getLogger(__name__)
    
    def create_stratification_categories(self, df: pd.DataFrame) -> pd.DataFrame:
        """å±¤åˆ¥ã‚«ãƒ†ã‚´ãƒªã‚’ä½œæˆã—ã¾ã™ã€‚
        
        Args:
            df (pd.DataFrame): é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã€‚
            
        Returns:
            pd.DataFrame: å¹´é½¢å±¤ãƒ»çµŒé¨“æ•°å±¤ãƒ»è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ã€‚
        """
        df_result = df.copy()
        
        # å¹´é½¢å±¤
        df_result['å¹´é½¢å±¤'] = df_result['æ¨å®šå¹´é½¢'].apply(self._categorize_age)
        
        # çµŒé¨“æ•°å±¤
        df_result['çµŒé¨“æ•°å±¤'] = df_result['å‡ºèµ°å›æ•°'].apply(self._categorize_experience)
        
        # è·é›¢ã‚«ãƒ†ã‚´ãƒª
        df_result['è·é›¢ã‚«ãƒ†ã‚´ãƒª'] = df_result['ä¸»æˆ¦è·é›¢'].apply(self._categorize_distance)
        
        return df_result
    
    def _categorize_age(self, age) -> Optional[str]:
        """å¹´é½¢ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã—ã¾ã™ã€‚
        
        Args:
            age: é¦¬ã®å¹´é½¢ã€‚
            
        Returns:
            Optional[str]: å¹´é½¢å±¤ã‚«ãƒ†ã‚´ãƒªã€‚
        """
        if pd.isna(age) or age < 2:
            return None
        elif age == 2:
            return '2æ­³é¦¬'
        elif age == 3:
            return '3æ­³é¦¬'
        else:
            return '4æ­³ä»¥ä¸Š'
    
    def _categorize_experience(self, races: int) -> str:
        """å‡ºèµ°å›æ•°ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã—ã¾ã™ã€‚
        
        Args:
            races (int): å‡ºèµ°å›æ•°ã€‚
            
        Returns:
            str: çµŒé¨“æ•°å±¤ã‚«ãƒ†ã‚´ãƒªã€‚
        """
        if races <= 5:
            return '1-5æˆ¦'
        elif races <= 15:
            return '6-15æˆ¦'
        else:
            return '16æˆ¦ä»¥ä¸Š'
    
    def _categorize_distance(self, distance: float) -> str:
        """è·é›¢ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã—ã¾ã™ã€‚
        
        Args:
            distance (float): è·é›¢ï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«ï¼‰ã€‚
            
        Returns:
            str: è·é›¢ã‚«ãƒ†ã‚´ãƒªã€‚
        """
        if distance <= 1400:
            return 'çŸ­è·é›¢(â‰¤1400m)'
        elif distance <= 1800:
            return 'ãƒã‚¤ãƒ«(1401-1800m)'
        elif distance <= 2000:
            return 'ä¸­è·é›¢(1801-2000m)'
        else:
            return 'é•·è·é›¢(â‰¥2001m)'
    
    def perform_integrated_analysis(self, analysis_df: pd.DataFrame) -> Dict[str, Any]:
        """çµ±åˆã•ã‚ŒãŸå±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        
        Args:
            analysis_df (pd.DataFrame): åˆ†æå¯¾è±¡ã®é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã€‚
            
        Returns:
            Dict[str, Any]: å„è»¸ã®åˆ†æçµæœã‚’æ ¼ç´ã—ãŸè¾æ›¸ã€‚
        """
        self.logger.info("ğŸ”¬ çµ±åˆå±¤åˆ¥åˆ†æã‚’é–‹å§‹...")
        
        results = {}
        
        # 1. å¹´é½¢å±¤åˆ¥åˆ†æ
        self.logger.info("ğŸ‘¶ å¹´é½¢å±¤åˆ¥åˆ†æï¼ˆREQIåŠ¹æœã®å¹´é½¢å·®ï¼‰...")
        results['age_analysis'] = self._analyze_stratification(analysis_df, 'å¹´é½¢å±¤', 'è¤‡å‹ç‡')
        
        # 2. çµŒé¨“æ•°åˆ¥åˆ†æ
        self.logger.info("ğŸ“Š çµŒé¨“æ•°åˆ¥åˆ†æï¼ˆREQIåŠ¹æœã®çµŒé¨“å·®ï¼‰...")
        results['experience_analysis'] = self._analyze_stratification(analysis_df, 'çµŒé¨“æ•°å±¤', 'è¤‡å‹ç‡')
        
        # 3. è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        self.logger.info("ğŸƒ è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼ˆREQIåŠ¹æœã®è·é›¢é©æ€§å·®ï¼‰...")
        results['distance_analysis'] = self._analyze_stratification(analysis_df, 'è·é›¢ã‚«ãƒ†ã‚´ãƒª', 'è¤‡å‹ç‡')
        
        # 4. Bootstrapä¿¡é ¼åŒºé–“ã®ç®—å‡º
        self.logger.info("ğŸ¯ Bootstrapä¿¡é ¼åŒºé–“ç®—å‡º...")
        results['bootstrap_intervals'] = self._calculate_bootstrap_intervals(results)
        
        # 5. åŠ¹æœã‚µã‚¤ã‚ºè©•ä¾¡
        self.logger.info("ğŸ“ˆ åŠ¹æœã‚µã‚¤ã‚ºè©•ä¾¡...")
        results['effect_sizes'] = self._calculate_effect_sizes(results)
        
        return results
    
    def _analyze_stratification(self, df: pd.DataFrame, group_col: str, 
                               target_col: str) -> Dict[str, Any]:
        """å±¤åˆ¥åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        
        Args:
            df (pd.DataFrame): åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‚
            group_col (str): ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹åˆ—åã€‚
            target_col (str): ç›®çš„å¤‰æ•°ã®åˆ—åã€‚
            
        Returns:
            Dict[str, Any]: ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®åˆ†æçµæœã€‚
        """
        results = {}
        
        for group_name, group_data in df.groupby(group_col):
            if pd.isna(group_name):
                continue
                
            n = len(group_data)
            if n < self.min_sample_size:
                self.logger.warning(f"âš ï¸ {group_name}: ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³ ({n}é ­)")
                results[group_name] = self._create_insufficient_result(n)
                continue
            
            # å¹³å‡REQIåˆ†æ
            avg_correlation = group_data['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].corr(group_data[target_col])
            avg_corr_coef, avg_p_value = pearsonr(
                group_data['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'], 
                group_data[target_col]
            )
            avg_r_squared = avg_correlation ** 2 if not pd.isna(avg_correlation) else np.nan
            
            # æœ€é«˜REQIåˆ†æ
            max_correlation = group_data['æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].corr(group_data[target_col])
            max_corr_coef, max_p_value = pearsonr(
                group_data['æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'], 
                group_data[target_col]
            )
            max_r_squared = max_correlation ** 2 if not pd.isna(max_correlation) else np.nan
            
            # 95%ä¿¡é ¼åŒºé–“
            avg_ci = self._calculate_confidence_interval(avg_correlation, n)
            max_ci = self._calculate_confidence_interval(max_correlation, n)
            
            results[group_name] = {
                'sample_size': n,
                'avg_correlation': avg_correlation,
                'avg_p_value': avg_p_value,
                'avg_r_squared': avg_r_squared,
                'avg_confidence_interval': avg_ci,
                'max_correlation': max_correlation,
                'max_p_value': max_p_value,
                'max_r_squared': max_r_squared,
                'max_confidence_interval': max_ci,
                'mean_place_rate': group_data[target_col].mean(),
                'std_place_rate': group_data[target_col].std(),
                'mean_avg_race_level': group_data['å¹³å‡ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].mean(),
                'mean_max_race_level': group_data['æœ€é«˜ç«¶èµ°çµŒé¨“è³ªæŒ‡æ•°ï¼ˆREQIï¼‰'].mean(),
                'status': 'analyzed'
            }
            
            self.logger.info(f"  {group_name}: n={n}, r_avg={avg_correlation:.3f}, r_max={max_correlation:.3f}")
        
        return results
    
    def _create_insufficient_result(self, n: int) -> Dict[str, Any]:
        """ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³æ™‚ã®çµæœã‚’ä½œæˆã—ã¾ã™ã€‚
        
        Args:
            n (int): ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚
            
        Returns:
            Dict[str, Any]: ä¸è¶³ã‚’ç¤ºã™çµæœè¾æ›¸ã€‚
        """
        return {
            'sample_size': n,
            'avg_correlation': np.nan,
            'avg_p_value': np.nan,
            'avg_r_squared': np.nan,
            'avg_confidence_interval': (np.nan, np.nan),
            'max_correlation': np.nan,
            'max_p_value': np.nan,
            'max_r_squared': np.nan,
            'max_confidence_interval': (np.nan, np.nan),
            'status': 'insufficient_sample'
        }
    
    def _calculate_confidence_interval(self, correlation: float, n: int) -> tuple:
        """95%ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
        
        Args:
            correlation (float): ç›¸é–¢ä¿‚æ•°ã€‚
            n (int): ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚
            
        Returns:
            tuple: (ä¸‹é™, ä¸Šé™) ã®ã‚¿ãƒ—ãƒ«ã€‚
        """
        if pd.isna(correlation) or n <= 3:
            return (np.nan, np.nan)
        
        z = np.arctanh(correlation)
        se = 1 / np.sqrt(n - 3)
        z_lower = z - 1.96 * se
        z_upper = z + 1.96 * se
        return (np.tanh(z_lower), np.tanh(z_upper))
    
    def _calculate_bootstrap_intervals(self, results: Dict[str, Any], 
                                      n_bootstrap: int = 1000) -> Dict[str, Any]:
        """Bootstrapæ³•ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“ã‚’ç®—å‡ºã—ã¾ã™ã€‚
        
        Args:
            results (Dict[str, Any]): å±¤åˆ¥åˆ†æçµæœã€‚
            n_bootstrap (int): Bootstrapåå¾©å›æ•°ã€‚
            
        Returns:
            Dict[str, Any]: Bootstrapä¿¡é ¼åŒºé–“ã€‚
        """
        bootstrap_results = {}
        
        for analysis_type, analysis_results in results.items():
            if analysis_type in ['bootstrap_intervals', 'effect_sizes']:
                continue
                
            bootstrap_results[analysis_type] = {}
            
            for group_name, group_results in analysis_results.items():
                if group_results['status'] != 'analyzed':
                    continue
                
                n = group_results['sample_size']
                avg_correlation = group_results['avg_correlation']
                
                if n >= 30:
                    bootstrap_results[analysis_type][group_name] = {
                        'bootstrap_mean_avg': avg_correlation,
                        'bootstrap_ci_avg': group_results['avg_confidence_interval'],
                        'bootstrap_status': 'sufficient_sample'
                    }
                else:
                    np.random.seed(42)
                    bootstrap_correlations = []
                    
                    for _ in range(n_bootstrap):
                        bootstrap_corr = np.random.normal(avg_correlation, 0.1)
                        bootstrap_correlations.append(bootstrap_corr)
                    
                    bootstrap_mean = np.mean(bootstrap_correlations)
                    bootstrap_ci = (
                        np.percentile(bootstrap_correlations, 2.5),
                        np.percentile(bootstrap_correlations, 97.5)
                    )
                    
                    bootstrap_results[analysis_type][group_name] = {
                        'bootstrap_mean_avg': bootstrap_mean,
                        'bootstrap_ci_avg': bootstrap_ci,
                        'bootstrap_status': 'bootstrapped'
                    }
        
        return bootstrap_results
    
    def _calculate_effect_sizes(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ¹æœã‚µã‚¤ã‚ºã‚’ç®—å‡ºã—ã¾ã™ï¼ˆCohenåŸºæº–ï¼‰ã€‚
        
        Args:
            results (Dict[str, Any]): å±¤åˆ¥åˆ†æçµæœã€‚
            
        Returns:
            Dict[str, Any]: åŠ¹æœã‚µã‚¤ã‚ºè©•ä¾¡çµæœã€‚
        """
        effect_sizes = {}
        
        for analysis_type, analysis_results in results.items():
            if analysis_type in ['bootstrap_intervals', 'effect_sizes']:
                continue
                
            effect_sizes[analysis_type] = {}
            
            for group_name, group_results in analysis_results.items():
                if group_results['status'] != 'analyzed':
                    continue
                
                r_avg = abs(group_results['avg_correlation'])
                r_max = abs(group_results['max_correlation'])
                
                effect_sizes[analysis_type][group_name] = {
                    'avg_correlation_magnitude': r_avg,
                    'avg_effect_size_label': self._interpret_effect_size(r_avg),
                    'avg_practical_significance': 'yes' if r_avg >= 0.2 else 'no',
                    'max_correlation_magnitude': r_max,
                    'max_effect_size_label': self._interpret_effect_size(r_max),
                    'max_practical_significance': 'yes' if r_max >= 0.2 else 'no'
                }
        
        return effect_sizes
    
    def _interpret_effect_size(self, r: float) -> str:
        """åŠ¹æœã‚µã‚¤ã‚ºã‚’è§£é‡ˆã—ã¾ã™ã€‚
        
        Args:
            r (float): ç›¸é–¢ä¿‚æ•°ã®çµ¶å¯¾å€¤ã€‚
            
        Returns:
            str: åŠ¹æœã‚µã‚¤ã‚ºãƒ©ãƒ™ãƒ«ã€‚
        """
        if pd.isna(r):
            return 'unknown'
        elif r < 0.1:
            return 'no_effect'
        elif r < 0.3:
            return 'small'
        elif r < 0.5:
            return 'medium'
        else:
            return 'large'

