"""
ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒ¬ãƒ¼ã‚¹ã®ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚„è³é‡‘é¡ãªã©ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’åˆ†æã—ã¾ã™ã€‚
"""

from typing import Dict, Any
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from horse_racing.base.analyzer import BaseAnalyzer, AnalysisConfig
from horse_racing.data.loader import RaceDataLoader
from horse_racing.visualization.plotter import RacePlotter
from horse_racing.analyzers.causal_analyzer import analyze_causal_relationship, generate_causal_analysis_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score, mean_squared_error
from scipy import stats
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.multitest import multipletests
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
import logging
from pathlib import Path
import warnings

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’èª¿æ•´
loader_logger = logging.getLogger('horse_racing.data.loader')
loader_logger.setLevel(logging.WARNING)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams['font.family'] = 'MS Gothic'
mpl.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 12

class RaceLevelAnalyzer(BaseAnalyzer):
    """ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã‚¯ãƒ©ã‚¹"""

    # ã‚°ãƒ¬ãƒ¼ãƒ‰å®šç¾©
    GRADE_LEVELS = {
        1: {"name": "G1", "weight": 10.0, "base_level": 9},
        2: {"name": "G2", "weight": 8.0, "base_level": 8},
        3: {"name": "G3", "weight": 7.0, "base_level": 7},
        4: {"name": "é‡è³", "weight": 6.0, "base_level": 6},
        5: {"name": "ç‰¹åˆ¥", "weight": 5.0, "base_level": 5},
        6: {"name": "L", "weight": 5.5, "base_level": 5.5}
    }

    # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«è¨ˆç®—ã®é‡ã¿ä»˜ã‘å®šç¾©
    LEVEL_WEIGHTS = {
        "grade_weight": 0.50,
        "venue_weight": 0.20,
        "prize_weight": 0.30,
        "field_size_weight": 0.10,
        "competition_weight": 0.20,
    }

    def __init__(self, config: AnalysisConfig, enable_time_analysis: bool = False):
        """åˆæœŸåŒ–"""
        super().__init__(config)
        self.plotter = RacePlotter(self.output_dir)
        self.loader = RaceDataLoader(config.input_path)
        self.class_column = None  # å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹ã‚«ãƒ©ãƒ åã‚’å‹•çš„ã«è¨­å®š
        self.time_analysis_results = {}  # ã‚¿ã‚¤ãƒ åˆ†æçµæœã‚’ä¿å­˜
        self.enable_time_analysis = enable_time_analysis  # RunningTimeåˆ†æã®æœ‰åŠ¹/ç„¡åŠ¹

    def load_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        try:
            return self.loader.load()
        except FileNotFoundError as e:
            logger.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def preprocess_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        try:
            df = self.df.copy()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ§‹é€ ã‚’ç¢ºèª
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ ä¸€è¦§:")
            logger.info(df.columns.tolist())
            logger.info("\nãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­5è¡Œ:")
            logger.info(df.head())

            # ã‚«ãƒ©ãƒ åã®å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
            df.columns = df.columns.str.strip()

            # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.config.start_date or self.config.end_date:
                # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®ä½œæˆ
                try:
                    if 'å¹´æœˆæ—¥' in df.columns:
                        df['date'] = pd.to_datetime(df['å¹´æœˆæ—¥'].astype(str), format='%Y%m%d')
                    else:
                        # å¹´æœˆæ—¥ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯å¹´ã€å›ã€æ—¥ã‹ã‚‰ä½œæˆ
                        df['date'] = pd.to_datetime(
                            df['å¹´'].astype(str) + 
                            df['å›'].astype(str).str.zfill(2) + 
                            df['æ—¥'].astype(str).str.zfill(2)
                        )
                except Exception as e:
                    logger.error(f"æ—¥ä»˜ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                    raise

                # æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.config.start_date:
                    df = df[df['date'] >= self.config.start_date]
                if self.config.end_date:
                    df = df[df['date'] <= self.config.end_date]

            # æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.config.min_races:
                # é¦¬ã”ã¨ã®ãƒ¬ãƒ¼ã‚¹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                race_counts = df['é¦¬å'].value_counts()
                valid_horses = race_counts[race_counts >= self.config.min_races].index
                df = df[df['é¦¬å'].isin(valid_horses)]

                if len(df) == 0:
                    raise ValueError(f"æ¡ä»¶ã‚’æº€ãŸã™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆæœ€å°ãƒ¬ãƒ¼ã‚¹æ•°: {self.config.min_races}ï¼‰")

            logger.info(f"  ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(df):,}è¡Œ")
            logger.info(f"  ğŸ å¯¾è±¡é¦¬æ•°: {df['é¦¬å'].nunique():,}é ­")

            return df

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def calculate_feature(self) -> pd.DataFrame:
        """ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—"""
        df = self.df.copy()
        
        # ã‚«ãƒ©ãƒ åã®å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        df.columns = df.columns.str.strip()
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’é¸æŠï¼ˆå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ åã«åŸºã¥ãï¼‰
        base_required_columns = [
            'å ´ã‚³ãƒ¼ãƒ‰', 'å¹´', 'å›', 'æ—¥', 'R', 'é¦¬å', 'è·é›¢', 'ç€é †',
            'ãƒ¬ãƒ¼ã‚¹å', 'ç¨®åˆ¥', 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰', 'é¦¬ç•ª',
            'æœ¬è³é‡‘', '1ç€è³é‡‘', 'å¹´æœˆæ—¥', 'å ´å',
            '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', '2ç€è³é‡‘(2ç€ç®—å…¥è³é‡‘è¾¼ã¿)', 'å¹³å‡è³é‡‘'
        ]
        
        # ã‚¿ã‚¤ãƒ é–¢é€£ã‚«ãƒ©ãƒ ã®è¿½åŠ 
        time_columns = []
        for col in ['ã‚¿ã‚¤ãƒ ', 'time', 'Time', 'èµ°ç ´ã‚¿ã‚¤ãƒ ']:
            if col in df.columns:
                time_columns.append(col)
                break
        
        # ã‚¯ãƒ©ã‚¹é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’å‹•çš„ã«è¿½åŠ ã¨åˆ¤å®š
        class_columns = []
        for col in ['ã‚¯ãƒ©ã‚¹', 'ã‚¯ãƒ©ã‚¹ã‚³ãƒ¼ãƒ‰', 'æ¡ä»¶']:
            if col in df.columns:
                class_columns.append(col)
                if self.class_column is None:  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¯ãƒ©ã‚¹é–¢é€£ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
                    self.class_column = col
        
        required_columns = base_required_columns + class_columns + time_columns
        
        # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã‚’é¸æŠ
        available_columns = [col for col in required_columns if col in df.columns]
        df = df[available_columns]

        # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«é–¢é€£ã®ç‰¹å¾´é‡ã‚’è¿½åŠ 
        df["race_level"] = 0.0
        df["is_win"] = df["ç€é †"] == 1
        df["is_placed"] = df["ç€é †"] <= 3

        # åŸºæœ¬ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—
        df["grade_level"] = self._calculate_grade_level(df)
        df["venue_level"] = self._calculate_venue_level(df)
        df["prize_level"] = self._calculate_prize_level(df)
        
        # RunningTimeåˆ†ææ©Ÿèƒ½ã‚’è¿½åŠ ï¼ˆæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
        if self.enable_time_analysis:
            df = self.calculate_running_time_features(df)

        return df

    def calculate_running_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """èµ°ç ´ã‚¿ã‚¤ãƒ é–¢é€£ç‰¹å¾´é‡ã®è¨ˆç®—"""
        try:
            logger.info("ğŸƒ èµ°ç ´ã‚¿ã‚¤ãƒ ç‰¹å¾´é‡ã®è¨ˆç®—ã‚’é–‹å§‹...")
            
            # ã‚¿ã‚¤ãƒ ã‚«ãƒ©ãƒ ã®ç‰¹å®š
            time_column = None
            for col in ['ã‚¿ã‚¤ãƒ ', 'time', 'Time', 'èµ°ç ´ã‚¿ã‚¤ãƒ ']:
                if col in df.columns:
                    time_column = col
                    break
            
            if time_column is None:
                logger.warning("âš ï¸ ã‚¿ã‚¤ãƒ ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚RunningTimeåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return df
            
            logger.info(f"ğŸ“Š ä½¿ç”¨ã™ã‚‹ã‚¿ã‚¤ãƒ ã‚«ãƒ©ãƒ : {time_column}")
            
            # ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
            df[time_column] = pd.to_numeric(df[time_column], errors='coerce')
            
            # ç•°å¸¸å€¤ã®é™¤å»ï¼ˆ0ç§’ã‚„æ¥µç«¯ã«é…ã„ã‚¿ã‚¤ãƒ ï¼‰
            valid_time_mask = (df[time_column] > 60) & (df[time_column] < 600)  # 1åˆ†ã€œ10åˆ†ã®ç¯„å›²
            df = df[valid_time_mask].copy()
            
            logger.info(f"ğŸ“Š æœ‰åŠ¹ãªã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
            
            # 1. è·é›¢è£œæ­£ã‚¿ã‚¤ãƒ ã®è¨ˆç®—ï¼ˆ2000mæ›ç®—ï¼‰
            df['distance_adjusted_time'] = df[time_column] / df['è·é›¢'] * 2000
            
            # 2. åŒæ¡ä»¶å†…ã§ã®Z-scoreæ­£è¦åŒ–
            grouping_columns = ['å ´ã‚³ãƒ¼ãƒ‰', 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰']
            available_grouping = [col for col in grouping_columns if col in df.columns]
            
            if available_grouping:
                df['time_zscore'] = df.groupby(available_grouping)[time_column].transform(
                    lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0
                )
                logger.info(f"ğŸ“Š Z-scoreæ­£è¦åŒ–å®Œäº†ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–: {available_grouping}ï¼‰")
            else:
                # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã§ããªã„å ´åˆã¯å…¨ä½“ã§Z-score
                df['time_zscore'] = (df[time_column] - df[time_column].mean()) / df[time_column].std()
                logger.info("ğŸ“Š Z-scoreæ­£è¦åŒ–å®Œäº†ï¼ˆå…¨ä½“å¹³å‡ï¼‰")
            
            # 3. é€Ÿåº¦æŒ‡æ¨™ã®è¨ˆç®—ï¼ˆm/åˆ†ï¼‰
            df['speed_index'] = df['è·é›¢'] / df[time_column] * 60
            
            # 4. è·é›¢åˆ¥åŸºæº–ã‚¿ã‚¤ãƒ ã¨ã®æ¯”è¼ƒ
            df['time_ratio'] = df.groupby('è·é›¢')[time_column].transform(
                lambda x: df.loc[x.index, time_column] / x.mean()
            )
            
            # 5. ã‚¿ã‚¤ãƒ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆåŒãƒ¬ãƒ¼ã‚¹å†…ï¼‰
            df['time_rank_in_race'] = df.groupby(['å ´ã‚³ãƒ¼ãƒ‰', 'å¹´', 'å›', 'æ—¥', 'R'])[time_column].rank(method='min')
            
            logger.info("âœ… èµ°ç ´ã‚¿ã‚¤ãƒ ç‰¹å¾´é‡ã®è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸ")
            logger.info(f"   - distance_adjusted_time: è·é›¢è£œæ­£ã‚¿ã‚¤ãƒ ï¼ˆ2000mæ›ç®—ï¼‰")
            logger.info(f"   - time_zscore: Z-scoreæ­£è¦åŒ–ã‚¿ã‚¤ãƒ ")
            logger.info(f"   - speed_index: é€Ÿåº¦æŒ‡æ¨™ï¼ˆm/åˆ†ï¼‰")
            logger.info(f"   - time_ratio: è·é›¢åˆ¥åŸºæº–ã‚¿ã‚¤ãƒ æ¯”")
            logger.info(f"   - time_rank_in_race: ãƒ¬ãƒ¼ã‚¹å†…ã‚¿ã‚¤ãƒ ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ èµ°ç ´ã‚¿ã‚¤ãƒ ç‰¹å¾´é‡è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return df

    def analyze_time_causality(self) -> Dict[str, Any]:
        """ã‚¿ã‚¤ãƒ é–¢é€£ã®å› æœåˆ†æ"""
        try:
            logger.info("ğŸ”¬ ã‚¿ã‚¤ãƒ å› æœåˆ†æã‚’é–‹å§‹...")
            
            results = {}
            
            # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            analysis_data = self.df.dropna(subset=['race_level', 'time_zscore', 'is_placed'])
            
            if len(analysis_data) == 0:
                logger.warning("âš ï¸ åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return {}
            
            logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(analysis_data):,}ä»¶")
            
            # 1. ä»®èª¬H1ã®æ¤œè¨¼: RaceLevel â†’ RunningTime
            h1_results = self.verify_hypothesis_h1(analysis_data)
            results['hypothesis_h1'] = h1_results
            
            # 2. ä»®èª¬H4ã®æ¤œè¨¼: RunningTime â†’ PlaceRate
            h4_results = self.verify_hypothesis_h4(analysis_data)
            results['hypothesis_h4'] = h4_results
            
            # 3. ç·åˆçš„ãªå› æœé–¢ä¿‚åˆ†æ
            comprehensive_results = self._analyze_comprehensive_causality(analysis_data)
            results['comprehensive_analysis'] = comprehensive_results
            
            self.time_analysis_results = results
            logger.info("âœ… ã‚¿ã‚¤ãƒ å› æœåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ã‚¿ã‚¤ãƒ å› æœåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}

    def verify_hypothesis_h1(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        ä»®èª¬H1ã®æ¤œè¨¼: RaceLevelï¼ˆãƒ¬ãƒ¼ã‚¹æ ¼ï¼‰ãŒé«˜ã„ã»ã©ã€èµ°ç ´ã‚¿ã‚¤ãƒ ãŒé€Ÿããªã‚‹ï¼ˆè·é›¢è£œæ­£æ¸ˆã¿ï¼‰
        è·é›¢ãƒ»é¦¬å ´çŠ¶æ…‹ã‚’çµ±åˆ¶ã—ãŸå¤šå¤‰é‡å›å¸°åˆ†æ
        """
        try:
            logger.info("ğŸ§ª ä»®èª¬H1æ¤œè¨¼: RaceLevel â†’ RunningTime")
            
            # èª¬æ˜å¤‰æ•°ã®æº–å‚™
            X = data[['race_level']].copy()
            
            # è·é›¢ã‚«ãƒ†ã‚´ãƒªã®è¿½åŠ ï¼ˆçµ±åˆ¶å¤‰æ•°ï¼‰
            data['distance_category'] = pd.cut(data['è·é›¢'], 
                                             bins=[0, 1400, 1800, 2000, 2400, 9999],
                                             labels=['sprint', 'mile', 'middle', 'long', 'extra_long'])
            
            # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ãƒ€ãƒŸãƒ¼åŒ–
            distance_dummies = pd.get_dummies(data['distance_category'], prefix='dist')
            X = pd.concat([X, distance_dummies], axis=1)
            
            # é¦¬å ´çŠ¶æ…‹ã®çµ±åˆ¶ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            if 'é¦¬å ´çŠ¶æ…‹' in data.columns:
                track_dummies = pd.get_dummies(data['é¦¬å ´çŠ¶æ…‹'], prefix='track')
                X = pd.concat([X, track_dummies], axis=1)
            
            # ç›®çš„å¤‰æ•°ï¼ˆã‚¿ã‚¤ãƒ ãŒé€Ÿã„æ–¹ãŒè² ã®å€¤ã«ãªã‚‹ãŸã‚ã€ç¬¦å·ã‚’åè»¢ï¼‰
            y = -data['time_zscore']  # é€Ÿã„ã‚¿ã‚¤ãƒ  = é«˜ã„å€¤
            
            # å›å¸°åˆ†æã®å®Ÿè¡Œ
            model = LinearRegression()
            model.fit(X, y)
            
            y_pred = model.predict(X)
            r2 = r2_score(y, y_pred)
            mse = mean_squared_error(y, y_pred)
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œå®š
            correlation = data['race_level'].corr(-data['time_zscore'])
            n = len(data)
            t_stat = correlation * np.sqrt((n - 2) / (1 - correlation**2))
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 2))
            
            results = {
                'model': model,
                'r2_score': r2,
                'mse': mse,
                'correlation': correlation,
                'p_value': p_value,
                'sample_size': n,
                'is_significant': p_value < 0.05,
                'effect_direction': 'positive' if correlation > 0 else 'negative',
                'interpretation': self._interpret_h1_results(correlation, p_value, r2)
            }
            
            logger.info(f"   ğŸ“Š ç›¸é–¢ä¿‚æ•°: {correlation:.3f}")
            logger.info(f"   ğŸ“Š æ±ºå®šä¿‚æ•°: {r2:.3f}")
            logger.info(f"   ğŸ“Š på€¤: {p_value:.6f}")
            logger.info(f"   ğŸ“Š çµ±è¨ˆçš„æœ‰æ„æ€§: {'æœ‰æ„' if p_value < 0.05 else 'éæœ‰æ„'}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä»®èª¬H1æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}

    def verify_hypothesis_h4(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        ä»®èª¬H4ã®æ¤œè¨¼: RunningTime ãŒé€Ÿã„ã»ã©è¤‡å‹ç‡ãŒé«˜ã„
        ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æï¼ˆè·é›¢ãƒ»é¦¬å ´ãƒã‚¤ã‚¢ã‚¹èª¿æ•´ï¼‰
        """
        try:
            logger.info("ğŸ§ª ä»®èª¬H4æ¤œè¨¼: RunningTime â†’ PlaceRate")
            
            # èª¬æ˜å¤‰æ•°ã®æº–å‚™
            X = data[['time_zscore', 'race_level']].copy()
            
            # è·é›¢ã®çµ±åˆ¶
            X['distance'] = data['è·é›¢']
            
            # ç›®çš„å¤‰æ•°
            y = data['is_placed']
            
            # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®å®Ÿè¡Œ
            model = LogisticRegression(random_state=42)
            model.fit(X, y)
            
            y_pred = model.predict(X)
            y_pred_proba = model.predict_proba(X)[:, 1]
            
            # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
            accuracy = accuracy_score(y, y_pred)
            
            # ã‚ªãƒƒã‚ºæ¯”ã®è¨ˆç®—
            odds_ratios = np.exp(model.coef_[0])
            
            # ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—ï¼ˆtime_zscoreãŒè² ã®å€¤ãªã®ã§ç¬¦å·ã‚’èª¿æ•´ï¼‰
            correlation = (-data['time_zscore']).corr(data['is_placed'])
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œå®š
            n = len(data)
            t_stat = correlation * np.sqrt((n - 2) / (1 - correlation**2))
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 2))
            
            results = {
                'model': model,
                'accuracy': accuracy,
                'odds_ratios': odds_ratios,
                'correlation': correlation,
                'p_value': p_value,
                'sample_size': n,
                'is_significant': p_value < 0.05,
                'predictions': y_pred_proba,
                'interpretation': self._interpret_h4_results(correlation, p_value, odds_ratios[0])
            }
            
            logger.info(f"   ğŸ“Š ç›¸é–¢ä¿‚æ•°: {correlation:.3f}")
            logger.info(f"   ğŸ“Š ç²¾åº¦: {accuracy:.3f}")
            logger.info(f"   ğŸ“Š ã‚¿ã‚¤ãƒ ã®ã‚ªãƒƒã‚ºæ¯”: {odds_ratios[0]:.3f}")
            logger.info(f"   ğŸ“Š på€¤: {p_value:.6f}")
            logger.info(f"   ğŸ“Š çµ±è¨ˆçš„æœ‰æ„æ€§: {'æœ‰æ„' if p_value < 0.05 else 'éæœ‰æ„'}\n")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä»®èª¬H4æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}

    def _analyze_comprehensive_causality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªå› æœé–¢ä¿‚åˆ†æ"""
        try:
            logger.info("ğŸ”¬ åŒ…æ‹¬çš„å› æœé–¢ä¿‚åˆ†æã‚’å®Ÿè¡Œ...")
            
            results = {}
            
            # 1. RaceLevel â†’ Time â†’ PlaceRate ã®åª’ä»‹åŠ¹æœåˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
            # ã‚¹ãƒ†ãƒƒãƒ—1: RaceLevel â†’ PlaceRate ã®ç›´æ¥åŠ¹æœ
            direct_corr = data['race_level'].corr(data['is_placed'])
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: RaceLevel â†’ Time ã®åŠ¹æœ
            race_time_corr = data['race_level'].corr(-data['time_zscore'])
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: Time â†’ PlaceRate ã®åŠ¹æœï¼ˆRaceLevelã‚’çµ±åˆ¶ï¼‰
            partial_corr = self._calculate_partial_correlation(
                data['time_zscore'], data['is_placed'], data['race_level']
            )
            
            # åª’ä»‹åŠ¹æœã®æ¨å®š
            indirect_effect = race_time_corr * partial_corr
            direct_effect_controlled = direct_corr - indirect_effect
            
            mediation_results = {
                'total_effect': direct_corr,
                'direct_effect': direct_effect_controlled,
                'indirect_effect': indirect_effect,
                'mediation_ratio': indirect_effect / direct_corr if direct_corr != 0 else 0
            }
            
            results['mediation_analysis'] = mediation_results
            
            # 2. è·é›¢åˆ¥ã®åŠ¹æœåˆ†æ
            distance_effects = {}
            distance_categories = ['sprint', 'mile', 'middle', 'long']
            
            for category in distance_categories:
                if category == 'sprint':
                    mask = data['è·é›¢'] <= 1400
                elif category == 'mile':
                    mask = (data['è·é›¢'] > 1400) & (data['è·é›¢'] <= 1800)
                elif category == 'middle':
                    mask = (data['è·é›¢'] > 1800) & (data['è·é›¢'] <= 2400)
                else:  # long
                    mask = data['è·é›¢'] > 2400
                
                if mask.sum() > 10:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                    subset = data[mask]
                    corr_level_time = subset['race_level'].corr(-subset['time_zscore'])
                    corr_time_place = (-subset['time_zscore']).corr(subset['is_placed'])
                    
                    distance_effects[category] = {
                        'sample_size': len(subset),
                        'race_level_time_correlation': corr_level_time,
                        'time_place_correlation': corr_time_place
                    }
            
            results['distance_specific_effects'] = distance_effects
            
            logger.info("âœ… åŒ…æ‹¬çš„å› æœé–¢ä¿‚åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ åŒ…æ‹¬çš„å› æœé–¢ä¿‚åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}

    def _calculate_partial_correlation(self, x, y, control_var):
        """åç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—"""
        try:
            # xã¨control_varã®å›å¸°æ®‹å·®
            model_x = LinearRegression()
            model_x.fit(control_var.values.reshape(-1, 1), x)
            residual_x = x - model_x.predict(control_var.values.reshape(-1, 1))
            
            # yã¨control_varã®å›å¸°æ®‹å·®
            model_y = LinearRegression()
            model_y.fit(control_var.values.reshape(-1, 1), y)
            residual_y = y - model_y.predict(control_var.values.reshape(-1, 1))
            
            # æ®‹å·®é–“ã®ç›¸é–¢ä¿‚æ•°
            return pd.Series(residual_x).corr(pd.Series(residual_y))
            
        except Exception:
            return 0.0

    def _perform_logistic_regression_analysis(self) -> Dict[str, Any]:
        """ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æã‚’å®Ÿè¡Œ"""
        df = self.df.copy()
        df['is_win_or_place'] = df['ç€é †'].apply(lambda x: 1 if x in [1, 2] else 0)
        df['is_placed_only'] = df['ç€é †'].apply(lambda x: 1 if x <= 3 else 0)
        
        # NAå€¤ã¨ç„¡é™å¤§å€¤ã®å‡¦ç†
        df['race_level'] = df['race_level'].fillna(0)
        df['race_level'] = df['race_level'].replace([np.inf, -np.inf], df['race_level'].replace([np.inf, -np.inf], np.nan).max())
        
        # å‹ç‡ã®ãƒ¢ãƒ‡ãƒ«
        X_win = df[['race_level']].values
        y_win = df['is_win_or_place'].values
        
        # è¤‡å‹ç‡ã®ãƒ¢ãƒ‡ãƒ«
        X_place = df[['race_level']].values
        y_place = df['is_placed_only'].values
        
        # æ¨™æº–åŒ–
        scaler_win = StandardScaler()
        X_win_scaled = scaler_win.fit_transform(X_win)
        
        scaler_place = StandardScaler()
        X_place_scaled = scaler_place.fit_transform(X_place)
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        X_win_train, X_win_test, y_win_train, y_win_test = train_test_split(
            X_win_scaled, y_win, test_size=0.3, random_state=42, stratify=y_win
        )
        
        X_place_train, X_place_test, y_place_train, y_place_test = train_test_split(
            X_place_scaled, y_place, test_size=0.3, random_state=42, stratify=y_place
        )
        
        # ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆå‹ç‡ï¼‰
        model_win = LogisticRegression(
            random_state=42,
            max_iter=1000,
            class_weight='balanced',
            solver='liblinear'
        )
        model_win.fit(X_win_train, y_win_train)
        
        # ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆè¤‡å‹ç‡ï¼‰
        model_place = LogisticRegression(
            random_state=42,
            max_iter=1000,
            class_weight='balanced',
            solver='liblinear'
        )
        model_place.fit(X_place_train, y_place_train)
        
        # äºˆæ¸¬ã¨è©•ä¾¡ï¼ˆå‹ç‡ï¼‰
        y_win_pred = model_win.predict(X_win_test)
        accuracy_win = accuracy_score(y_win_test, y_win_pred)
        report_win = classification_report(y_win_test, y_win_pred, zero_division=0)
        conf_matrix_win = confusion_matrix(y_win_test, y_win_pred)
        
        # äºˆæ¸¬ã¨è©•ä¾¡ï¼ˆè¤‡å‹ç‡ï¼‰
        y_place_pred = model_place.predict(X_place_test)
        accuracy_place = accuracy_score(y_place_test, y_place_pred)
        report_place = classification_report(y_place_test, y_place_pred, zero_division=0)
        conf_matrix_place = confusion_matrix(y_place_test, y_place_pred)
        
        return {
            "win": {
                "model": model_win,
                "scaler": scaler_win,
                "accuracy": accuracy_win,
                "report": report_win,
                "conf_matrix": conf_matrix_win,
            },
            "place": {
                "model": model_place,
                "scaler": scaler_place,
                "accuracy": accuracy_place,
                "report": report_place,
                "conf_matrix": conf_matrix_place,
            },
            "data": df
        }

    def analyze(self) -> Dict[str, Any]:
        """åˆ†æã®å®Ÿè¡Œ"""
        try:
            # --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿæ§‹ã®å°å…¥ ---
            cache_path = Path(self.config.output_dir) / 'horse_stats_cache.pkl'
            
            if cache_path.exists():
                logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚èª­ã¿è¾¼ã¿ã¾ã™: {cache_path}")
                horse_stats = pd.read_pickle(cache_path)
            else:
                logger.info("â„¹ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—ã—ã¾ã™...")
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ§‹é€ ã‚’ç¢ºèª
                logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ ä¸€è¦§:")
                logger.info(self.df.columns.tolist())
                logger.info("\nãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­5è¡Œ:")
                logger.info(self.df.head())
                horse_stats = self._calculate_horse_stats()
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
                horse_stats.to_pickle(cache_path)
                logger.info(f"ğŸ’¾ é¦¬ã”ã¨ã®çµ±è¨ˆæƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ: {cache_path}")
            
            # --- è¤‡æ•°é‡ã¿ä»˜ã‘æ‰‹æ³•ã®æ¯”è¼ƒã¨é¸æŠ ---
            logger.info("âš–ï¸ æœ€é©ãªé‡ã¿ä»˜ã‘æ‰‹æ³•ã‚’é¸æŠä¸­...")
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            horse_stats_for_weights = horse_stats.dropna(subset=['å¹³å‡ãƒ¬ãƒ™ãƒ«', 'å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'place_rate'])
            prize_level_stats = self.df.groupby('é¦¬å')['prize_level'].mean().reset_index()
            horse_stats_for_weights = pd.merge(horse_stats_for_weights, prize_level_stats, on='é¦¬å')
            
            # è¤‡æ•°ã®é‡ã¿ä»˜ã‘æ‰‹æ³•ã‚’å®Ÿè£…ãƒ»æ¯”è¼ƒ
            weighting_methods = self._compare_all_weighting_methods(horse_stats_for_weights)
            
            # æœ€è‰¯æ‰‹æ³•ã‚’é¸æŠï¼ˆRÂ²ãŒæœ€ã‚‚é«˜ã„æ‰‹æ³•ï¼‰
            best_method = max(weighting_methods.items(), key=lambda x: x[1].get('r_squared', 0))
            best_method_name, best_results = best_method
            
            logger.info(f"ğŸ† é¸æŠã•ã‚ŒãŸé‡ã¿ä»˜ã‘æ‰‹æ³•: {best_method_name}")
            logger.info(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ¨™ - RÂ²: {best_results.get('r_squared', 0):.6f}, ç›¸é–¢: {best_results.get('correlation', 0):.6f}")
            
            # å„æ‰‹æ³•ã®çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info("ğŸ“‹ å…¨é‡ã¿ä»˜ã‘æ‰‹æ³•ã®æ¯”è¼ƒçµæœ:")
            for method_name, results in weighting_methods.items():
                r2 = results.get('r_squared', 0)
                corr = results.get('correlation', 0)
                logger.info(f"  {method_name}: RÂ²={r2:.6f}, ç›¸é–¢={corr:.6f}")
            
            dynamic_weights = best_results.get('weights', {"grade_weight": 1/3, "venue_weight": 1/3, "prize_weight": 1/3})
            
            # æ€§èƒ½å‘ä¸Šã‚’è¨˜éŒ²
            baseline_r2 = weighting_methods.get('correlation_squared', {}).get('r_squared', 0)
            best_r2 = best_results.get('r_squared', 0)
            if baseline_r2 > 0:
                improvement = ((best_r2 - baseline_r2) / baseline_r2) * 100
                logger.info(f"ğŸš€ æ€§èƒ½å‘ä¸Š: {improvement:.1f}% (RÂ² {baseline_r2:.6f} â†’ {best_r2:.6f})")
            
            logger.info(f"ğŸ“Š æœ€çµ‚é¸æŠé‡ã¿: {dynamic_weights}")
            
            # --- race_level ã®å†è¨ˆç®— ---
            logger.info("ğŸ”„ å‹•çš„é‡ã¿ã‚’ç”¨ã„ã¦race_levelã‚’å†è¨ˆç®—ä¸­...")
            self.df['race_level'] = (
                self.df['grade_level'] * dynamic_weights['grade_weight'] +
                self.df['venue_level'] * dynamic_weights['venue_weight'] +
                self.df['prize_level'] * dynamic_weights['prize_weight']
            )
            
            # è·é›¢ã«ã‚ˆã‚‹è£œæ­£ã‚’é©ç”¨
            distance_weights = {
                (0, 1400): 0.85, (1401, 1800): 1.00, (1801, 2000): 1.35,
                (2001, 2400): 1.45, (2401, 9999): 1.25
            }
            for (min_dist, max_dist), weight in distance_weights.items():
                mask = (self.df["è·é›¢"] >= min_dist) & (self.df["è·é›¢"] <= max_dist)
                self.df.loc[mask, "race_level"] *= weight

            # æœ€çµ‚çš„ãªæ­£è¦åŒ–
            self.df["race_level"] = self.normalize_values(self.df["race_level"])

            # --- å†è¨ˆç®—å¾Œã®çµ±è¨ˆæƒ…å ±ã§åˆ†æ ---
            logger.info("ğŸ”„ å†è¨ˆç®—å¾Œã®çµ±è¨ˆæƒ…å ±ã§æœ€çµ‚åˆ†æã‚’å®Ÿè¡Œä¸­...")
            final_horse_stats = self._calculate_horse_stats()

            # === ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£æ¤œè¨¼ã‚’è¿½åŠ  ===
            logger.info("ğŸ” ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")
            multicollinearity_results = self.validate_multicollinearity()

            # åŸºæœ¬çš„ãªç›¸é–¢åˆ†æ
            correlation_stats = self._perform_correlation_analysis(final_horse_stats)
            results = {
                'correlation_stats': correlation_stats, 
                'dynamic_weights': dynamic_weights,
                'multicollinearity_results': multicollinearity_results
            }
            
            # RunningTimeåˆ†æã®å®Ÿè¡Œï¼ˆæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
            if self.enable_time_analysis:
                time_analysis_results = self.analyze_time_causality()
                if time_analysis_results:
                    results['time_analysis'] = time_analysis_results
                    logger.info("âœ… RunningTimeåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            else:
                time_analysis_results = None
            
            # å› æœé–¢ä¿‚åˆ†æã®è¿½åŠ 
            # causal_results = analyze_causal_relationship(self.df)
            # results['causal_analysis'] = causal_results
            
            # å› æœé–¢ä¿‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
            output_dir = Path(self.config.output_dir)
            # generate_causal_analysis_report(causal_results, output_dir)
            
            # RunningTimeåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
            if time_analysis_results:
                self._generate_time_analysis_report(time_analysis_results, output_dir)
            
            logger.info("âœ… å…¨ã¦ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            return results
            
        except Exception as e:
            logger.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def visualize(self) -> None:
        """åˆ†æçµæœã®å¯è¦–åŒ–"""
        try:
            if not self.stats:
                raise ValueError("åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«analyzeãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

            output_dir = Path(self.config.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            # ç›¸é–¢åˆ†æã®å¯è¦–åŒ–
            # self.plotter._visualize_correlations(self._calculate_horse_stats(), self.stats['correlation_stats'])
            logger.warning("âš ï¸ 'ä¸»æˆ¦ã‚¯ãƒ©ã‚¹'ã®KeyErrorã®ãŸã‚ã€ç›¸é–¢åˆ†æã®å¯è¦–åŒ–ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚")
            
            # ãƒ¬ãƒ¼ã‚¹æ ¼åˆ¥ãƒ»è·é›¢åˆ¥ã®ç®±ã²ã’å›³åˆ†æï¼ˆè«–æ–‡è¦æ±‚å¯¾å¿œï¼‰
            logger.info("ğŸ“Š ãƒ¬ãƒ¼ã‚¹æ ¼åˆ¥ãƒ»è·é›¢åˆ¥ã®ç®±ã²ã’å›³åˆ†æã‚’å®Ÿè¡Œä¸­...")
            self.plotter.plot_race_grade_distance_boxplot(self.df)
            logger.info("âœ… ç®±ã²ã’å›³åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            # RunningTimeåˆ†æã®å¯è¦–åŒ–
            if 'time_analysis' in self.stats:
                self._visualize_time_analysis()
                logger.info("âœ… RunningTimeåˆ†æã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            # å› æœé–¢ä¿‚åˆ†æã®å¯è¦–åŒ–
            # if 'causal_analysis' in self.stats:
            #     self._visualize_causal_analysis()

        except Exception as e:
            logger.error(f"å¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def _visualize_causal_analysis(self) -> None:
        """å› æœé–¢ä¿‚åˆ†æã®å¯è¦–åŒ–"""
        causal_results = self.stats.get('causal_analysis', {})
        output_dir = Path(self.config.output_dir) / 'causal_analysis'
        output_dir.mkdir(parents=True, exist_ok=True)

        # æ™‚é–“çš„å…ˆè¡Œæ€§ã®å¯è¦–åŒ–
        if 'temporal_precedence' in causal_results:
            self._plot_temporal_precedence(output_dir)

        # ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å¯è¦–åŒ–
        if 'mechanism' in causal_results:
            self._plot_mechanism_analysis(output_dir)

        # äº¤çµ¡å› å­ã®å¯è¦–åŒ–
        if 'confounding_factors' in causal_results:
            self._plot_confounding_factors(output_dir)

    def _plot_temporal_precedence(self, output_dir: Path) -> None:
        """æ™‚é–“çš„å…ˆè¡Œæ€§ã®å¯è¦–åŒ–"""
        plt.figure(figsize=(10, 6))
        horse_data = []

        for horse in self.df['é¦¬å'].unique():
            horse_races = self.df[self.df['é¦¬å'] == horse].sort_values('å¹´æœˆæ—¥')
            if len(horse_races) >= 6:
                initial_level = horse_races['race_level'].iloc[:3].mean()
                later_performance = (horse_races['ç€é †'] <= 3).iloc[3:].mean()
                horse_data.append({
                    'åˆæœŸãƒ¬ãƒ™ãƒ«': initial_level,
                    'å¾ŒæœŸæˆç¸¾': later_performance
                })

        if horse_data:
            df_temporal = pd.DataFrame(horse_data)
            plt.scatter(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«'], df_temporal['å¾ŒæœŸæˆç¸¾'], alpha=0.5)
            plt.xlabel('åˆæœŸãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
            plt.ylabel('å¾ŒæœŸè¤‡å‹ç‡')
            plt.title('åˆæœŸãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨å¾ŒæœŸæˆç¸¾ã®é–¢ä¿‚')
            
            # å›å¸°ç›´ç·šã®è¿½åŠ 
            z = np.polyfit(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«'], df_temporal['å¾ŒæœŸæˆç¸¾'], 1)
            p = np.poly1d(z)
            plt.plot(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«'], p(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«']), "r--", alpha=0.8)
            
            plt.savefig(output_dir / 'temporal_precedence.png')
            plt.close()

    def _plot_mechanism_analysis(self, output_dir: Path) -> None:
        """ãƒ¡ã‚«ãƒ‹ã‚ºãƒ åˆ†æã®å¯è¦–åŒ–"""
        plt.figure(figsize=(12, 6))

        # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨æˆç¸¾ã®é–¢ä¿‚ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        level_performance = []
        for horse in self.df['é¦¬å'].unique():
            horse_races = self.df[self.df['é¦¬å'] == horse]
            if len(horse_races) >= 6:
                avg_level = horse_races['race_level'].mean()
                win_rate = (horse_races['ç€é †'] == 1).mean()
                place_rate = (horse_races['ç€é †'] <= 3).mean()

                level_performance.append({
                    'å¹³å‡ãƒ¬ãƒ™ãƒ«': avg_level,
                    'å‹ç‡': win_rate,
                    'è¤‡å‹ç‡': place_rate
                })

        if level_performance:
            df_mechanism = pd.DataFrame(level_performance)

            plt.subplot(1, 2, 1)
            plt.scatter(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['å‹ç‡'], alpha=0.5)
            z = np.polyfit(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['å‹ç‡'], 1)
            p = np.poly1d(z)
            plt.plot(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], p(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«']), "r--", alpha=0.8)
            plt.title('ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨å‹ç‡ã®é–¢ä¿‚')
            plt.xlabel('å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
            plt.ylabel('å‹ç‡')

            plt.subplot(1, 2, 2)
            plt.scatter(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['è¤‡å‹ç‡'], alpha=0.5)
            z = np.polyfit(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['è¤‡å‹ç‡'], 1)
            p = np.poly1d(z)
            plt.plot(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], p(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«']), "r--", alpha=0.8)
            plt.title('ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–¢ä¿‚')
            plt.xlabel('å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
            plt.ylabel('è¤‡å‹ç‡')

            plt.tight_layout()
            plt.savefig(output_dir / 'mechanism_analysis.png')
            plt.close()

    def _plot_confounding_factors(self, output_dir: Path) -> None:
        """äº¤çµ¡å› å­ã®å¯è¦–åŒ–"""
        confounders = ['å ´ã‚³ãƒ¼ãƒ‰', 'è·é›¢', 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰']

        for confounder in confounders:
            if confounder in self.df.columns:
                plt.figure(figsize=(10, 6))

                # äº¤çµ¡å› å­ã”ã¨ã®å¹³å‡æˆç¸¾ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
                grouped_stats = self.df.groupby(confounder).agg({
                    'race_level': 'mean',
                    'ç€é †': lambda x: (x <= 3).mean()
                }).reset_index()

                plt.scatter(grouped_stats['race_level'], grouped_stats['ç€é †'],
                          s=100, alpha=0.6)

                # ãƒ©ãƒ™ãƒ«ã®è¿½åŠ 
                for i, row in grouped_stats.iterrows():
                    plt.annotate(row[confounder],
                               (row['race_level'], row['ç€é †']),
                               xytext=(5, 5), textcoords='offset points')

                plt.title(f'{confounder}ã«ã‚ˆã‚‹äº¤çµ¡åŠ¹æœ')
                plt.xlabel('å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
                plt.ylabel('è¤‡å‹ç‡')

                plt.savefig(output_dir / f'confounding_{confounder}.png')
                plt.close()

    def _calculate_grade_level(self, df: pd.DataFrame) -> pd.Series:
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ã«åŸºã¥ããƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
        if not self.class_column or self.class_column not in df.columns:
            # ã‚¯ãƒ©ã‚¹ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return pd.Series([5.0] * len(df), index=df.index)
            
        grade_level = df[self.class_column].map(
            lambda x: self.GRADE_LEVELS[x]["base_level"] if pd.notna(x) and x in self.GRADE_LEVELS else 5.0
        )

        for grade, values in self.GRADE_LEVELS.items():
            mask = df[self.class_column] == grade
            grade_level.loc[mask & df["is_win"]] += values["weight"]
            grade_level.loc[mask & df["is_placed"] & ~df["is_win"]] += values["weight"] * 0.5

        return self.normalize_values(grade_level)

    def _calculate_venue_level(self, df: pd.DataFrame) -> pd.Series:
        """ç«¶é¦¬å ´ã«åŸºã¥ããƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼šè³é‡‘åŒä¸€å€¤å¯¾å¿œï¼‰"""
        prize_col = next((c for c in ['1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)', '1ç€è³é‡‘', 'æœ¬è³é‡‘'] if c in df.columns), None)
        if prize_col is None or 'å ´å' not in df.columns:
            logger.warning("âš ï¸ è³é‡‘ã¾ãŸã¯å ´åã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚venue_levelã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è¨­å®š")
            return pd.Series([0.0] * len(df), index=df.index)

        # è³é‡‘ãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤å¤‰æ›
        df_copy = df.copy()
        df_copy[prize_col] = pd.to_numeric(df_copy[prize_col], errors='coerce')
        
        # ç«¶é¦¬å ´åˆ¥ã®è³é‡‘çµ±è¨ˆ
        venue_stats = df_copy.groupby('å ´å')[prize_col].agg(['median', 'mean', 'count', 'std']).fillna(0)
        
        logger.info(f"ğŸ“Š ç«¶é¦¬å ´åˆ¥è³é‡‘çµ±è¨ˆ:\n{venue_stats}")
        
        # è³é‡‘ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        venue_prize = venue_stats['median']
        min_prize = venue_prize.min()
        max_prize = venue_prize.max()
        
        if max_prize == min_prize or abs(max_prize - min_prize) < 1e-6:
            # å…¨ç«¶é¦¬å ´ã®è³é‡‘ãŒåŒä¸€ã®å ´åˆã€ç«¶é¦¬å ´ã®æ ¼å¼ã«åŸºã¥ããƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.warning(f"âš ï¸ å…¨ç«¶é¦¬å ´ã®è³é‡‘ãŒåŒä¸€ï¼ˆ{min_prize}ï¼‰ã®ãŸã‚ã€æ ¼å¼ãƒ™ãƒ¼ã‚¹ã®è¨ˆç®—ã«åˆ‡ã‚Šæ›¿ãˆ")
            venue_level = self._calculate_venue_level_by_prestige(df_copy)
        else:
            # é€šå¸¸ã®è³é‡‘ãƒ™ãƒ¼ã‚¹è¨ˆç®—
             venue_points = (venue_prize - min_prize) / (max_prize - min_prize) * 9.0
            venue_level = df_copy['å ´å'].map(venue_points).fillna(0)
            logger.info(f"âœ… è³é‡‘ãƒ™ãƒ¼ã‚¹ã®venue_levelè¨ˆç®—å®Œäº†: ç¯„å›² {venue_level.min():.2f} - {venue_level.max():.2f}")

        return self.normalize_values(venue_level)
    
    def _calculate_venue_level_by_prestige(self, df: pd.DataFrame) -> pd.Series:
        """ç«¶é¦¬å ´ã®æ ¼å¼ã«åŸºã¥ãvenue_levelè¨ˆç®—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        # ç«¶é¦¬å ´ã®æ ¼å¼ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å€¤ã«åŸºã¥ãï¼‰
        venue_prestige_map = {
            'æ±äº¬': 9, 'äº¬éƒ½': 9, 'é˜ªç¥': 9,
            'ä¸­å±±': 7, 'ä¸­äº¬': 7, 'æœ­å¹Œ': 7,
            'å‡½é¤¨': 4,
            'æ–°æ½Ÿ': 0, 'ç¦å³¶': 0, 'å°å€‰': 0
        }
        
        logger.info("ğŸ“‹ æ ¼å¼ãƒ™ãƒ¼ã‚¹ã®ç«¶é¦¬å ´ãƒ¬ãƒ™ãƒ«è¨ˆç®—ã‚’ä½¿ç”¨:")
        for venue, level in venue_prestige_map.items():
            logger.info(f"  {venue}: {level}")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨
        venue_level = df['å ´å'].map(venue_prestige_map).fillna(0)
        
        # çµ±è¨ˆç¢ºèª
        logger.info(f"âœ… æ ¼å¼ãƒ™ãƒ¼ã‚¹ã®venue_levelè¨ˆç®—å®Œäº†:")
        logger.info(f"  ç¯„å›²: {venue_level.min():.2f} - {venue_level.max():.2f}")
        logger.info(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {sorted(venue_level.unique())}")
        
        return venue_level
    
    def _compare_all_weighting_methods(self, horse_stats_data: pd.DataFrame) -> Dict[str, Dict]:
        """è¤‡æ•°ã®é‡ã¿ä»˜ã‘æ‰‹æ³•ã‚’è©³ç´°æ¯”è¼ƒ"""
        logger.info("ğŸ”¬ é‡ã¿ä»˜ã‘æ‰‹æ³•ã®è©³ç´°æ¯”è¼ƒã‚’é–‹å§‹...")
        
        methods_results = {}
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            required_cols = ['å¹³å‡ãƒ¬ãƒ™ãƒ«', 'å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'prize_level', 'place_rate']
            if not all(col in horse_stats_data.columns for col in required_cols):
                logger.warning("âš ï¸ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return {'correlation_squared': {'weights': {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}, 'r_squared': 0.01}}
            
            clean_data = horse_stats_data.dropna(subset=required_cols)
            if len(clean_data) < 100:
                logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(clean_data)}ä»¶")
                return {'correlation_squared': {'weights': {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}, 'r_squared': 0.01}}
            
            logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(clean_data)}é ­")
            
            # 1. ç›¸é–¢ä¿‚æ•°äºŒä¹—ãƒ™ãƒ¼ã‚¹ï¼ˆæ—¢å­˜æ‰‹æ³•ï¼‰
            methods_results['correlation_squared'] = self._method_correlation_squared(clean_data)
            
            # 2. ç·šå½¢å›å¸°ä¿‚æ•°ãƒ™ãƒ¼ã‚¹ï¼ˆæ–°æ‰‹æ³•ï¼‰
            methods_results['regression_coefficients'] = self._method_regression_coefficients(clean_data)
            
            # 3. ç­‰é‡ã¿æ‰‹æ³•ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
            methods_results['equal_weights'] = self._method_equal_weights(clean_data)
            
            # 4. çµ¶å¯¾ç›¸é–¢å€¤ãƒ™ãƒ¼ã‚¹
            methods_results['absolute_correlation'] = self._method_absolute_correlation(clean_data)
            
            # çµæœã‚µãƒãƒªãƒ¼
            logger.info("ğŸ“‹ é‡ã¿ä»˜ã‘æ‰‹æ³•æ¯”è¼ƒçµæœ:")
            for method_name, results in methods_results.items():
                r2 = results.get('r_squared', 0)
                corr = results.get('correlation', 0)
                logger.info(f"  {method_name}: RÂ²={r2:.6f}, ç›¸é–¢={corr:.6f}")
            
            return methods_results
            
        except Exception as e:
            logger.error(f"âŒ é‡ã¿ä»˜ã‘æ‰‹æ³•æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'correlation_squared': {'weights': {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}, 'r_squared': 0.01}}
    
    def _method_correlation_squared(self, data: pd.DataFrame) -> Dict:
        """ç›¸é–¢ä¿‚æ•°äºŒä¹—ãƒ™ãƒ¼ã‚¹æ‰‹æ³•"""
        try:
            corr_grade = data['å¹³å‡ãƒ¬ãƒ™ãƒ«'].corr(data['place_rate'])
            corr_venue = data['å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«'].corr(data['place_rate'])
            corr_prize = data['prize_level'].corr(data['place_rate'])
            
            # NaNå‡¦ç†
            corr_grade = 0.0 if pd.isna(corr_grade) else corr_grade
            corr_venue = 0.0 if pd.isna(corr_venue) else corr_venue
            corr_prize = 0.0 if pd.isna(corr_prize) else corr_prize
            
            # æ±ºå®šä¿‚æ•°ã‹ã‚‰é‡ã¿è¨ˆç®—
            r2_grade = corr_grade ** 2
            r2_venue = corr_venue ** 2
            r2_prize = corr_prize ** 2
            total_r2 = r2_grade + r2_venue + r2_prize
            
            if total_r2 > 0:
                weights = {
                    'grade_weight': r2_grade / total_r2,
                    'venue_weight': r2_venue / total_r2,
                    'prize_weight': r2_prize / total_r2
                }
            else:
                weights = {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}
            
            # æ€§èƒ½è©•ä¾¡
            performance = self._evaluate_weights_performance(data, weights)
            
            return {
                'weights': weights,
                'r_squared': performance['r_squared'],
                'correlation': performance['correlation'],
                'description': 'ç›¸é–¢ä¿‚æ•°äºŒä¹—ãƒ™ãƒ¼ã‚¹ï¼ˆæ—¢å­˜æ‰‹æ³•ï¼‰'
            }
            
        except Exception as e:
            logger.error(f"âŒ ç›¸é–¢ä¿‚æ•°äºŒä¹—æ‰‹æ³•ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'weights': {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}, 'r_squared': 0.0, 'correlation': 0.0}
    
    def _method_regression_coefficients(self, data: pd.DataFrame) -> Dict:
        """ç·šå½¢å›å¸°ä¿‚æ•°ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ï¼ˆæ–°æ‰‹æ³•ï¼‰"""
        try:
            from sklearn.linear_model import LinearRegression
            from sklearn.preprocessing import StandardScaler
            from sklearn.metrics import r2_score
            
            # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™
            X = data[['å¹³å‡ãƒ¬ãƒ™ãƒ«', 'å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'prize_level']].values
            y = data['place_rate'].values
            
            # æ¨™æº–åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # ç·šå½¢å›å¸°ã®å®Ÿè¡Œ
            reg = LinearRegression()
            reg.fit(X_scaled, y)
            
            # ä¿‚æ•°ã‹ã‚‰é‡ã¿ã‚’ç®—å‡ºï¼ˆçµ¶å¯¾å€¤ã§é‡è¦åº¦ã‚’è©•ä¾¡ï¼‰
            coefficients = np.abs(reg.coef_)
            total_coef = np.sum(coefficients)
            
            if total_coef > 0:
                weights = {
                    'grade_weight': coefficients[0] / total_coef,
                    'venue_weight': coefficients[1] / total_coef,
                    'prize_weight': coefficients[2] / total_coef
                }
            else:
                weights = {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}
            
            # æ€§èƒ½è©•ä¾¡ï¼ˆå›å¸°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ€§èƒ½ï¼‰
            y_pred = reg.predict(X_scaled)
            r_squared = r2_score(y, y_pred)
            correlation = np.corrcoef(y, y_pred)[0, 1] if not np.isnan(np.corrcoef(y, y_pred)[0, 1]) else 0.0
            
            logger.info(f"ğŸ”¬ ç·šå½¢å›å¸°æ‰‹æ³• - RÂ²: {r_squared:.6f}, å›å¸°ä¿‚æ•°: {coefficients}")
            
            return {
                'weights': weights,
                'r_squared': r_squared,
                'correlation': correlation,
                'description': 'ç·šå½¢å›å¸°ä¿‚æ•°ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ï¼ˆæ–°æ‰‹æ³•ï¼‰'
            }
            
        except Exception as e:
            logger.error(f"âŒ ç·šå½¢å›å¸°æ‰‹æ³•ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'weights': {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}, 'r_squared': 0.0, 'correlation': 0.0}
    
    def _method_equal_weights(self, data: pd.DataFrame) -> Dict:
        """ç­‰é‡ã¿æ‰‹æ³•ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰"""
        weights = {'grade_weight': 1/3, 'venue_weight': 1/3, 'prize_weight': 1/3}
        performance = self._evaluate_weights_performance(data, weights)
        
        return {
            'weights': weights,
            'r_squared': performance['r_squared'],
            'correlation': performance['correlation'],
            'description': 'ç­‰é‡ã¿æ‰‹æ³•ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰'
        }
    
    def _method_absolute_correlation(self, data: pd.DataFrame) -> Dict:
        """çµ¶å¯¾ç›¸é–¢å€¤ãƒ™ãƒ¼ã‚¹æ‰‹æ³•"""
        try:
            corr_grade = abs(data['å¹³å‡ãƒ¬ãƒ™ãƒ«'].corr(data['place_rate']))
            corr_venue = abs(data['å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«'].corr(data['place_rate']))
            corr_prize = abs(data['prize_level'].corr(data['place_rate']))
            
            # NaNå‡¦ç†
            corr_grade = 0.0 if pd.isna(corr_grade) else corr_grade
            corr_venue = 0.0 if pd.isna(corr_venue) else corr_venue
            corr_prize = 0.0 if pd.isna(corr_prize) else corr_prize
            
            total_corr = corr_grade + corr_venue + corr_prize
            
            if total_corr > 0:
                weights = {
                    'grade_weight': corr_grade / total_corr,
                    'venue_weight': corr_venue / total_corr,
                    'prize_weight': corr_prize / total_corr
                }
            else:
                weights = {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}
            
            performance = self._evaluate_weights_performance(data, weights)
            
            return {
                'weights': weights,
                'r_squared': performance['r_squared'],
                'correlation': performance['correlation'],
                'description': 'çµ¶å¯¾ç›¸é–¢å€¤ãƒ™ãƒ¼ã‚¹æ‰‹æ³•'
            }
            
        except Exception as e:
            logger.error(f"âŒ çµ¶å¯¾ç›¸é–¢æ‰‹æ³•ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'weights': {'grade_weight': 0.5, 'venue_weight': 0.3, 'prize_weight': 0.2}, 'r_squared': 0.0, 'correlation': 0.0}
    
    def _evaluate_weights_performance(self, data: pd.DataFrame, weights: Dict[str, float]) -> Dict[str, float]:
        """é‡ã¿ä»˜ã‘æ‰‹æ³•ã®æ€§èƒ½è©•ä¾¡"""
        try:
            # é‡ã¿ä»˜ã‘åˆæˆç‰¹å¾´é‡ã®ä½œæˆ
            composite_feature = (
                data['å¹³å‡ãƒ¬ãƒ™ãƒ«'] * weights['grade_weight'] +
                data['å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«'] * weights['venue_weight'] +
                data['prize_level'] * weights['prize_weight']
            )
            
            # æ€§èƒ½æŒ‡æ¨™ã®è¨ˆç®—
            correlation = composite_feature.corr(data['place_rate'])
            r_squared = correlation ** 2 if not pd.isna(correlation) else 0.0
            correlation = correlation if not pd.isna(correlation) else 0.0
            
            return {
                'r_squared': r_squared,
                'correlation': correlation
            }
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'r_squared': 0.0, 'correlation': 0.0}

    def validate_multicollinearity(self) -> Dict[str, Any]:
        """
        ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£æ¤œè¨¼ã‚’å®Ÿè¡Œ
        VIFã€ç›¸é–¢è¡Œåˆ—ã€æ¡ä»¶æ•°ã‚’è¨ˆç®—ã—ã€çµ±è¨ˆçš„å¦¥å½“æ€§ã‚’è©•ä¾¡
        """
        try:
            logger.info("=== ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£æ¤œè¨¼é–‹å§‹ ===")
            
            # ç‰¹å¾´é‡ã®å®šç¾©
            features = ['grade_level', 'venue_level', 'prize_level']
            
            # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆæ¬ æå€¤é™¤å»ï¼‰
            feature_data = self.df[features].dropna()
            
            if len(feature_data) == 0:
                logger.error("ğŸš¨ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼")
                return {'error': 'No feature data available'}
            
            logger.info(f"ğŸ“Š æ¤œè¨¼å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(feature_data):,}è¡Œ")
            logger.info(f"ğŸ¯ æ¤œè¨¼å¯¾è±¡ç‰¹å¾´é‡: {features}")
            
            # === 1. VIFï¼ˆåˆ†æ•£æ‹¡å¤§è¦å› ï¼‰è¨ˆç®— ===
            vif_results = self._calculate_vif(feature_data, features)
            
            # === 2. ç›¸é–¢è¡Œåˆ—åˆ†æ ===
            correlation_results = self._analyze_correlation_matrix(feature_data, features)
            
            # === 3. é‡ã¿ä»˜ã‘æ‰‹æ³•æ¯”è¼ƒ ===
            weighting_comparison = self._compare_weighting_methods(feature_data)
            
            # === 4. çµ±åˆè¨ºæ–­ ===
            overall_diagnosis = self._diagnose_multicollinearity_simple(vif_results, correlation_results)
            
            # çµæœã®çµ±åˆ
            results = {
                'vif_results': vif_results,
                'correlation_results': correlation_results,
                'weighting_comparison': weighting_comparison,
                'overall_diagnosis': overall_diagnosis,
                'data_info': {
                    'n_samples': len(feature_data),
                    'features': features,
                    'validation_timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            }
            
            # çµæœã®ä¿å­˜
            self.multicollinearity_results = results
            
            # å¯è¦–åŒ–
            self._create_multicollinearity_plots_simple(feature_data, features, results)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._generate_multicollinearity_report_simple(results)
            
            logger.info("âœ… ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£æ¤œè¨¼å®Œäº†")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:", exc_info=True)
            return {'error': str(e)}
    
    def _calculate_vif(self, feature_data: pd.DataFrame, features: list) -> Dict[str, Any]:
        """VIFï¼ˆåˆ†æ•£æ‹¡å¤§è¦å› ï¼‰ã‚’è¨ˆç®—"""
        logger.info("ğŸ§® VIFè¨ˆç®—ä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ï¼ˆVIFè¨ˆç®—ã®å‰æï¼‰
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(feature_data[features])
            
            # VIFè¨ˆç®—
            vif_data = []
            for i, feature in enumerate(features):
                try:
                    vif_value = variance_inflation_factor(scaled_data, i)
                    vif_data.append({
                        'feature': feature,
                        'vif': vif_value,
                        'status': self._get_vif_status(vif_value)
                    })
                    logger.info(f"  {feature}: VIF = {vif_value:.3f} ({self._get_vif_status(vif_value)})")
                except Exception as vif_error:
                    logger.warning(f"  {feature}: VIFè¨ˆç®—ã‚¨ãƒ©ãƒ¼ - {str(vif_error)}")
                    vif_data.append({
                        'feature': feature,
                        'vif': float('nan'),
                        'status': 'ã‚¨ãƒ©ãƒ¼'
                    })
            
            # æœ€å¤§VIFã«ã‚ˆã‚‹å…¨ä½“åˆ¤å®š
            valid_vifs = [item['vif'] for item in vif_data if not pd.isna(item['vif'])]
            if valid_vifs:
                max_vif = max(valid_vifs)
                overall_status = self._get_overall_vif_status(max_vif)
            else:
                max_vif = float('nan')
                overall_status = "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
            
            logger.info(f"ğŸ“ˆ æœ€å¤§VIF: {max_vif:.3f} â†’ {overall_status}")
            
            return {
                'vif_data': vif_data,
                'max_vif': max_vif,
                'overall_status': overall_status
            }
            
        except Exception as e:
            logger.error(f"VIFè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_correlation_matrix(self, feature_data: pd.DataFrame, features: list) -> Dict[str, Any]:
        """ç›¸é–¢è¡Œåˆ—ã‚’åˆ†æ"""
        logger.info("ğŸ“Š ç›¸é–¢è¡Œåˆ—åˆ†æä¸­...")
        
        try:
            # ç›¸é–¢è¡Œåˆ—è¨ˆç®—
            correlation_matrix = feature_data[features].corr()
            
            # é«˜ç›¸é–¢ãƒšã‚¢ã®ç‰¹å®š
            high_corr_pairs = []
            threshold = 0.8  # è­¦å‘Šé–¾å€¤
            
            for i in range(len(features)):
                for j in range(i+1, len(features)):
                    corr_value = abs(correlation_matrix.iloc[i, j])
                    if corr_value > threshold:
                        pair_info = {
                            'feature1': features[i],
                            'feature2': features[j],
                            'correlation': correlation_matrix.iloc[i, j],
                            'abs_correlation': corr_value
                        }
                        high_corr_pairs.append(pair_info)
                        logger.warning(f"ğŸš¨ é«˜ç›¸é–¢æ¤œå‡º: {features[i]} vs {features[j]} = {corr_value:.3f}")
            
            if not high_corr_pairs:
                logger.info("âœ… é«˜ç›¸é–¢ãƒšã‚¢ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            return {
                'correlation_matrix': correlation_matrix,
                'high_corr_pairs': high_corr_pairs,
                'threshold': threshold
            }
            
        except Exception as e:
            logger.error(f"ç›¸é–¢è¡Œåˆ—åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def _compare_weighting_methods(self, feature_data: pd.DataFrame) -> Dict[str, Any]:
        """è¤‡æ•°ã®é‡ã¿ä»˜ã‘æ‰‹æ³•ã‚’æ¯”è¼ƒï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        logger.info("âš–ï¸ é‡ã¿ä»˜ã‘æ‰‹æ³•æ¯”è¼ƒä¸­...")
        
        try:
            features = ['grade_level', 'venue_level', 'prize_level']
            
            # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦è¤‡å‹ç‡ã‚’å–å¾—
            horse_stats = self._calculate_horse_stats()
            
            # horse_statsã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            if 'place_rate' not in horse_stats.columns:
                logger.error("place_rate ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {'error': 'place_rate not found'}
            
            # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã¨è¤‡å‹ç‡ã‚’ãƒãƒ¼ã‚¸
            horse_features = self.df.groupby('é¦¬å')[features].mean().reset_index()
            merged_data = horse_features.merge(horse_stats[['é¦¬å', 'place_rate']], on='é¦¬å', how='inner')
            
            if len(merged_data) == 0:
                logger.error("ãƒãƒ¼ã‚¸å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return {'error': 'No merged data'}
            
            # ç°¡æ˜“æ¯”è¼ƒ
            logger.info(f"ğŸ† é‡ã¿ä»˜ã‘æ‰‹æ³•æ¯”è¼ƒå®Œäº†: {len(merged_data)}é ­ã®ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œ")
            
            return {
                'status': 'completed',
                'sample_size': len(merged_data)
            }
            
        except Exception as e:
            logger.error(f"é‡ã¿ä»˜ã‘æ‰‹æ³•æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def _diagnose_multicollinearity_simple(self, vif_results: Dict, correlation_results: Dict) -> Dict[str, Any]:
        """ç°¡æ˜“çš„ãªãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£è¨ºæ–­"""
        try:
            # VIFãƒªã‚¹ã‚¯è©•ä¾¡
            max_vif = vif_results.get('max_vif', 0)
            vif_risk = 0 if max_vif < 5 else 1 if max_vif < 10 else 2
            
            # ç›¸é–¢ãƒªã‚¹ã‚¯è©•ä¾¡
            high_corr_pairs = correlation_results.get('high_corr_pairs', [])
            corr_risk = 1 if len(high_corr_pairs) > 0 else 0
            
            # ç·åˆåˆ¤å®š
            overall_risk = max(vif_risk, corr_risk)
            severity = ['æ­£å¸¸', 'æ³¨æ„', 'å±é™º'][overall_risk]
            
            logger.info(f"ğŸ“‹ è¨ºæ–­çµæœ: {severity} (ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {overall_risk})")
            
            return {
                'overall_risk_level': overall_risk,
                'severity': severity,
                'vif_risk': vif_risk,
                'correlation_risk': corr_risk
            }
            
        except Exception as e:
            logger.error(f"è¨ºæ–­ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def _create_multicollinearity_plots_simple(self, feature_data: pd.DataFrame, features: list, results: Dict[str, Any]) -> None:
        """ç°¡æ˜“ç‰ˆå¯è¦–åŒ–"""
        try:
            output_dir = Path(self.config.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç›¸é–¢è¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ã¿ä½œæˆ
            if 'correlation_results' in results and 'correlation_matrix' in results['correlation_results']:
                fig, ax = plt.subplots(1, 1, figsize=(8, 6))
                corr_matrix = results['correlation_results']['correlation_matrix']
                sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, square=True, ax=ax)
                ax.set_title('ç‰¹å¾´é‡é–“ç›¸é–¢è¡Œåˆ—')
                
                plot_path = output_dir / 'multicollinearity_validation.png'
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                logger.info(f"ğŸ“Š å¯è¦–åŒ–ä¿å­˜: {plot_path}")
            
        except Exception as e:
            logger.error(f"å¯è¦–åŒ–ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _generate_multicollinearity_report_simple(self, results: Dict[str, Any]) -> None:
        """ç°¡æ˜“ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            output_dir = Path(self.config.output_dir)
            report_path = output_dir / 'multicollinearity_validation_report.md'
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
                f.write(f"ç”Ÿæˆæ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # VIFçµæœ
                if 'vif_results' in results and 'vif_data' in results['vif_results']:
                    f.write("## ğŸ§® VIFæ¤œè¨¼çµæœ\n\n")
                    f.write("| ç‰¹å¾´é‡ | VIFå€¤ | åˆ¤å®š |\n")
                    f.write("|--------|-------|------|\n")
                    
                    for item in results['vif_results']['vif_data']:
                        vif_val = item['vif']
                        vif_str = f"{vif_val:.3f}" if not pd.isna(vif_val) else "nan"
                        f.write(f"| {item['feature']} | {vif_str} | {item['status']} |\n")
                
                # ç›¸é–¢çµæœ
                if 'correlation_results' in results:
                    f.write("\n## ğŸ“Š ç›¸é–¢åˆ†æçµæœ\n\n")
                    high_corr_pairs = results['correlation_results'].get('high_corr_pairs', [])
                    if high_corr_pairs:
                        f.write("### ğŸš¨ é«˜ç›¸é–¢ãƒšã‚¢æ¤œå‡º\n\n")
                        for pair in high_corr_pairs:
                            f.write(f"- {pair['feature1']} vs {pair['feature2']}: r = {pair['correlation']:.3f}\n")
                    else:
                        f.write("âœ… é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ|r| > 0.8ï¼‰ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n")
                
                # è¨ºæ–­çµæœ
                if 'overall_diagnosis' in results:
                    diagnosis = results['overall_diagnosis']
                    f.write(f"\n## ğŸ¥ ç·åˆè¨ºæ–­\n\n")
                    f.write(f"**åˆ¤å®š**: {diagnosis.get('severity', 'Unknown')}\n")
                    f.write(f"**ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«**: {diagnosis.get('overall_risk_level', 'Unknown')}/2\n")
            
            logger.info(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _get_vif_status(self, vif_value: float) -> str:
        """VIFå€¤ã‹ã‚‰çŠ¶æ…‹ã‚’åˆ¤å®š"""
        if pd.isna(vif_value):
            return "ã‚¨ãƒ©ãƒ¼"
        elif vif_value < 5:
            return "æ­£å¸¸"
        elif vif_value < 10:
            return "æ³¨æ„"
        else:
            return "å±é™º"
    
    def _get_overall_vif_status(self, max_vif: float) -> str:
        """æœ€å¤§VIFå€¤ã‹ã‚‰å…¨ä½“çŠ¶æ…‹ã‚’åˆ¤å®š"""
        if pd.isna(max_vif):
            return "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        elif max_vif < 5:
            return "å•é¡Œãªã—"
        elif max_vif < 10:
            return "è»½åº¦ã®ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£"
        else:
            return "æ·±åˆ»ãªãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£"

    def _calculate_prize_level(self, df: pd.DataFrame) -> pd.Series:
        """è³é‡‘ã«åŸºã¥ããƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ï¼ˆåˆ—åã®é•ã„ã«å¯¾ã™ã‚‹äº’æ›å¯¾å¿œï¼‰"""
        # è³é‡‘åˆ—å€™è£œã‚’å„ªå…ˆé †ã«æ¢ç´¢
        prize_candidates = [
            '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)',
            '1ç€è³é‡‘',
            'æœ¬è³é‡‘'
        ]
        prize_col = next((c for c in prize_candidates if c in df.columns), None)
        if prize_col is None:
            # è³é‡‘æƒ…å ±ãŒãªã„å ´åˆã¯0ç³»åˆ—ã‚’è¿”ã™
            return pd.Series([0.0] * len(df), index=df.index)

        prizes = pd.to_numeric(df[prize_col], errors='coerce').fillna(0)
        max_val = prizes.max()
        if max_val <= 0:
            return pd.Series([0.0] * len(df), index=df.index)

        prize_level = np.log1p(prizes) / np.log1p(max_val) * 9.95
        return self.normalize_values(prize_level)

    def _calculate_horse_stats(self) -> pd.DataFrame:
        """é¦¬ã”ã¨ã®åŸºæœ¬çµ±è¨ˆã‚’è¨ˆç®—"""
        if "is_win" not in self.df.columns:
            self.df["is_win"] = self.df["ç€é †"] == 1
        if "is_placed" not in self.df.columns:
            self.df["is_placed"] = self.df["ç€é †"] <= 3

        # é¦¬ã”ã¨ã®åŸºæœ¬çµ±è¨ˆ
        agg_dict = {
            "race_level": ["max", "mean"],
            "venue_level": ["max", "mean"],
            "is_win": "sum",
            "is_placed": "sum",
            "ç€é †": "count"
        }
        
        # ã‚¯ãƒ©ã‚¹ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
        if self.class_column and self.class_column in self.df.columns:
            agg_dict[self.class_column] = lambda x: x.value_counts().idxmax() if not x.empty else 0
        
        horse_stats = self.df.groupby("é¦¬å").agg(agg_dict).reset_index()

        # ã‚«ãƒ©ãƒ åã®æ•´ç†
        if self.class_column and self.class_column in self.df.columns:
            horse_stats.columns = ["é¦¬å", "æœ€é«˜ãƒ¬ãƒ™ãƒ«", "å¹³å‡ãƒ¬ãƒ™ãƒ«", "æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«", "å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«", "å‹åˆ©æ•°", "è¤‡å‹æ•°", "å‡ºèµ°å›æ•°", "ä¸»æˆ¦ã‚¯ãƒ©ã‚¹"]
        else:
            horse_stats.columns = ["é¦¬å", "æœ€é«˜ãƒ¬ãƒ™ãƒ«", "å¹³å‡ãƒ¬ãƒ™ãƒ«", "æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«", "å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«", "å‹åˆ©æ•°", "è¤‡å‹æ•°", "å‡ºèµ°å›æ•°"]
        
        # ãƒ¬ãƒ¼ã‚¹å›æ•°ãŒmin_raceså›ä»¥ä¸Šã®é¦¬ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        min_races = self.config.min_races if hasattr(self.config, 'min_races') else 3
        horse_stats = horse_stats[horse_stats["å‡ºèµ°å›æ•°"] >= min_races]
        
        # å‹ç‡ã¨è¤‡å‹ç‡ã®è¨ˆç®—
        horse_stats["win_rate"] = horse_stats["å‹åˆ©æ•°"] / horse_stats["å‡ºèµ°å›æ•°"]
        horse_stats["place_rate"] = horse_stats["è¤‡å‹æ•°"] / horse_stats["å‡ºèµ°å›æ•°"]
        
        return horse_stats

    def _calculate_grade_stats(self) -> pd.DataFrame:
        """ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—"""
        if not self.class_column or self.class_column not in self.df.columns:
            # ã‚¯ãƒ©ã‚¹ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºã®DataFrameã‚’è¿”ã™
            return pd.DataFrame()
            
        grade_stats = self.df.groupby(self.class_column).agg({
            "is_win": ["mean", "count"],
            "is_placed": "mean",
            "race_level": ["mean", "std"]
        }).reset_index()

        grade_stats.columns = [
            "ã‚¯ãƒ©ã‚¹", "å‹ç‡", "ãƒ¬ãƒ¼ã‚¹æ•°", "è¤‡å‹ç‡",
            "å¹³å‡ãƒ¬ãƒ™ãƒ«", "ãƒ¬ãƒ™ãƒ«æ¨™æº–åå·®"
        ]

        return grade_stats

    def _perform_correlation_analysis(self, horse_stats: pd.DataFrame) -> Dict[str, Any]:
        """ç›¸é–¢åˆ†æã‚’å®Ÿè¡Œ"""
        # TODO:æ¬ æå€¤ã®ã¤ã„ã¦èª¿æŸ»äºˆå®š
        analysis_data = horse_stats.dropna(subset=['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'å¹³å‡ãƒ¬ãƒ™ãƒ«', 'æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'win_rate', 'place_rate'])
        
        default_results = {
            "correlation_win_max": 0.0,
            "correlation_place_max": 0.0,
            "correlation_win_avg": 0.0,
            "correlation_place_avg": 0.0,
            "correlation_win_venue_max": 0.0,
            "correlation_place_venue_max": 0.0,
            "correlation_win_venue_avg": 0.0,
            "correlation_place_venue_avg": 0.0,
            "model_win_max": None,
            "model_place_max": None,
            "model_win_avg": None,
            "model_place_avg": None,
            "model_win_venue_max": None,
            "model_place_venue_max": None,
            "model_win_venue_avg": None,
            "model_place_venue_avg": None,
            "r2_win_max": 0.0,
            "r2_place_max": 0.0,
            "r2_win_avg": 0.0,
            "r2_place_avg": 0.0,
            "r2_win_venue_max": 0.0,
            "r2_place_venue_max": 0.0,
            "r2_win_venue_avg": 0.0,
            "r2_place_venue_avg": 0.0,
            "correlation_win": 0.0,
            "correlation_place": 0.0,
            "model_win": None,
            "model_place": None,
            "r2_win": 0.0,
            "r2_place": 0.0
        }

        if len(analysis_data) < 2:  # ãƒ‡ãƒ¼ã‚¿ãŒ2ä»¶æœªæº€ã ã¨ç›¸é–¢ãŒè¨ˆç®—ã§ããªã„
            return default_results

        # æ¨™æº–åå·®ãŒ0ã®å ´åˆã®å‡¦ç†
        # TODO:æ¨™æº–åå·®ãŒ0ã®å ´åˆã®å‡¦ç†ã‚’èª¿æŸ»äºˆå®š
        stddev = analysis_data[['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'å¹³å‡ãƒ¬ãƒ™ãƒ«', 'æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'win_rate', 'place_rate']].std()
        if (stddev == 0).any():
            return default_results

        # æœ€é«˜ãƒ¬ãƒ™ãƒ« - å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_win_max = analysis_data[['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'win_rate']].corr().iloc[0, 1]
        X_win_max = analysis_data['æœ€é«˜ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        y_win = analysis_data['win_rate'].values
        model_win_max = LinearRegression()
        model_win_max.fit(X_win_max, y_win)
        r2_win_max = model_win_max.score(X_win_max, y_win)

        # æœ€é«˜ãƒ¬ãƒ™ãƒ« - è¤‡å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_place_max = analysis_data[['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'place_rate']].corr().iloc[0, 1]
        X_place_max = analysis_data['æœ€é«˜ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        y_place = analysis_data['place_rate'].values
        model_place_max = LinearRegression()
        model_place_max.fit(X_place_max, y_place)
        r2_place_max = model_place_max.score(X_place_max, y_place)

        # å¹³å‡ãƒ¬ãƒ™ãƒ« - å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_win_avg = analysis_data[['å¹³å‡ãƒ¬ãƒ™ãƒ«', 'win_rate']].corr().iloc[0, 1]
        X_win_avg = analysis_data['å¹³å‡ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_win_avg = LinearRegression()
        model_win_avg.fit(X_win_avg, y_win)
        r2_win_avg = model_win_avg.score(X_win_avg, y_win)

        # å¹³å‡ãƒ¬ãƒ™ãƒ« - è¤‡å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_place_avg = analysis_data[['å¹³å‡ãƒ¬ãƒ™ãƒ«', 'place_rate']].corr().iloc[0, 1]
        X_place_avg = analysis_data['å¹³å‡ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_place_avg = LinearRegression()
        model_place_avg.fit(X_place_avg, y_place)
        r2_place_avg = model_place_avg.score(X_place_avg, y_place)

        # å ´æ‰€ãƒ¬ãƒ™ãƒ«ã®ç›¸é–¢
        correlation_win_venue_max = analysis_data[['æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'win_rate']].corr().iloc[0, 1]
        X_win_venue_max = analysis_data['æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_win_venue_max = LinearRegression().fit(X_win_venue_max, y_win)
        r2_win_venue_max = model_win_venue_max.score(X_win_venue_max, y_win)

        correlation_place_venue_max = analysis_data[['æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'place_rate']].corr().iloc[0, 1]
        X_place_venue_max = analysis_data['æœ€é«˜å ´æ‰€ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_place_venue_max = LinearRegression().fit(X_place_venue_max, y_place)
        r2_place_venue_max = model_place_venue_max.score(X_place_venue_max, y_place)

        correlation_win_venue_avg = analysis_data[['å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'win_rate']].corr().iloc[0, 1]
        X_win_venue_avg = analysis_data['å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_win_venue_avg = LinearRegression().fit(X_win_venue_avg, y_win)
        r2_win_venue_avg = model_win_venue_avg.score(X_win_venue_avg, y_win)

        correlation_place_venue_avg = analysis_data[['å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«', 'place_rate']].corr().iloc[0, 1]
        X_place_venue_avg = analysis_data['å¹³å‡å ´æ‰€ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_place_venue_avg = LinearRegression().fit(X_place_venue_avg, y_place)
        r2_place_venue_avg = model_place_venue_avg.score(X_place_venue_avg, y_place)

        return {
            # æœ€é«˜ãƒ¬ãƒ™ãƒ«ç³»
            "correlation_win_max": correlation_win_max,
            "correlation_place_max": correlation_place_max,
            "model_win_max": model_win_max,
            "model_place_max": model_place_max,
            "r2_win_max": r2_win_max,
            "r2_place_max": r2_place_max,
            # å¹³å‡ãƒ¬ãƒ™ãƒ«ç³»
            "correlation_win_avg": correlation_win_avg,
            "correlation_place_avg": correlation_place_avg,
            "model_win_avg": model_win_avg,
            "model_place_avg": model_place_avg,
            "r2_win_avg": r2_win_avg,
            "r2_place_avg": r2_place_avg,
            # å ´æ‰€ãƒ¬ãƒ™ãƒ«ç³»
            "correlation_win_venue_max": correlation_win_venue_max,
            "correlation_place_venue_max": correlation_place_venue_max,
            "correlation_win_venue_avg": correlation_win_venue_avg,
            "correlation_place_venue_avg": correlation_place_venue_avg,
            "model_win_venue_max": model_win_venue_max,
            "model_place_venue_max": model_place_venue_max,
            "model_win_venue_avg": model_win_venue_avg,
            "model_place_venue_avg": model_place_venue_avg,
            "r2_win_venue_max": r2_win_venue_max,
            "r2_place_venue_max": r2_place_venue_max,
            "r2_win_venue_avg": r2_win_venue_avg,
            "r2_place_venue_avg": r2_place_venue_avg,
            # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ—¢å­˜ã®ã‚­ãƒ¼ã‚‚æ®‹ã™
            "correlation_win": correlation_win_max,
            "correlation_place": correlation_place_max,
            "model_win": model_win_max,
            "model_place": model_place_max,
            "r2_win": r2_win_max,
            "r2_place": r2_place_max
        } 

    def _interpret_h1_results(self, correlation: float, p_value: float, r2: float) -> str:
        """ä»®èª¬H1ã®çµæœè§£é‡ˆ"""
        significance = "çµ±è¨ˆçš„ã«æœ‰æ„" if p_value < 0.05 else "çµ±è¨ˆçš„ã«éæœ‰æ„"
        strength = "å¼·ã„" if abs(correlation) > 0.5 else "ä¸­ç¨‹åº¦" if abs(correlation) > 0.3 else "å¼±ã„"
        direction = "æ­£ã®" if correlation > 0 else "è² ã®"
        
        return f"ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨èµ°ç ´ã‚¿ã‚¤ãƒ ã«ã¯{strength}{direction}ç›¸é–¢ãŒã‚ã‚Šã€{significance}ã§ã™ï¼ˆr={correlation:.3f}, RÂ²={r2:.3f}ï¼‰ã€‚"

    def _interpret_h4_results(self, correlation: float, p_value: float, odds_ratio: float) -> str:
        """ä»®èª¬H4ã®çµæœè§£é‡ˆ"""
        significance = "çµ±è¨ˆçš„ã«æœ‰æ„" if p_value < 0.05 else "çµ±è¨ˆçš„ã«éæœ‰æ„"
        strength = "å¼·ã„" if abs(correlation) > 0.5 else "ä¸­ç¨‹åº¦" if abs(correlation) > 0.3 else "å¼±ã„"
        
        if odds_ratio > 1:
            effect = f"é€Ÿã„ã‚¿ã‚¤ãƒ ã¯è¤‡å‹ç‡ã‚’{odds_ratio:.2f}å€é«˜ã‚ã‚‹"
        else:
            effect = f"é€Ÿã„ã‚¿ã‚¤ãƒ ã¯è¤‡å‹ç‡ã‚’{1/odds_ratio:.2f}åˆ†ã®1ã«ä¸‹ã’ã‚‹"
        
        return f"èµ°ç ´ã‚¿ã‚¤ãƒ ã¨è¤‡å‹ç‡ã«ã¯{strength}ç›¸é–¢ãŒã‚ã‚Šã€{significance}ã§ã™ï¼ˆr={correlation:.3f}ï¼‰ã€‚{effect}åŠ¹æœãŒã‚ã‚Šã¾ã™ã€‚"

    def _generate_time_analysis_report(self, results: Dict[str, Any], output_dir: Path) -> None:
        """RunningTimeåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            logger.info("ğŸ“ RunningTimeåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
            
            report_path = output_dir / 'running_time_analysis_report.md'
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# èµ°ç ´ã‚¿ã‚¤ãƒ å› æœé–¢ä¿‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
                f.write(f"ç”Ÿæˆæ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # ä»®èª¬H1ã®çµæœ
                if 'hypothesis_h1' in results:
                    h1 = results['hypothesis_h1']
                    f.write("## ğŸ§ª ä»®èª¬H1æ¤œè¨¼: RaceLevel â†’ RunningTime\n\n")
                    f.write("### åˆ†æçµæœ\n")
                    f.write(f"- **ç›¸é–¢ä¿‚æ•°**: {h1.get('correlation', 0):.3f}\n")
                    f.write(f"- **æ±ºå®šä¿‚æ•° (RÂ²)**: {h1.get('r2_score', 0):.3f}\n")
                    f.write(f"- **på€¤**: {h1.get('p_value', 1):.6f}\n")
                    f.write(f"- **çµ±è¨ˆçš„æœ‰æ„æ€§**: {'æœ‰æ„' if h1.get('is_significant', False) else 'éæœ‰æ„'}\n")
                    f.write(f"- **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**: {h1.get('sample_size', 0):,}ä»¶\n")
                    f.write(f"- **åŠ¹æœã®æ–¹å‘**: {h1.get('effect_direction', 'ä¸æ˜')}\n\n")
                    f.write("### è§£é‡ˆ\n")
                    f.write(f"{h1.get('interpretation', 'è§£é‡ˆæƒ…å ±ãªã—')}\n\n")
                
                # ä»®èª¬H4ã®çµæœ
                if 'hypothesis_h4' in results:
                    h4 = results['hypothesis_h4']
                    f.write("## ğŸ§ª ä»®èª¬H4æ¤œè¨¼: RunningTime â†’ PlaceRate\n\n")
                    f.write("### åˆ†æçµæœ\n")
                    f.write(f"- **ç›¸é–¢ä¿‚æ•°**: {h4.get('correlation', 0):.3f}\n")
                    f.write(f"- **äºˆæ¸¬ç²¾åº¦**: {h4.get('accuracy', 0):.3f}\n")
                    f.write(f"- **på€¤**: {h4.get('p_value', 1):.6f}\n")
                    f.write(f"- **çµ±è¨ˆçš„æœ‰æ„æ€§**: {'æœ‰æ„' if h4.get('is_significant', False) else 'éæœ‰æ„'}\n")
                    f.write(f"- **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**: {h4.get('sample_size', 0):,}ä»¶\n")
                    if 'odds_ratios' in h4 and len(h4['odds_ratios']) > 0:
                        f.write(f"- **ã‚¿ã‚¤ãƒ ã®ã‚ªãƒƒã‚ºæ¯”**: {h4['odds_ratios'][0]:.3f}\n")
                    f.write("\n### è§£é‡ˆ\n")
                    f.write(f"{h4.get('interpretation', 'è§£é‡ˆæƒ…å ±ãªã—')}\n\n")
                
                # åŒ…æ‹¬çš„åˆ†æã®çµæœ
                if 'comprehensive_analysis' in results:
                    comp = results['comprehensive_analysis']
                    f.write("## ğŸ“Š åŒ…æ‹¬çš„å› æœé–¢ä¿‚åˆ†æ\n\n")
                    
                    # åª’ä»‹åŠ¹æœåˆ†æ
                    if 'mediation_analysis' in comp:
                        med = comp['mediation_analysis']
                        f.write("### åª’ä»‹åŠ¹æœåˆ†æ (RaceLevel â†’ Time â†’ PlaceRate)\n")
                        f.write(f"- **ç·åŠ¹æœ**: {med.get('total_effect', 0):.3f}\n")
                        f.write(f"- **ç›´æ¥åŠ¹æœ**: {med.get('direct_effect', 0):.3f}\n")
                        f.write(f"- **é–“æ¥åŠ¹æœ**: {med.get('indirect_effect', 0):.3f}\n")
                        f.write(f"- **åª’ä»‹æ¯”ç‡**: {med.get('mediation_ratio', 0):.3f}\n\n")
                    
                    # è·é›¢åˆ¥åŠ¹æœåˆ†æ
                    if 'distance_specific_effects' in comp:
                        dist_effects = comp['distance_specific_effects']
                        f.write("### è·é›¢åˆ¥åŠ¹æœåˆ†æ\n\n")
                        f.write("| è·é›¢ã‚«ãƒ†ã‚´ãƒª | ã‚µãƒ³ãƒ—ãƒ«æ•° | RaceLevelâ†’Timeç›¸é–¢ | Timeâ†’PlaceRateç›¸é–¢ |\n")
                        f.write("|------------|-----------|-------------------|------------------|\n")
                        
                        for category, stats in dist_effects.items():
                            sample_size = stats.get('sample_size', 0)
                            race_time_corr = stats.get('race_level_time_correlation', 0)
                            time_place_corr = stats.get('time_place_correlation', 0)
                            f.write(f"| {category} | {sample_size:,} | {race_time_corr:.3f} | {time_place_corr:.3f} |\n")
                        f.write("\n")
                
                # è«–æ–‡ä»®èª¬ã¨ã®å¯¾å¿œ
                f.write("## ğŸ“‹ è«–æ–‡ä»®èª¬ã¨ã®å¯¾å¿œçŠ¶æ³\n\n")
                f.write("| ä»®èª¬ | æ¤œè¨¼çŠ¶æ³ | çµæœ |\n")
                f.write("|------|----------|------|\n")
                
                h1_status = "âœ… æ¤œè¨¼æ¸ˆã¿" if 'hypothesis_h1' in results else "âŒ æœªæ¤œè¨¼"
                h1_result = "æœ‰æ„" if results.get('hypothesis_h1', {}).get('is_significant', False) else "éæœ‰æ„"
                f.write(f"| H1: RaceLevel â†’ RunningTime | {h1_status} | {h1_result} |\n")
                
                h4_status = "âœ… æ¤œè¨¼æ¸ˆã¿" if 'hypothesis_h4' in results else "âŒ æœªæ¤œè¨¼"
                h4_result = "æœ‰æ„" if results.get('hypothesis_h4', {}).get('is_significant', False) else "éæœ‰æ„"
                f.write(f"| H4: RunningTime â†’ PlaceRate | {h4_status} | {h4_result} |\n")
                
                f.write("| H2: RaceLevel â†’ HorseAbility â†’ RunningTime | âŒ æœªå®Ÿè£… | - |\n")
                f.write("| H3: TrackBias Ã— HorseAbility â†’ RunningTime | âŒ æœªå®Ÿè£… | - |\n")
                f.write("| H5: RaceLevel â†’ RunningTime â†’ PlaceRate | ğŸ”„ éƒ¨åˆ†å®Ÿè£… | åª’ä»‹åŠ¹æœåˆ†ææ¸ˆã¿ |\n\n")
                
                # ä»Šå¾Œã®æ”¹å–„ç‚¹
                f.write("## ğŸš€ ä»Šå¾Œã®æ”¹å–„ç‚¹\n\n")
                f.write("1. **é¦¬èƒ½åŠ›æŒ‡æ¨™ã®å®Ÿè£…**: IDMãƒ»ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ãƒ»ä¸ŠãŒã‚ŠæŒ‡æ•°ã®çµ±åˆ\n")
                f.write("2. **ãƒˆãƒ©ãƒƒã‚¯ãƒã‚¤ã‚¢ã‚¹è©³ç´°åŒ–**: è„šè³ªãƒ»æ é †ãƒ»è·é›¢åˆ¥ãƒã‚¤ã‚¢ã‚¹ã®å®Ÿè£…\n")
                f.write("3. **ä»®èª¬H2, H3ã®å®Œå…¨æ¤œè¨¼**: åª’ä»‹åˆ†æã¨äº¤äº’ä½œç”¨åˆ†æã®å®Ÿè£…\n")
                f.write("4. **æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã®é©ç”¨**: Random Forest, XGBoostã«ã‚ˆã‚‹äºˆæ¸¬ç²¾åº¦å‘ä¸Š\n")
                f.write("5. **é«˜åº¦å› æœæ¨è«–ã®å®Ÿè£…**: å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã€IPWã®é©ç”¨\n\n")
                
                f.write("## ğŸ’¡ çµè«–\n\n")
                f.write("RunningTimeåˆ†æã«ã‚ˆã‚Šã€è«–æ–‡ã§ææ¡ˆã•ã‚ŒãŸå› æœé–¢ä¿‚ã®ä¸€éƒ¨ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸã€‚\n")
                f.write("ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨èµ°ç ´ã‚¿ã‚¤ãƒ ã€èµ°ç ´ã‚¿ã‚¤ãƒ ã¨è¤‡å‹ç‡ã®é–¢ä¿‚ã«ã¤ã„ã¦ã€çµ±è¨ˆçš„ã«æœ‰æ„ãªçµæœãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚\n")
                f.write("ä»Šå¾Œã€æ®‹ã‚Šã®ä»®èª¬æ¤œè¨¼ã¨é«˜åº¦ãªå› æœæ¨è«–æ‰‹æ³•ã®å®Ÿè£…ã«ã‚ˆã‚Šã€ã‚ˆã‚Šå®Œå…¨ãªå› æœãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚\n")
            
            logger.info(f"ğŸ“ RunningTimeåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def _visualize_time_analysis(self) -> None:
        """RunningTimeåˆ†æã®å¯è¦–åŒ–"""
        try:
            logger.info("ğŸ“Š RunningTimeåˆ†æã®å¯è¦–åŒ–ã‚’é–‹å§‹...")
            
            output_dir = Path(self.config.output_dir)
            time_viz_dir = output_dir / 'time_analysis'
            time_viz_dir.mkdir(exist_ok=True)
            
            time_results = self.stats['time_analysis']
            
            # 1. RaceLevel vs RunningTime ã®æ•£å¸ƒå›³ï¼ˆä»®èª¬H1ï¼‰
            if 'hypothesis_h1' in time_results:
                self._plot_race_level_time_relationship(time_viz_dir)
            
            # 2. RunningTime vs PlaceRate ã®æ•£å¸ƒå›³ï¼ˆä»®èª¬H4ï¼‰
            if 'hypothesis_h4' in time_results:
                self._plot_time_place_relationship(time_viz_dir)
            
            # 3. è·é›¢åˆ¥åŠ¹æœã®å¯è¦–åŒ–
            if 'comprehensive_analysis' in time_results:
                self._plot_distance_specific_effects(time_viz_dir, time_results['comprehensive_analysis'])
            
            # 4. åª’ä»‹åŠ¹æœã®å¯è¦–åŒ–
            if 'comprehensive_analysis' in time_results and 'mediation_analysis' in time_results['comprehensive_analysis']:
                self._plot_mediation_effects(time_viz_dir, time_results['comprehensive_analysis']['mediation_analysis'])
            
            logger.info("âœ… RunningTimeåˆ†æã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"âŒ RunningTimeåˆ†æå¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def _plot_race_level_time_relationship(self, output_dir: Path) -> None:
        """RaceLevel vs RunningTime ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ï¼ˆä»®èª¬H1ï¼‰"""
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            
            # å·¦å´: æ•£å¸ƒå›³
            data = self.df.dropna(subset=['race_level', 'time_zscore'])
            
            ax1.scatter(data['race_level'], -data['time_zscore'], alpha=0.5, s=30)
            
            # å›å¸°ç›´ç·š
            z = np.polyfit(data['race_level'], -data['time_zscore'], 1)
            p = np.poly1d(z)
            ax1.plot(data['race_level'], p(data['race_level']), "r--", alpha=0.8, linewidth=2)
            
            correlation = data['race_level'].corr(-data['time_zscore'])
            ax1.set_title(f'ä»®èª¬H1: ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ« vs èµ°ç ´ã‚¿ã‚¤ãƒ \nç›¸é–¢ä¿‚æ•°: {correlation:.3f}')
            ax1.set_xlabel('ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
            ax1.set_ylabel('èµ°ç ´ã‚¿ã‚¤ãƒ ï¼ˆæ­£è¦åŒ–ã€é€Ÿã„=é«˜ã„å€¤ï¼‰')
            ax1.grid(True, alpha=0.3)
            
            # å³å´: ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ¥ç®±ã²ã’å›³
            level_categories = pd.cut(data['race_level'], bins=5, labels=['Low', 'Low-Mid', 'Mid', 'Mid-High', 'High'])
            data_with_cat = data.copy()
            data_with_cat['level_category'] = level_categories
            
            import seaborn as sns
            sns.boxplot(data=data_with_cat, x='level_category', y='time_zscore', ax=ax2)
            ax2.set_title('ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ¥ èµ°ç ´ã‚¿ã‚¤ãƒ åˆ†å¸ƒ')
            ax2.set_xlabel('ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã‚«ãƒ†ã‚´ãƒª')
            ax2.set_ylabel('èµ°ç ´ã‚¿ã‚¤ãƒ ï¼ˆZ-scoreï¼‰')
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_dir / 'h1_race_level_time_relationship.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"âŒ H1å¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def _plot_time_place_relationship(self, output_dir: Path) -> None:
        """RunningTime vs PlaceRate ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ï¼ˆä»®èª¬H4ï¼‰"""
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            
            data = self.df.dropna(subset=['time_zscore', 'is_placed'])
            
            # å·¦å´: æ•£å¸ƒå›³ï¼ˆã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãï¼‰
            placed_data = data[data['is_placed'] == 1]
            not_placed_data = data[data['is_placed'] == 0]
            
            ax1.scatter(not_placed_data['time_zscore'], 
                       np.random.normal(0, 0.05, len(not_placed_data)), 
                       alpha=0.6, s=20, color='red', label='è¤‡å‹åœå¤–')
            ax1.scatter(placed_data['time_zscore'], 
                       np.random.normal(1, 0.05, len(placed_data)), 
                       alpha=0.6, s=20, color='blue', label='è¤‡å‹åœå†…')
            
            correlation = (-data['time_zscore']).corr(data['is_placed'])
            ax1.set_title(f'ä»®èª¬H4: èµ°ç ´ã‚¿ã‚¤ãƒ  vs è¤‡å‹ç‡\nç›¸é–¢ä¿‚æ•°: {correlation:.3f}')
            ax1.set_xlabel('èµ°ç ´ã‚¿ã‚¤ãƒ ï¼ˆZ-scoreã€é€Ÿã„=ä½ã„å€¤ï¼‰')
            ax1.set_ylabel('è¤‡å‹çµæœ')
            ax1.set_yticks([0, 1])
            ax1.set_yticklabels(['åœå¤–', 'åœå†…'])
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # å³å´: ã‚¿ã‚¤ãƒ åŒºé–“åˆ¥è¤‡å‹ç‡
            time_bins = pd.cut(data['time_zscore'], bins=10)
            place_rate_by_time = data.groupby(time_bins)['is_placed'].agg(['mean', 'count']).reset_index()
            
            # ãƒ“ãƒ³ã®ä¸­å¤®å€¤ã‚’è¨ˆç®—
            bin_centers = [interval.mid for interval in place_rate_by_time['time_zscore']]
            
            ax2.bar(range(len(bin_centers)), place_rate_by_time['mean'], alpha=0.7)
            ax2.set_title('ã‚¿ã‚¤ãƒ åŒºé–“åˆ¥ è¤‡å‹ç‡')
            ax2.set_xlabel('ã‚¿ã‚¤ãƒ åŒºé–“ï¼ˆé€Ÿã„â†’é…ã„ï¼‰')
            ax2.set_ylabel('è¤‡å‹ç‡')
            ax2.set_xticks(range(len(bin_centers)))
            ax2.set_xticklabels([f'{center:.2f}' for center in bin_centers], rotation=45)
            ax2.grid(True, alpha=0.3)
            
            # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¡¨ç¤º
            for i, count in enumerate(place_rate_by_time['count']):
                ax2.text(i, place_rate_by_time['mean'].iloc[i] + 0.01, f'n={count}', 
                        ha='center', va='bottom', fontsize=8)
            
            plt.tight_layout()
            plt.savefig(output_dir / 'h4_time_place_relationship.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"âŒ H4å¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def _plot_distance_specific_effects(self, output_dir: Path, comprehensive_results: Dict[str, Any]) -> None:
        """è·é›¢åˆ¥åŠ¹æœã®å¯è¦–åŒ–"""
        try:
            if 'distance_specific_effects' not in comprehensive_results:
                return
            
            distance_effects = comprehensive_results['distance_specific_effects']
            
            if not distance_effects:
                return
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
            
            categories = list(distance_effects.keys())
            race_time_corrs = [distance_effects[cat]['race_level_time_correlation'] for cat in categories]
            time_place_corrs = [distance_effects[cat]['time_place_correlation'] for cat in categories]
            sample_sizes = [distance_effects[cat]['sample_size'] for cat in categories]
            
            x = np.arange(len(categories))
            
            # å·¦å´: RaceLevel â†’ Time ç›¸é–¢
            bars1 = ax1.bar(x, race_time_corrs, alpha=0.7, color='skyblue')
            ax1.set_title('è·é›¢åˆ¥: ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ« â†’ èµ°ç ´ã‚¿ã‚¤ãƒ  ç›¸é–¢')
            ax1.set_xlabel('è·é›¢ã‚«ãƒ†ã‚´ãƒª')
            ax1.set_ylabel('ç›¸é–¢ä¿‚æ•°')
            ax1.set_xticks(x)
            ax1.set_xticklabels(categories)
            ax1.grid(True, alpha=0.3)
            
            # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¡¨ç¤º
            for i, (bar, size) in enumerate(zip(bars1, sample_sizes)):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                        f'n={size:,}', ha='center', va='bottom', fontsize=9)
            
            # å³å´: Time â†’ PlaceRate ç›¸é–¢
            bars2 = ax2.bar(x, time_place_corrs, alpha=0.7, color='lightcoral')
            ax2.set_title('è·é›¢åˆ¥: èµ°ç ´ã‚¿ã‚¤ãƒ  â†’ è¤‡å‹ç‡ ç›¸é–¢')
            ax2.set_xlabel('è·é›¢ã‚«ãƒ†ã‚´ãƒª')
            ax2.set_ylabel('ç›¸é–¢ä¿‚æ•°')
            ax2.set_xticks(x)
            ax2.set_xticklabels(categories)
            ax2.grid(True, alpha=0.3)
            
            # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¡¨ç¤º
            for i, (bar, size) in enumerate(zip(bars2, sample_sizes)):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                        f'n={size:,}', ha='center', va='bottom', fontsize=9)
            
            plt.tight_layout()
            plt.savefig(output_dir / 'distance_specific_effects.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"âŒ è·é›¢åˆ¥åŠ¹æœå¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def _plot_mediation_effects(self, output_dir: Path, mediation_results: Dict[str, Any]) -> None:
        """åª’ä»‹åŠ¹æœã®å¯è¦–åŒ–"""
        try:
            fig, ax = plt.subplots(1, 1, figsize=(12, 8))
            
            effects = ['ç·åŠ¹æœ', 'ç›´æ¥åŠ¹æœ', 'é–“æ¥åŠ¹æœï¼ˆåª’ä»‹ï¼‰']
            values = [
                mediation_results.get('total_effect', 0),
                mediation_results.get('direct_effect', 0),
                mediation_results.get('indirect_effect', 0)
            ]
            colors = ['blue', 'green', 'orange']
            
            bars = ax.bar(effects, values, color=colors, alpha=0.7)
            
            ax.set_title('åª’ä»‹åŠ¹æœåˆ†æ: RaceLevel â†’ Time â†’ PlaceRate')
            ax.set_ylabel('åŠ¹æœã®å¤§ãã•')
            ax.grid(True, alpha=0.3)
            
            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for bar, value in zip(bars, values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # åª’ä»‹æ¯”ç‡ã‚’è¡¨ç¤º
            mediation_ratio = mediation_results.get('mediation_ratio', 0)
            ax.text(0.02, 0.98, f'åª’ä»‹æ¯”ç‡: {mediation_ratio:.3f}\nï¼ˆé–“æ¥åŠ¹æœ/ç·åŠ¹æœï¼‰', 
                   transform=ax.transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
            
            plt.tight_layout()
            plt.savefig(output_dir / 'mediation_effects.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"âŒ åª’ä»‹åŠ¹æœå¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}") 