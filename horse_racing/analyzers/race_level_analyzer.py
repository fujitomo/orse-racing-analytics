"""
ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒ¬ãƒ¼ã‚¹ã®ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚„è³é‡‘é¡ãªã©ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’åˆ†æã—ã¾ã™ã€‚
"""

from typing import Dict, Any
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from horse_racing.base.analyzer import BaseAnalyzer, AnalysisConfig
from horse_racing.data.loader import RaceDataLoader
from horse_racing.visualization.plotter import RacePlotter
from horse_racing.analyzers.causal_analyzer import analyze_causal_relationship, generate_causal_analysis_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import matplotlib as mpl
import logging
from pathlib import Path

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’èª¿æ•´
loader_logger = logging.getLogger('horse_racing.data.loader')
loader_logger.setLevel(logging.WARNING)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams['font.family'] = 'MS Gothic'
mpl.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 12

class RaceLevelAnalyzer(BaseAnalyzer):
    """ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«åˆ†æã‚¯ãƒ©ã‚¹"""

    # ã‚°ãƒ¬ãƒ¼ãƒ‰å®šç¾©
    GRADE_LEVELS = {
        1: {"name": "G1", "weight": 10.0, "base_level": 9},
        2: {"name": "G2", "weight": 8.0, "base_level": 8},
        3: {"name": "G3", "weight": 7.0, "base_level": 7},
        4: {"name": "é‡è³", "weight": 6.0, "base_level": 6},
        5: {"name": "ç‰¹åˆ¥", "weight": 5.0, "base_level": 5},
        6: {"name": "L", "weight": 5.5, "base_level": 5.5}
    }

    # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«è¨ˆç®—ã®é‡ã¿ä»˜ã‘å®šç¾©
    LEVEL_WEIGHTS = {
        "grade_weight": 0.60,
        "prize_weight": 0.40,
        "field_size_weight": 0.10,
        "competition_weight": 0.20,
    }

    def __init__(self, config: AnalysisConfig):
        """åˆæœŸåŒ–"""
        super().__init__(config)
        self.plotter = RacePlotter(self.output_dir)
        self.loader = RaceDataLoader(config.input_path)

    @staticmethod
    def determine_grade_by_prize(row: pd.Series) -> int:
        """è³é‡‘ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°"""
        prize = row.get("æœ¬è³é‡‘")
        if pd.isna(prize):
            return None
            
        match prize:
            case p if p >= 10000: return 1
            case p if p >= 7000: return 2
            case p if p >= 4500: return 3
            case p if p >= 3500: return 4
            case p if p >= 2000: return 6
            case _: return 5

    @staticmethod
    def determine_grade(row: pd.Series) -> int:
        """ãƒ¬ãƒ¼ã‚¹åã¨ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’åˆ¤å®šã™ã‚‹"""
        race_name = str(row.get("ãƒ¬ãƒ¼ã‚¹å", "")).upper().replace("ï¼§", "G").replace("ï¼¬", "L")
        race_type = row.get("ç¨®åˆ¥", 99)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        keyword_to_grade = {
            "G1": 1,
            "G2": 2,
            "G3": 3,
            "é‡è³": 4,
            "L": 6,
        }

        # ãƒãƒƒãƒã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™
        for keyword, grade in keyword_to_grade.items():
            if keyword in race_name:
                return grade
        
        # ãƒãƒƒãƒã™ã‚‹ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯è³é‡‘ã«ã‚ˆã‚‹åˆ¤å®š
        if "æœ¬è³é‡‘" in row.index:
            prize_grade = RaceLevelAnalyzer.determine_grade_by_prize(row)
            if prize_grade is not None:
                return prize_grade
        
        # ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹åˆ¤å®š
        match race_type:
            case 11 | 12: return 5
            case 13 | 14: return 5
            case 20:
                if "J.G1" in race_name: return 1
                if "J.G2" in race_name: return 2
                if "J.G3" in race_name: return 3
                return 5
            case _: return 5

    def load_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        try:
            return self.loader.load()
        except FileNotFoundError as e:
            logger.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def preprocess_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        try:
            df = self.df.copy()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ§‹é€ ã‚’ç¢ºèª
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ ä¸€è¦§:")
            logger.info(df.columns.tolist())
            logger.info("\nãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­5è¡Œ:")
            logger.info(df.head())

            # ã‚«ãƒ©ãƒ åã®å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
            df.columns = df.columns.str.strip()

            # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.config.start_date or self.config.end_date:
                # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®ä½œæˆ
                try:
                    if 'å¹´æœˆæ—¥' in df.columns:
                        df['date'] = pd.to_datetime(df['å¹´æœˆæ—¥'].astype(str), format='%Y%m%d')
                    else:
                        # å¹´æœˆæ—¥ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯å¹´ã€å›ã€æ—¥ã‹ã‚‰ä½œæˆ
                        df['date'] = pd.to_datetime(
                            df['å¹´'].astype(str) + 
                            df['å›'].astype(str).str.zfill(2) + 
                            df['æ—¥'].astype(str).str.zfill(2)
                        )
                except Exception as e:
                    logger.error(f"æ—¥ä»˜ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                    raise

                # æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.config.start_date:
                    df = df[df['date'] >= self.config.start_date]
                if self.config.end_date:
                    df = df[df['date'] <= self.config.end_date]

            # æœ€å°ãƒ¬ãƒ¼ã‚¹æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.config.min_races:
                # é¦¬ã”ã¨ã®ãƒ¬ãƒ¼ã‚¹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                race_counts = df['é¦¬å'].value_counts()
                valid_horses = race_counts[race_counts >= self.config.min_races].index
                df = df[df['é¦¬å'].isin(valid_horses)]

                if len(df) == 0:
                    raise ValueError(f"æ¡ä»¶ã‚’æº€ãŸã™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆæœ€å°ãƒ¬ãƒ¼ã‚¹æ•°: {self.config.min_races}ï¼‰")

            logger.info(f"  ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(df):,}è¡Œ")
            logger.info(f"  ğŸ å¯¾è±¡é¦¬æ•°: {df['é¦¬å'].nunique():,}é ­")

            return df

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def calculate_feature(self) -> pd.DataFrame:
        """ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—"""
        df = self.df.copy()
        
        # ã‚«ãƒ©ãƒ åã®å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        df.columns = df.columns.str.strip()
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’é¸æŠ
        required_columns = [
            'å ´ã‚³ãƒ¼ãƒ‰', 'å¹´', 'å›', 'æ—¥', 'R', 'é¦¬å', 'è·é›¢', 'ç€é †',
            'ãƒ¬ãƒ¼ã‚¹å', 'ç¨®åˆ¥', 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰', 'é¦¬ç•ª', 'ã‚°ãƒ¬ãƒ¼ãƒ‰',
            'æœ¬è³é‡‘', '1ç€è³é‡‘', 'å¹´æœˆæ—¥'
        ]
        df = df[required_columns]

        # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«é–¢é€£ã®ç‰¹å¾´é‡ã‚’è¿½åŠ 
        df["race_level"] = 0.0
        df["is_win"] = df["ç€é †"] == 1
        df["is_placed"] = df["ç€é †"] <= 3

        # åŸºæœ¬ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—
        grade_level = self._calculate_grade_level(df)
        prize_level = self._calculate_prize_level(df)

        # é‡ã¿ä»˜ã‘åˆç®—
        df["race_level"] = (
            grade_level * self.LEVEL_WEIGHTS["grade_weight"] +
            prize_level * self.LEVEL_WEIGHTS["prize_weight"]
        )

        # è·é›¢ã«ã‚ˆã‚‹åŸºæœ¬è£œæ­£
        distance_weights = {
            (0, 1400): 0.85,     # ã‚¹ãƒ—ãƒªãƒ³ãƒˆ
            (1401, 1800): 1.00,  # ãƒã‚¤ãƒ«
            (1801, 2000): 1.35,  # ä¸­è·é›¢
            (2001, 2400): 1.45,  # ä¸­é•·è·é›¢
            (2401, 9999): 1.25,  # é•·è·é›¢
        }

        # è·é›¢å¸¯ã«ã‚ˆã‚‹åŸºæœ¬è£œæ­£ã‚’é©ç”¨
        for (min_dist, max_dist), weight in distance_weights.items():
            mask = (df["è·é›¢"] >= min_dist) & (df["è·é›¢"] <= max_dist)
            df.loc[mask, "race_level"] *= weight

        # 2000mç‰¹åˆ¥ãƒœãƒ¼ãƒŠã‚¹
        mask_2000m = (df["è·é›¢"] >= 1900) & (df["è·é›¢"] <= 2100)
        df.loc[mask_2000m, "race_level"] *= 1.35

        # ã‚°ãƒ¬ãƒ¼ãƒ‰ã¨è·é›¢ã®ç›¸äº’ä½œç”¨ã‚’è€ƒæ…®
        high_grade_mask = df["ã‚°ãƒ¬ãƒ¼ãƒ‰"].isin([1, 2, 3])  # G1, G2, G3
        optimal_distance_mask = (df["è·é›¢"] >= 1800) & (df["è·é›¢"] <= 2400)
        df.loc[high_grade_mask & optimal_distance_mask, "race_level"] *= 1.15

        # æœ€çµ‚çš„ãªæ­£è¦åŒ–ï¼ˆ0-10ã®ç¯„å›²ã«åã‚ã‚‹ï¼‰
        df["race_level"] = self.normalize_values(df["race_level"])

        return df

    def _perform_logistic_regression_analysis(self) -> Dict[str, Any]:
        """ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æã‚’å®Ÿè¡Œ"""
        df = self.df.copy()
        df['is_win_or_place'] = df['ç€é †'].apply(lambda x: 1 if x in [1, 2] else 0)
        df['is_placed_only'] = df['ç€é †'].apply(lambda x: 1 if x <= 3 else 0)
        
        # NAå€¤ã¨ç„¡é™å¤§å€¤ã®å‡¦ç†
        df['race_level'] = df['race_level'].fillna(0)
        df['race_level'] = df['race_level'].replace([np.inf, -np.inf], df['race_level'].replace([np.inf, -np.inf], np.nan).max())
        
        # å‹ç‡ã®ãƒ¢ãƒ‡ãƒ«
        X_win = df[['race_level']].values
        y_win = df['is_win_or_place'].values
        
        # è¤‡å‹ç‡ã®ãƒ¢ãƒ‡ãƒ«
        X_place = df[['race_level']].values
        y_place = df['is_placed_only'].values
        
        # æ¨™æº–åŒ–
        scaler_win = StandardScaler()
        X_win_scaled = scaler_win.fit_transform(X_win)
        
        scaler_place = StandardScaler()
        X_place_scaled = scaler_place.fit_transform(X_place)
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        X_win_train, X_win_test, y_win_train, y_win_test = train_test_split(
            X_win_scaled, y_win, test_size=0.3, random_state=42, stratify=y_win
        )
        
        X_place_train, X_place_test, y_place_train, y_place_test = train_test_split(
            X_place_scaled, y_place, test_size=0.3, random_state=42, stratify=y_place
        )
        
        # ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆå‹ç‡ï¼‰
        model_win = LogisticRegression(
            random_state=42,
            max_iter=1000,
            class_weight='balanced',
            solver='liblinear'
        )
        model_win.fit(X_win_train, y_win_train)
        
        # ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆè¤‡å‹ç‡ï¼‰
        model_place = LogisticRegression(
            random_state=42,
            max_iter=1000,
            class_weight='balanced',
            solver='liblinear'
        )
        model_place.fit(X_place_train, y_place_train)
        
        # äºˆæ¸¬ã¨è©•ä¾¡ï¼ˆå‹ç‡ï¼‰
        y_win_pred = model_win.predict(X_win_test)
        accuracy_win = accuracy_score(y_win_test, y_win_pred)
        report_win = classification_report(y_win_test, y_win_pred, zero_division=0)
        conf_matrix_win = confusion_matrix(y_win_test, y_win_pred)
        
        # äºˆæ¸¬ã¨è©•ä¾¡ï¼ˆè¤‡å‹ç‡ï¼‰
        y_place_pred = model_place.predict(X_place_test)
        accuracy_place = accuracy_score(y_place_test, y_place_pred)
        report_place = classification_report(y_place_test, y_place_pred, zero_division=0)
        conf_matrix_place = confusion_matrix(y_place_test, y_place_pred)
        
        return {
            "win": {
                "model": model_win,
                "scaler": scaler_win,
                "accuracy": accuracy_win,
                "report": report_win,
                "conf_matrix": conf_matrix_win,
            },
            "place": {
                "model": model_place,
                "scaler": scaler_place,
                "accuracy": accuracy_place,
                "report": report_place,
                "conf_matrix": conf_matrix_place,
            },
            "data": df
        }

    def analyze(self) -> Dict[str, Any]:
        """åˆ†æã®å®Ÿè¡Œ"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ§‹é€ ã‚’ç¢ºèª
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ ä¸€è¦§:")
            logger.info(self.df.columns.tolist())
            logger.info("\nãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­5è¡Œ:")
            logger.info(self.df.head())
            
            # åŸºæœ¬çš„ãªç›¸é–¢åˆ†æ
            correlation_stats = self._perform_correlation_analysis(self._calculate_horse_stats())
            results = {'correlation_stats': correlation_stats}
            
            # å› æœé–¢ä¿‚åˆ†æã®è¿½åŠ 
            causal_results = analyze_causal_relationship(self.df)
            results['causal_analysis'] = causal_results
            
            # å› æœé–¢ä¿‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
            output_dir = Path(self.config.output_dir)
            generate_causal_analysis_report(causal_results, output_dir)
            
            logger.info("âœ… å› æœé–¢ä¿‚åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            return results
            
        except Exception as e:
            logger.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def visualize(self) -> None:
        """åˆ†æçµæœã®å¯è¦–åŒ–"""
        try:
            if not self.stats:
                raise ValueError("åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«analyzeãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

            output_dir = Path(self.config.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            # ç›¸é–¢åˆ†æã®å¯è¦–åŒ–
            self._visualize_correlations()
            
            # å› æœé–¢ä¿‚åˆ†æã®å¯è¦–åŒ–
            if 'causal_analysis' in self.stats:
                self._visualize_causal_analysis()

        except Exception as e:
            logger.error(f"å¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def _visualize_causal_analysis(self) -> None:
        """å› æœé–¢ä¿‚åˆ†æã®å¯è¦–åŒ–"""
        causal_results = self.stats.get('causal_analysis', {})
        output_dir = Path(self.config.output_dir) / 'causal_analysis'
        output_dir.mkdir(parents=True, exist_ok=True)

        # æ™‚é–“çš„å…ˆè¡Œæ€§ã®å¯è¦–åŒ–
        if 'temporal_precedence' in causal_results:
            self._plot_temporal_precedence(output_dir)

        # ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å¯è¦–åŒ–
        if 'mechanism' in causal_results:
            self._plot_mechanism_analysis(output_dir)

        # äº¤çµ¡å› å­ã®å¯è¦–åŒ–
        if 'confounding_factors' in causal_results:
            self._plot_confounding_factors(output_dir)

    def _plot_temporal_precedence(self, output_dir: Path) -> None:
        """æ™‚é–“çš„å…ˆè¡Œæ€§ã®å¯è¦–åŒ–"""
        plt.figure(figsize=(10, 6))
        horse_data = []

        for horse in self.df['é¦¬å'].unique():
            horse_races = self.df[self.df['é¦¬å'] == horse].sort_values('å¹´æœˆæ—¥')
            if len(horse_races) >= 6:
                initial_level = horse_races['race_level'].iloc[:3].mean()
                later_performance = (horse_races['ç€é †'] <= 3).iloc[3:].mean()
                horse_data.append({
                    'åˆæœŸãƒ¬ãƒ™ãƒ«': initial_level,
                    'å¾ŒæœŸæˆç¸¾': later_performance
                })

        if horse_data:
            df_temporal = pd.DataFrame(horse_data)
            plt.scatter(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«'], df_temporal['å¾ŒæœŸæˆç¸¾'], alpha=0.5)
            plt.xlabel('åˆæœŸãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
            plt.ylabel('å¾ŒæœŸè¤‡å‹ç‡')
            plt.title('åˆæœŸãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨å¾ŒæœŸæˆç¸¾ã®é–¢ä¿‚')
            
            # å›å¸°ç›´ç·šã®è¿½åŠ 
            z = np.polyfit(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«'], df_temporal['å¾ŒæœŸæˆç¸¾'], 1)
            p = np.poly1d(z)
            plt.plot(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«'], p(df_temporal['åˆæœŸãƒ¬ãƒ™ãƒ«']), "r--", alpha=0.8)
            
            plt.savefig(output_dir / 'temporal_precedence.png')
            plt.close()

    def _plot_mechanism_analysis(self, output_dir: Path) -> None:
        """ãƒ¡ã‚«ãƒ‹ã‚ºãƒ åˆ†æã®å¯è¦–åŒ–"""
        plt.figure(figsize=(12, 6))

        # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨æˆç¸¾ã®é–¢ä¿‚ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        level_performance = []
        for horse in self.df['é¦¬å'].unique():
            horse_races = self.df[self.df['é¦¬å'] == horse]
            if len(horse_races) >= 6:
                avg_level = horse_races['race_level'].mean()
                win_rate = (horse_races['ç€é †'] == 1).mean()
                place_rate = (horse_races['ç€é †'] <= 3).mean()

                level_performance.append({
                    'å¹³å‡ãƒ¬ãƒ™ãƒ«': avg_level,
                    'å‹ç‡': win_rate,
                    'è¤‡å‹ç‡': place_rate
                })

        if level_performance:
            df_mechanism = pd.DataFrame(level_performance)

            plt.subplot(1, 2, 1)
            plt.scatter(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['å‹ç‡'], alpha=0.5)
            z = np.polyfit(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['å‹ç‡'], 1)
            p = np.poly1d(z)
            plt.plot(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], p(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«']), "r--", alpha=0.8)
            plt.title('ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨å‹ç‡ã®é–¢ä¿‚')
            plt.xlabel('å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
            plt.ylabel('å‹ç‡')

            plt.subplot(1, 2, 2)
            plt.scatter(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['è¤‡å‹ç‡'], alpha=0.5)
            z = np.polyfit(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], df_mechanism['è¤‡å‹ç‡'], 1)
            p = np.poly1d(z)
            plt.plot(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«'], p(df_mechanism['å¹³å‡ãƒ¬ãƒ™ãƒ«']), "r--", alpha=0.8)
            plt.title('ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–¢ä¿‚')
            plt.xlabel('å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
            plt.ylabel('è¤‡å‹ç‡')

            plt.tight_layout()
            plt.savefig(output_dir / 'mechanism_analysis.png')
            plt.close()

    def _plot_confounding_factors(self, output_dir: Path) -> None:
        """äº¤çµ¡å› å­ã®å¯è¦–åŒ–"""
        confounders = ['å ´ã‚³ãƒ¼ãƒ‰', 'è·é›¢', 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰']

        for confounder in confounders:
            if confounder in self.df.columns:
                plt.figure(figsize=(10, 6))

                # äº¤çµ¡å› å­ã”ã¨ã®å¹³å‡æˆç¸¾ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
                grouped_stats = self.df.groupby(confounder).agg({
                    'race_level': 'mean',
                    'ç€é †': lambda x: (x <= 3).mean()
                }).reset_index()

                plt.scatter(grouped_stats['race_level'], grouped_stats['ç€é †'],
                          s=100, alpha=0.6)

                # ãƒ©ãƒ™ãƒ«ã®è¿½åŠ 
                for i, row in grouped_stats.iterrows():
                    plt.annotate(row[confounder],
                               (row['race_level'], row['ç€é †']),
                               xytext=(5, 5), textcoords='offset points')

                plt.title(f'{confounder}ã«ã‚ˆã‚‹äº¤çµ¡åŠ¹æœ')
                plt.xlabel('å¹³å‡ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«')
                plt.ylabel('è¤‡å‹ç‡')

                plt.savefig(output_dir / f'confounding_{confounder}.png')
                plt.close()

    def _calculate_grade_level(self, df: pd.DataFrame) -> pd.Series:
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ã«åŸºã¥ããƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
        grade_level = df["ã‚°ãƒ¬ãƒ¼ãƒ‰"].map(
            lambda x: self.GRADE_LEVELS[x]["base_level"] if pd.notna(x) else 5.0
        )

        for grade, values in self.GRADE_LEVELS.items():
            mask = df["ã‚°ãƒ¬ãƒ¼ãƒ‰"] == grade
            grade_level.loc[mask & df["is_win"]] += values["weight"]
            grade_level.loc[mask & df["is_placed"] & ~df["is_win"]] += values["weight"] * 0.5

        return self.normalize_values(grade_level)

    def _calculate_prize_level(self, df: pd.DataFrame) -> pd.Series:
        """è³é‡‘ã«åŸºã¥ããƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
        prize_level = np.log1p(df["1ç€è³é‡‘"]) / np.log1p(df["1ç€è³é‡‘"].max()) * 9.95
        return self.normalize_values(prize_level)

    def _calculate_horse_stats(self) -> pd.DataFrame:
        """é¦¬ã”ã¨ã®åŸºæœ¬çµ±è¨ˆã‚’è¨ˆç®—"""
        # é¦¬ã”ã¨ã®åŸºæœ¬çµ±è¨ˆ
        horse_stats = self.df.groupby("é¦¬å").agg({
            "race_level": ["max", "mean"],
            "is_win": "sum",
            "is_placed": "sum",
            "ç€é †": "count",
            "ã‚°ãƒ¬ãƒ¼ãƒ‰": lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else None
        }).reset_index()

        # ã‚«ãƒ©ãƒ åã®æ•´ç†
        horse_stats.columns = ["é¦¬å", "æœ€é«˜ãƒ¬ãƒ™ãƒ«", "å¹³å‡ãƒ¬ãƒ™ãƒ«", "å‹åˆ©æ•°", "è¤‡å‹æ•°", "å‡ºèµ°å›æ•°", "ä¸»æˆ¦ã‚°ãƒ¬ãƒ¼ãƒ‰"]
        
        # ãƒ¬ãƒ¼ã‚¹å›æ•°ãŒmin_raceså›ä»¥ä¸Šã®é¦¬ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        min_races = self.config.min_races if hasattr(self.config, 'min_races') else 3
        horse_stats = horse_stats[horse_stats["å‡ºèµ°å›æ•°"] >= min_races]
        
        # å‹ç‡ã¨è¤‡å‹ç‡ã®è¨ˆç®—
        horse_stats["win_rate"] = horse_stats["å‹åˆ©æ•°"] / horse_stats["å‡ºèµ°å›æ•°"]
        horse_stats["place_rate"] = horse_stats["è¤‡å‹æ•°"] / horse_stats["å‡ºèµ°å›æ•°"]
        
        return horse_stats

    def _calculate_grade_stats(self) -> pd.DataFrame:
        """ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—"""
        grade_stats = self.df.groupby("ã‚°ãƒ¬ãƒ¼ãƒ‰").agg({
            "is_win": ["mean", "count"],
            "is_placed": "mean",
            "race_level": ["mean", "std"]
        }).reset_index()

        grade_stats.columns = [
            "ã‚°ãƒ¬ãƒ¼ãƒ‰", "å‹ç‡", "ãƒ¬ãƒ¼ã‚¹æ•°", "è¤‡å‹ç‡",
            "å¹³å‡ãƒ¬ãƒ™ãƒ«", "ãƒ¬ãƒ™ãƒ«æ¨™æº–åå·®"
        ]

        return grade_stats

    def _perform_correlation_analysis(self, horse_stats: pd.DataFrame) -> Dict[str, Any]:
        """ç›¸é–¢åˆ†æã‚’å®Ÿè¡Œ"""
        # TODO:æ¬ æå€¤ã®ã¤ã„ã¦èª¿æŸ»äºˆå®š
        analysis_data = horse_stats.dropna(subset=['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'å¹³å‡ãƒ¬ãƒ™ãƒ«', 'win_rate', 'place_rate'])
        
        if len(analysis_data) == 0:
            return {}

        # æ¨™æº–åå·®ãŒ0ã®å ´åˆã®å‡¦ç†
        # TODO:æ¨™æº–åå·®ãŒ0ã®å ´åˆã®å‡¦ç†ã‚’èª¿æŸ»äºˆå®š
        stddev = analysis_data[['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'å¹³å‡ãƒ¬ãƒ™ãƒ«', 'win_rate', 'place_rate']].std()
        if (stddev == 0).any():
            return {
                "correlation_win_max": 0.0,
                "correlation_place_max": 0.0,
                "correlation_win_avg": 0.0,
                "correlation_place_avg": 0.0,
                "model_win_max": None,
                "model_place_max": None,
                "model_win_avg": None,
                "model_place_avg": None,
                "r2_win_max": 0.0,
                "r2_place_max": 0.0,
                "r2_win_avg": 0.0,
                "r2_place_avg": 0.0
            }

        # æœ€é«˜ãƒ¬ãƒ™ãƒ« - å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_win_max = analysis_data[['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'win_rate']].corr().iloc[0, 1]
        X_win_max = analysis_data['æœ€é«˜ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        y_win = analysis_data['win_rate'].values
        model_win_max = LinearRegression()
        model_win_max.fit(X_win_max, y_win)
        r2_win_max = model_win_max.score(X_win_max, y_win)

        # æœ€é«˜ãƒ¬ãƒ™ãƒ« - è¤‡å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_place_max = analysis_data[['æœ€é«˜ãƒ¬ãƒ™ãƒ«', 'place_rate']].corr().iloc[0, 1]
        X_place_max = analysis_data['æœ€é«˜ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        y_place = analysis_data['place_rate'].values
        model_place_max = LinearRegression()
        model_place_max.fit(X_place_max, y_place)
        r2_place_max = model_place_max.score(X_place_max, y_place)

        # å¹³å‡ãƒ¬ãƒ™ãƒ« - å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_win_avg = analysis_data[['å¹³å‡ãƒ¬ãƒ™ãƒ«', 'win_rate']].corr().iloc[0, 1]
        X_win_avg = analysis_data['å¹³å‡ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_win_avg = LinearRegression()
        model_win_avg.fit(X_win_avg, y_win)
        r2_win_avg = model_win_avg.score(X_win_avg, y_win)

        # å¹³å‡ãƒ¬ãƒ™ãƒ« - è¤‡å‹ç‡ã®ç›¸é–¢ä¿‚æ•°ã¨å›å¸°åˆ†æ
        correlation_place_avg = analysis_data[['å¹³å‡ãƒ¬ãƒ™ãƒ«', 'place_rate']].corr().iloc[0, 1]
        X_place_avg = analysis_data['å¹³å‡ãƒ¬ãƒ™ãƒ«'].values.reshape(-1, 1)
        model_place_avg = LinearRegression()
        model_place_avg.fit(X_place_avg, y_place)
        r2_place_avg = model_place_avg.score(X_place_avg, y_place)

        return {
            # æœ€é«˜ãƒ¬ãƒ™ãƒ«ç³»
            "correlation_win_max": correlation_win_max,
            "correlation_place_max": correlation_place_max,
            "model_win_max": model_win_max,
            "model_place_max": model_place_max,
            "r2_win_max": r2_win_max,
            "r2_place_max": r2_place_max,
            # å¹³å‡ãƒ¬ãƒ™ãƒ«ç³»
            "correlation_win_avg": correlation_win_avg,
            "correlation_place_avg": correlation_place_avg,
            "model_win_avg": model_win_avg,
            "model_place_avg": model_place_avg,
            "r2_win_avg": r2_win_avg,
            "r2_place_avg": r2_place_avg,
            # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ—¢å­˜ã®ã‚­ãƒ¼ã‚‚æ®‹ã™
            "correlation_win": correlation_win_max,
            "correlation_place": correlation_place_max,
            "model_win": model_win_max,
            "model_place": model_place_max,
            "r2_win": r2_win_max,
            "r2_place": r2_place_max
        } 