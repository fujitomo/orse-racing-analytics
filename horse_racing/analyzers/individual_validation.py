"""
ç¬¬5éƒ¨: å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ï¼ˆç›¸é–¢åˆ†æï¼‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒ¬ãƒãƒ¼ãƒˆã§è¦æ±‚ã•ã‚Œã¦ã„ã‚‹å„è¦ç´ ã®å€‹åˆ¥æ¤œè¨¼ã‚’å®Ÿè£…
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from pathlib import Path
import logging
from typing import Dict, Any, Tuple
import json

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆçµ±ä¸€è¨­å®šã‚’ä½¿ç”¨ï¼‰
from horse_racing.utils.font_config import setup_japanese_fonts
setup_japanese_fonts(suppress_warnings=True)

class IndividualElementValidator:
    """
    å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ã‚¯ãƒ©ã‚¹
    ãƒ¬ãƒãƒ¼ãƒˆç¬¬5éƒ¨ã«å¯¾å¿œã™ã‚‹åˆ†æã‚’å®Ÿè£…
    """
    
    def __init__(self, race_data: pd.DataFrame, output_dir: Path):
        """
        åˆæœŸåŒ–
        
        Args:
            race_data: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.race_data = race_data
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # çµæœä¿å­˜ç”¨
        self.results = {}
        
    def calculate_point_levels(self) -> pd.DataFrame:
        """
        ãƒã‚¤ãƒ³ãƒˆåˆ¶ã§ã®ãƒ¬ãƒ™ãƒ«è¨ˆç®—
        ãƒ¬ãƒãƒ¼ãƒˆã®3.1ç¯€ã®ä»•æ§˜ã«åŸºã¥ã
        """
        logger.info("ğŸ† ãƒã‚¤ãƒ³ãƒˆåˆ¶ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ä¸­...")
        
        df = self.race_data.copy()
        
        # 1. ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—ï¼ˆè³é‡‘ã«åŸºã¥ãï¼‰
        df['grade_points'] = self._calculate_grade_points(df)
        
        # 2. ç«¶é¦¬å ´ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—ï¼ˆè³é‡‘ã«åŸºã¥ãï¼‰
        df['venue_points'] = self._calculate_venue_points(df)
        
        # 3. è·é›¢ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãï¼‰
        df['distance_points'] = self._calculate_distance_points(df)
        
        logger.info(f"âœ… ãƒã‚¤ãƒ³ãƒˆè¨ˆç®—å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        
        return df
        
    def _calculate_grade_points(self, df: pd.DataFrame) -> pd.Series:
        """
        ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨ˆç®—
        1ç€è³é‡‘ã®ä¸­å¤®å€¤ã«åŸºã¥ãMinMaxScaleræ­£è¦åŒ–
        """
        # è³é‡‘ã‚«ãƒ©ãƒ ã®ç‰¹å®š
        prize_col = self._find_prize_column(df)
        if prize_col is None:
            logger.warning("âš ï¸ è³é‡‘ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’0ã§è¨­å®š")
            return pd.Series([0.0] * len(df), index=df.index)
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥è³é‡‘ä¸­å¤®å€¤ã®è¨ˆç®—
        grade_col = self._find_grade_column(df)
        if grade_col is None:
            logger.warning("âš ï¸ ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’0ã§è¨­å®š")
            return pd.Series([0.0] * len(df), index=df.index)
        
        # è³é‡‘ãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤å¤‰æ›
        df[prize_col] = pd.to_numeric(df[prize_col], errors='coerce')
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ã®è³é‡‘ä¸­å¤®å€¤
        grade_prize_median = df.groupby(grade_col)[prize_col].median().dropna()
        
        if len(grade_prize_median) == 0:
            logger.warning("âš ï¸ ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥è³é‡‘ãƒ‡ãƒ¼ã‚¿ãŒè¨ˆç®—ã§ãã¾ã›ã‚“ã€‚ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’0ã§è¨­å®š")
            return pd.Series([0.0] * len(df), index=df.index)
        
        # MinMaxScalerã«ã‚ˆã‚‹æ­£è¦åŒ–ï¼ˆ0-9ãƒã‚¤ãƒ³ãƒˆï¼‰
        scaler = MinMaxScaler(feature_range=(0, 9))
        normalized_values = scaler.fit_transform(grade_prize_median.values.reshape(-1, 1)).flatten()
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰â†’ãƒã‚¤ãƒ³ãƒˆã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
        grade_points_map = dict(zip(grade_prize_median.index, normalized_values))
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«é©ç”¨
        grade_points = df[grade_col].map(grade_points_map).fillna(0)
        
        logger.info(f"ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¯„å›²: {grade_points.min():.2f} - {grade_points.max():.2f}")
        
        return grade_points
        
    def _calculate_venue_points(self, df: pd.DataFrame) -> pd.Series:
        """
        ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆã®è¨ˆç®—
        ç«¶é¦¬å ´åˆ¥1ç€è³é‡‘ä¸­å¤®å€¤ã«åŸºã¥ãMinMaxScaleræ­£è¦åŒ–
        """
        # è³é‡‘ã‚«ãƒ©ãƒ ã¨ç«¶é¦¬å ´ã‚«ãƒ©ãƒ ã®ç‰¹å®š
        prize_col = self._find_prize_column(df)
        venue_col = self._find_venue_column(df)
        
        if prize_col is None or venue_col is None:
            logger.warning("âš ï¸ è³é‡‘ã¾ãŸã¯ç«¶é¦¬å ´ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆã‚’0ã§è¨­å®š")
            return pd.Series([0.0] * len(df), index=df.index)
        
        # è³é‡‘ãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤å¤‰æ›
        df[prize_col] = pd.to_numeric(df[prize_col], errors='coerce')
        
        # ç«¶é¦¬å ´åˆ¥ã®è³é‡‘ä¸­å¤®å€¤
        venue_prize_median = df.groupby(venue_col)[prize_col].median().dropna()
        
        if len(venue_prize_median) == 0:
            logger.warning("âš ï¸ ç«¶é¦¬å ´åˆ¥è³é‡‘ãƒ‡ãƒ¼ã‚¿ãŒè¨ˆç®—ã§ãã¾ã›ã‚“ã€‚ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆã‚’0ã§è¨­å®š")
            return pd.Series([0.0] * len(df), index=df.index)
        
        # MinMaxScalerã«ã‚ˆã‚‹æ­£è¦åŒ–ï¼ˆ0-9ãƒã‚¤ãƒ³ãƒˆï¼‰
        scaler = MinMaxScaler(feature_range=(0, 9))
        normalized_values = scaler.fit_transform(venue_prize_median.values.reshape(-1, 1)).flatten()
        
        # ç«¶é¦¬å ´â†’ãƒã‚¤ãƒ³ãƒˆã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
        venue_points_map = dict(zip(venue_prize_median.index, normalized_values))
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«é©ç”¨
        venue_points = df[venue_col].map(venue_points_map).fillna(0)
        
        logger.info(f"ğŸ“Š ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆç¯„å›²: {venue_points.min():.2f} - {venue_points.max():.2f}")
        
        return venue_points
        
    def _calculate_distance_points(self, df: pd.DataFrame) -> pd.Series:
        """
        è·é›¢ãƒã‚¤ãƒ³ãƒˆã®è¨ˆç®—
        ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãè£œæ­£ä¿‚æ•°
        """
        distance_col = 'è·é›¢'
        if distance_col not in df.columns:
            logger.warning("âš ï¸ è·é›¢ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è·é›¢ãƒã‚¤ãƒ³ãƒˆã‚’1.0ã§è¨­å®š")
            return pd.Series([1.0] * len(df), index=df.index)
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãè·é›¢è£œæ­£ä¿‚æ•°ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ3.1ç¯€ã‚ˆã‚Šï¼‰
        distance_weights = {
            'sprint': 0.85,      # ~1400m
            'mile': 1.00,        # 1401-1800m (åŸºæº–)
            'middle': 1.35,      # 1801-2000m
            'long_middle': 1.45, # 2001-2400m
            'long': 1.25         # 2401m~
        }
        
        def categorize_distance(distance):
            if distance <= 1400:
                return distance_weights['sprint']
            elif distance <= 1800:
                return distance_weights['mile']
            elif distance <= 2000:
                return distance_weights['middle']
            elif distance <= 2400:
                return distance_weights['long_middle']
            else:
                return distance_weights['long']
        
        distance_points = df[distance_col].apply(categorize_distance)
        
        logger.info(f"ğŸ“Š è·é›¢ãƒã‚¤ãƒ³ãƒˆç¯„å›²: {distance_points.min():.2f} - {distance_points.max():.2f}")
        
        return distance_points
        
    def _find_prize_column(self, df: pd.DataFrame) -> str:
        """è³é‡‘ã‚«ãƒ©ãƒ ã‚’æ¢ç´¢"""
        prize_candidates = [
            '1ç€è³é‡‘(1ç€ç®—å…¥è³é‡‘è¾¼ã¿)',
            '1ç€è³é‡‘',
            'æœ¬è³é‡‘'
        ]
        for col in prize_candidates:
            if col in df.columns:
                return col
        return None
        
    def _find_grade_column(self, df: pd.DataFrame) -> str:
        """ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚«ãƒ©ãƒ ã‚’æ¢ç´¢"""
        grade_candidates = ['ã‚°ãƒ¬ãƒ¼ãƒ‰_x']
        for col in grade_candidates:
            if col in df.columns and df[col].nunique() > 1:
                return col
        return None
        
    def _find_venue_column(self, df: pd.DataFrame) -> str:
        """ç«¶é¦¬å ´ã‚«ãƒ©ãƒ ã‚’æ¢ç´¢"""
        venue_candidates = ['å ´å', 'ç«¶é¦¬å ´', 'å ´ã‚³ãƒ¼ãƒ‰']
        for col in venue_candidates:
            if col in df.columns:
                return col
        return None
        
    def calculate_horse_statistics(self, df_with_points: pd.DataFrame) -> pd.DataFrame:
        """
        é¦¬ã”ã¨ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        """
        logger.info("ğŸ é¦¬ã”ã¨ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ä¸­...")
        
        # è¤‡å‹åˆ¤å®š
        df_with_points['is_placed'] = df_with_points['ç€é †'] <= 3
        
        # é¦¬ã”ã¨ã®é›†è¨ˆ
        agg_dict = {
            'grade_points': ['mean', 'max'],
            'venue_points': ['mean', 'max'],
            'distance_points': ['mean'],
            'is_placed': ['sum', 'count']
        }
        # åˆæˆãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯å¹³å‡ãƒ»æœ€å¤§ã‚‚é›†è¨ˆ
        if 'race_point' in df_with_points.columns:
            agg_dict['race_point'] = ['mean', 'max']
        
        horse_stats = df_with_points.groupby('é¦¬å').agg(agg_dict).reset_index()
        
        # ã‚«ãƒ©ãƒ åã®æ•´ç†
        if 'race_point' in df_with_points.columns:
            horse_stats.columns = [
                'é¦¬å', 
                'å¹³å‡ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆ', 'æœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆ',
                'å¹³å‡ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆ', 'æœ€é«˜ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆ',
                'å¹³å‡è·é›¢ãƒã‚¤ãƒ³ãƒˆ',
                'è¤‡å‹å›æ•°', 'å‡ºèµ°å›æ•°',
                'å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ', 'æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ'
            ]
        else:
            horse_stats.columns = [
                'é¦¬å', 
                'å¹³å‡ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆ', 'æœ€é«˜ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆ',
                'å¹³å‡ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆ', 'æœ€é«˜ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆ',
                'å¹³å‡è·é›¢ãƒã‚¤ãƒ³ãƒˆ',
                'è¤‡å‹å›æ•°', 'å‡ºèµ°å›æ•°'
            ]
        
        # è¤‡å‹ç‡ã®è¨ˆç®—
        horse_stats['è¤‡å‹ç‡'] = horse_stats['è¤‡å‹å›æ•°'] / horse_stats['å‡ºèµ°å›æ•°']
        
        # æœ€å°å‡ºèµ°å›æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿
        min_races = 2
        horse_stats = horse_stats[horse_stats['å‡ºèµ°å›æ•°'] >= min_races]
        
        logger.info(f"ğŸ“Š åˆ†æå¯¾è±¡é¦¬æ•°: {len(horse_stats)}é ­ï¼ˆæœ€ä½{min_races}æˆ¦ä»¥ä¸Šï¼‰")
        
        return horse_stats

    def compute_composite_race_points(self, df_with_points: pd.DataFrame, weights: Dict[str, float]) -> pd.DataFrame:
        """
        åˆæˆãƒ¬ãƒ™ãƒ«ãƒã‚¤ãƒ³ãƒˆï¼ˆRacePointï¼‰ã‚’ç®—å‡º
        åŸºç¤ç‚¹ = grade_points*w1 + venue_points*w2
        è£œæ­£ä¿‚æ•° = 1 + w3*(distance_points - 1)
        RacePoint = åŸºç¤ç‚¹ * è£œæ­£ä¿‚æ•°
        """
        logger.info("ğŸ§® åˆæˆãƒ¬ãƒ™ãƒ«ãƒã‚¤ãƒ³ãƒˆï¼ˆRacePointï¼‰ã‚’è¨ˆç®—ä¸­...")
        df = df_with_points.copy()

        # é‡ã¿ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯å‡ç­‰å‰²ï¼‰
        w_grade = float(weights.get('grade_weight', 1/3))
        w_venue = float(weights.get('venue_weight', 1/3))
        w_distance = float(weights.get('distance_weight', 1/3))

        # åŸºç¤ç‚¹ï¼ˆè·é›¢ã¯è£œæ­£ã¨ã—ã¦åˆ¥æ‰±ã„ï¼‰
        base_score = df['grade_points'] * w_grade + df['venue_points'] * w_venue
        # è·é›¢è£œæ­£ï¼ˆw3=0ã§ç„¡è£œæ­£ã€w3=1ã§ãƒ•ãƒ«é©ç”¨ï¼‰
        distance_multiplier = 1.0 + w_distance * (df['distance_points'] - 1.0)

        df['race_point'] = base_score * distance_multiplier

        logger.info(
            "âœ… åˆæˆãƒã‚¤ãƒ³ãƒˆè¨ˆç®—å®Œäº†: base[min={:.2f}, max={:.2f}] Ã— mult[min={:.2f}, max={:.2f}] â†’ race_point[min={:.2f}, max={:.2f}]".format(
                float(base_score.min()), float(base_score.max()),
                float(distance_multiplier.min()), float(distance_multiplier.max()),
                float(df['race_point'].min()), float(df['race_point'].max())
            )
        )

        return df

    def _create_composite_scatter(self, x, y, y_pred, title: str, filename: str, xlabel: str) -> None:
        """åˆæˆãƒã‚¤ãƒ³ãƒˆã¨è¤‡å‹ç‡ã®æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šï¼‰ã‚’ä¿å­˜"""
        plt.figure(figsize=(12, 8))
        plt.scatter(x, y, alpha=0.6, s=50, color='steelblue', edgecolors='white', linewidth=0.5)
        sort_idx = np.argsort(x)
        plt.plot(x[sort_idx], y_pred[sort_idx], 'r-', linewidth=2, label='å›å¸°ç›´ç·š')
        plt.title(title, fontsize=14, pad=16)
        plt.xlabel(xlabel)
        plt.ylabel('è¤‡å‹ç‡')
        plt.ylim(-0.05, 1.05)
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig(self.output_dir / filename, dpi=300, bbox_inches='tight')
        plt.close()

    def create_composite_scatter_plots(self, horse_stats: pd.DataFrame) -> None:
        """
        åˆæˆãƒã‚¤ãƒ³ãƒˆï¼ˆå¹³å‡ãƒ»æœ€å¤§ï¼‰ã¨è¤‡å‹ç‡ã®æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šï¼‰ã‚’ä½œæˆ
        """
        if 'å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ' not in horse_stats.columns or 'æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ' not in horse_stats.columns:
            logger.warning("âš ï¸ åˆæˆãƒã‚¤ãƒ³ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ•£å¸ƒå›³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        # å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ
        valid_avg = horse_stats.dropna(subset=['å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ', 'è¤‡å‹ç‡'])
        if len(valid_avg) >= 10:
            X = valid_avg['å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ'].values.reshape(-1, 1)
            y = valid_avg['è¤‡å‹ç‡'].values
            model = LinearRegression().fit(X, y)
            y_pred = model.predict(X)
            r = np.corrcoef(valid_avg['å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ'], valid_avg['è¤‡å‹ç‡'])[0, 1]
            title = f'å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆã¨è¤‡å‹ç‡ã®é–¢ä¿‚\nç›¸é–¢ä¿‚æ•°: r={r:.3f}, RÂ²={model.score(X, y):.3f}'
            self._create_composite_scatter(
                x=valid_avg['å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ'].values,
                y=y,
                y_pred=y_pred,
                title=title,
                filename='avg_race_level_place_rate_scatter.png',
                xlabel='å¹³å‡åˆæˆãƒã‚¤ãƒ³ãƒˆ'
            )

        # æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ
        valid_max = horse_stats.dropna(subset=['æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ', 'è¤‡å‹ç‡'])
        if len(valid_max) >= 10:
            X = valid_max['æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ'].values.reshape(-1, 1)
            y = valid_max['è¤‡å‹ç‡'].values
            model = LinearRegression().fit(X, y)
            y_pred = model.predict(X)
            r = np.corrcoef(valid_max['æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ'], valid_max['è¤‡å‹ç‡'])[0, 1]
            title = f'æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆã¨è¤‡å‹ç‡ã®é–¢ä¿‚\nç›¸é–¢ä¿‚æ•°: r={r:.3f}, RÂ²={model.score(X, y):.3f}'
            self._create_composite_scatter(
                x=valid_max['æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ'].values,
                y=y,
                y_pred=y_pred,
                title=title,
                filename='max_race_level_place_rate_scatter.png',
                xlabel='æœ€é«˜åˆæˆãƒã‚¤ãƒ³ãƒˆ'
            )
        
    def perform_individual_validation(self, horse_stats: pd.DataFrame) -> Dict[str, Any]:
        """
        å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ã‚’å®Ÿè¡Œ
        ãƒ¬ãƒãƒ¼ãƒˆç¬¬5éƒ¨ã®æ¤œè¨¼é …ç›®1-3ã«å¯¾å¿œ
        """
        logger.info("ğŸ”¬ å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ã‚’é–‹å§‹...")
        
        results = {}
        
        # æ¤œè¨¼é …ç›®1: ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢æ¤œè¨¼
        grade_results = self._validate_element_correlation(
            horse_stats, 'å¹³å‡ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆ', 'è¤‡å‹ç‡', 'ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«'
        )
        results['grade_validation'] = grade_results
        
        # æ¤œè¨¼é …ç›®2: ç«¶é¦¬å ´ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢æ¤œè¨¼
        venue_results = self._validate_element_correlation(
            horse_stats, 'å¹³å‡ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆ', 'è¤‡å‹ç‡', 'ç«¶é¦¬å ´ãƒ¬ãƒ™ãƒ«'
        )
        results['venue_validation'] = venue_results
        
        # æ¤œè¨¼é …ç›®3: è·é›¢ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢æ¤œè¨¼
        distance_results = self._validate_element_correlation(
            horse_stats, 'å¹³å‡è·é›¢ãƒã‚¤ãƒ³ãƒˆ', 'è¤‡å‹ç‡', 'è·é›¢ãƒ¬ãƒ™ãƒ«'
        )
        results['distance_validation'] = distance_results
        
        # race_levelã«ã¯æ—¢ã«è¤‡å‹çµæœãŒçµ±åˆæ¸ˆã¿ã®ãŸã‚ã€è¿½åŠ ã®æ¤œè¨¼ã¯ä¸è¦
        
        # é‡ã¿ä»˜ã‘è¨ˆç®—
        weights = self._calculate_weights(results)
        results['calculated_weights'] = weights
        
        # çµæœä¿å­˜
        self.results = results
        
        logger.info("âœ… å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼å®Œäº†")
        
        return results
        
    def _validate_element_correlation(
        self, 
        horse_stats: pd.DataFrame, 
        x_col: str, 
        y_col: str, 
        element_name: str
    ) -> Dict[str, Any]:
        """
        è¦ç´ ã¨è¤‡å‹ç‡ã®ç›¸é–¢æ¤œè¨¼
        """
        logger.info(f"ğŸ“Š {element_name}ã®ç›¸é–¢æ¤œè¨¼ä¸­...")
        
        # æ¬ æå€¤é™¤å»
        valid_data = horse_stats.dropna(subset=[x_col, y_col])
        
        if len(valid_data) < 10:
            logger.warning(f"âš ï¸ {element_name}: æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ ({len(valid_data)}ä»¶)")
            return self._create_empty_result()
        
        x = valid_data[x_col]
        y = valid_data[y_col]
        
        # ç›¸é–¢ä¿‚æ•°è¨ˆç®—ï¼ˆPearsonã¨Spearmanï¼‰
        pearson_corr, pearson_p = stats.pearsonr(x, y)
        spearman_corr, spearman_p = stats.spearmanr(x, y)
        
        # ç·šå½¢å›å¸°
        model = LinearRegression()
        X = x.values.reshape(-1, 1)
        model.fit(X, y)
        y_pred = model.predict(X)
        r2_score = model.score(X, y)
        
        # åŠ¹æœã‚µã‚¤ã‚ºã®åˆ¤å®šï¼ˆCohenåŸºæº–ï¼‰
        effect_size = self._interpret_effect_size(abs(pearson_corr))
        
        results = {
            'element_name': element_name,
            'sample_size': len(valid_data),
            'pearson_correlation': pearson_corr,
            'pearson_p_value': pearson_p,
            'spearman_correlation': spearman_corr,
            'spearman_p_value': spearman_p,
            'r2_score': r2_score,
            'effect_size': effect_size,
            'is_significant': pearson_p < 0.05,
            'regression_model': model,
            'x_values': x.values,
            'y_values': y.values,
            'y_predicted': y_pred
        }
        
        logger.info(f"   ğŸ“ˆ {element_name} - ç›¸é–¢ä¿‚æ•°: {pearson_corr:.3f} (p={pearson_p:.3f})")
        logger.info(f"   ğŸ“ˆ åŠ¹æœã‚µã‚¤ã‚º: {effect_size}, æœ‰æ„æ€§: {'æœ‰æ„' if pearson_p < 0.05 else 'éæœ‰æ„'}")
        
        return results
        
    def _interpret_effect_size(self, abs_correlation: float) -> str:
        """åŠ¹æœã‚µã‚¤ã‚ºã®è§£é‡ˆï¼ˆCohenåŸºæº–ï¼‰"""
        if abs_correlation < 0.1:
            return "ç„¡åŠ¹æœ"
        elif abs_correlation < 0.3:
            return "å°åŠ¹æœ"
        elif abs_correlation < 0.5:
            return "ä¸­åŠ¹æœ"
        else:
            return "å¤§åŠ¹æœ"
            
    def _create_empty_result(self) -> Dict[str, Any]:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        return {
            'element_name': 'ä¸æ˜',
            'sample_size': 0,
            'pearson_correlation': 0.0,
            'pearson_p_value': 1.0,
            'spearman_correlation': 0.0,
            'spearman_p_value': 1.0,
            'r2_score': 0.0,
            'effect_size': 'ç„¡åŠ¹æœ',
            'is_significant': False,
            'regression_model': None,
            'x_values': np.array([]),
            'y_values': np.array([]),
            'y_predicted': np.array([])
        }
        
    def _calculate_weights(self, validation_results: Dict[str, Any]) -> Dict[str, float]:
        """
        ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€è¨˜è¼‰ã®å‹•çš„é‡ã¿è¨ˆç®—ï¼ˆè¨“ç·´æœŸé–“: 2010-2020å¹´ï¼‰
        w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)
        """
        logger.info("âš–ï¸ ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®å‹•çš„é‡ã¿è¨ˆç®—ä¸­...")
        
        try:
            # è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢
            if hasattr(self, 'df') and self.df is not None:
                train_data = self.df[(self.df['å¹´'] >= 2010) & (self.df['å¹´'] <= 2020)].copy()
                
                if len(train_data) == 0:
                    logger.warning("âš ï¸ è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ã—ã¾ã™ã€‚")
                    train_data = self.df.copy()
                
                logger.info(f"ğŸ“Š è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ã§ã®å‹•çš„é‡ã¿è¨ˆç®—:")
                logger.info(f"   å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(train_data):,}è¡Œ")
                logger.info(f"   å¯¾è±¡æœŸé–“: {train_data['å¹´'].min()}-{train_data['å¹´'].max()}å¹´")
                
                # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€ã®æ–¹æ³•ã§ç›¸é–¢åˆ†æ
                target_col = 'horse_place_rate'
                if target_col not in train_data.columns:
                    target_col = 'è¤‡å‹ç‡'  # ä»£æ›¿åˆ—å
                
                if target_col in train_data.columns:
                    # å„è¦ç´ ã®ç›¸é–¢è¨ˆç®—
                    grade_corr = abs(train_data.get('grade_level', train_data.get('ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«', pd.Series([0]))).corr(train_data[target_col]))
                    venue_corr = abs(train_data.get('venue_level', train_data.get('å ´æ‰€ãƒ¬ãƒ™ãƒ«', pd.Series([0]))).corr(train_data[target_col]))
                    distance_corr = abs(train_data.get('distance_level', train_data.get('è·é›¢ãƒ¬ãƒ™ãƒ«', pd.Series([0]))).corr(train_data[target_col]))
                    
                    # NaNå‡¦ç†
                    grade_corr = grade_corr if not pd.isna(grade_corr) else 0.0
                    venue_corr = venue_corr if not pd.isna(venue_corr) else 0.0
                    distance_corr = distance_corr if not pd.isna(distance_corr) else 0.0
                    
                    # å¯„ä¸åº¦è¨ˆç®—ï¼ˆç›¸é–¢ã®2ä¹—ï¼‰
                    grade_contribution = grade_corr ** 2
                    venue_contribution = venue_corr ** 2
                    distance_contribution = distance_corr ** 2
                    total_contribution = grade_contribution + venue_contribution + distance_contribution
                    
                    logger.info(f"ğŸ” ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€ã®ç›¸é–¢åˆ†æçµæœ:")
                    logger.info(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰ç›¸é–¢: r = {grade_corr:.3f}, rÂ² = {grade_contribution:.3f}")
                    logger.info(f"   å ´æ‰€ç›¸é–¢: r = {venue_corr:.3f}, rÂ² = {venue_contribution:.3f}")
                    logger.info(f"   è·é›¢ç›¸é–¢: r = {distance_corr:.3f}, rÂ² = {distance_contribution:.3f}")
                    logger.info(f"   ç·å¯„ä¸åº¦: {total_contribution:.3f}")
                    
                    # é‡ã¿è¨ˆç®—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€ã®å¼ï¼‰
                    if total_contribution > 0:
                        grade_weight = grade_contribution / total_contribution
                        venue_weight = venue_contribution / total_contribution
                        distance_weight = distance_contribution / total_contribution
                        
                        logger.info(f"ğŸ“Š è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰å‹•çš„é‡ã¿ç®—å‡ºçµæœ:")
                        logger.info(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰: {grade_weight:.3f} ({grade_weight*100:.1f}%)")
                        logger.info(f"   å ´æ‰€: {venue_weight:.3f} ({venue_weight*100:.1f}%)")
                        logger.info(f"   è·é›¢: {distance_weight:.3f} ({distance_weight*100:.1f}%)")
                        logger.info("âœ… ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ : w_i = r_iÂ² / Î£r_iÂ²")
                        
                        # ğŸ“ è©³ç´°ãªé‡ã¿æƒ…å ±ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
                        logger.info("ğŸ“Š ========== å€‹åˆ¥æ¤œè¨¼ã§å‹•çš„é‡ã¿è¨ˆç®—å®Œäº† ==========")
                        logger.info("âš–ï¸ ç®—å‡ºã•ã‚ŒãŸé‡ã¿é…åˆ†:")
                        logger.info(f"   ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {grade_weight:.4f} ({grade_weight*100:.2f}%)")
                        logger.info(f"   ğŸ“Š å ´æ‰€é‡ã¿: {venue_weight:.4f} ({venue_weight*100:.2f}%)")
                        logger.info(f"   ğŸ“Š è·é›¢é‡ã¿: {distance_weight:.4f} ({distance_weight*100:.2f}%)")
                        logger.info("ğŸ“Š REQIè¨ˆç®—å¼:")
                        logger.info(f"   race_level = {grade_weight:.4f} Ã— grade_level + {venue_weight:.4f} Ã— venue_level + {distance_weight:.4f} Ã— distance_level")
                        logger.info("=" * 60)
                        
                        return {
                            'grade_weight': grade_weight,
                            'venue_weight': venue_weight, 
                            'distance_weight': distance_weight
                        }
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å‚è€ƒå€¤
            logger.warning("âš ï¸ å‹•çš„è¨ˆç®—å¤±æ•—ã€‚ãƒ¬ãƒãƒ¼ãƒˆè¨˜è¼‰ã®å‚è€ƒå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            weights = {
                'grade_weight': 0.636,   # 63.6% - ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€è¨˜è¼‰å€¤
                'venue_weight': 0.323,   # 32.3% - ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€è¨˜è¼‰å€¤
                'distance_weight': 0.041 # 4.1%  - ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€è¨˜è¼‰å€¤
            }
            
            logger.info(f"ğŸ“Š é©ç”¨ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€å‚è€ƒå€¤ï¼‰:")
            logger.info(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰: {weights['grade_weight']:.3f} (63.6%)")
            logger.info(f"   å ´æ‰€: {weights['venue_weight']:.3f} (32.3%)")
            logger.info(f"   è·é›¢: {weights['distance_weight']:.3f} (4.1%)")
            
            return weights
            
        except Exception as e:
            logger.error(f"âŒ å‹•çš„é‡ã¿è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                'grade_weight': 0.33,
                'venue_weight': 0.33,
                'distance_weight': 0.34
            }
        
    def create_scatter_plots(self, horse_stats: pd.DataFrame, results: Dict[str, Any]) -> None:
        """
        æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šä»˜ãï¼‰ã®ä½œæˆ
        ãƒ¬ãƒãƒ¼ãƒˆã§è¦æ±‚ã•ã‚Œã¦ã„ã‚‹å¯è¦–åŒ–
        """
        logger.info("ğŸ“Š æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šä»˜ãï¼‰ã‚’ä½œæˆä¸­...")
        
        # å€‹åˆ¥è¦ç´ ã®æ•£å¸ƒå›³
        self._create_individual_scatter_plot(
            results['grade_validation'], 
            'grade_level_place_rate_scatter_points.png',
            'ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨è¤‡å‹ç‡ã®ç›¸é–¢'
        )
        
        self._create_individual_scatter_plot(
            results['venue_validation'], 
            'venue_level_place_rate_scatter_points.png',
            'ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆã¨è¤‡å‹ç‡ã®ç›¸é–¢'
        )
        
        self._create_individual_scatter_plot(
            results['distance_validation'], 
            'distance_level_place_rate_scatter_points.png',
            'è·é›¢ãƒã‚¤ãƒ³ãƒˆã¨è¤‡å‹ç‡ã®ç›¸é–¢'
        )
        
        # race_levelã«ã¯æ—¢ã«è¤‡å‹çµæœãŒçµ±åˆæ¸ˆã¿ã®ãŸã‚ã€è¿½åŠ ã®æ•£å¸ƒå›³ã¯ä¸è¦
        
        # çµ±åˆæ•£å¸ƒå›³
        self._create_comprehensive_scatter_plot(results)
        
        logger.info("âœ… æ•£å¸ƒå›³ã®ä½œæˆå®Œäº†")
        
    def _create_individual_scatter_plot(
        self, 
        validation_result: Dict[str, Any], 
        filename: str, 
        title: str
    ) -> None:
        """å€‹åˆ¥è¦ç´ ã®æ•£å¸ƒå›³ä½œæˆ"""
        
        if validation_result['sample_size'] == 0:
            logger.warning(f"âš ï¸ {title}: ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã®ãŸã‚æ•£å¸ƒå›³ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
            
        plt.figure(figsize=(12, 8))
        
        x = validation_result['x_values']
        y = validation_result['y_values']
        y_pred = validation_result['y_predicted']
        
        # æ•£å¸ƒå›³
        plt.scatter(x, y, alpha=0.6, s=50, color='steelblue', edgecolors='white', linewidth=0.5)
        
        # å›å¸°ç›´ç·š
        sort_indices = np.argsort(x)
        plt.plot(x[sort_indices], y_pred[sort_indices], 'r-', linewidth=2, label='å›å¸°ç›´ç·š')
        
        # çµ±è¨ˆæƒ…å ±
        corr = validation_result['pearson_correlation']
        p_val = validation_result['pearson_p_value']
        r2 = validation_result['r2_score']
        effect = validation_result['effect_size']
        significance = 'æœ‰æ„' if validation_result['is_significant'] else 'éæœ‰æ„'
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨çµ±è¨ˆæƒ…å ±
        plt.title(f'{title}\nç›¸é–¢ä¿‚æ•°: r={corr:.3f} (p={p_val:.3f}), RÂ²={r2:.3f}, åŠ¹æœã‚µã‚¤ã‚º: {effect}, {significance}', 
                 fontsize=14, pad=20)
        plt.xlabel(validation_result['element_name'] + 'ãƒã‚¤ãƒ³ãƒˆ', fontsize=12)
        plt.ylabel('è¤‡å‹ç‡', fontsize=12)
        
        # ã‚°ãƒªãƒƒãƒ‰ã¨è£…é£¾
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # çµ±è¨ˆæƒ…å ±ãƒœãƒƒã‚¯ã‚¹
        stats_text = f'ã‚µãƒ³ãƒ—ãƒ«æ•°: {validation_result["sample_size"]:,}é ­\n'
        stats_text += f'Pearson: r={corr:.3f}\n'
        stats_text += f'Spearman: Ï={validation_result["spearman_correlation"]:.3f}\n'
        stats_text += f'æ±ºå®šä¿‚æ•°: RÂ²={r2:.3f}'
        
        plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.output_dir / filename, dpi=300, bbox_inches='tight')
        plt.close()
        
    def _create_comprehensive_scatter_plot(self, results: Dict[str, Any]) -> None:
        """çµ±åˆæ•£å¸ƒå›³ã®ä½œæˆ"""
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        validations = [
            (results['grade_validation'], 'ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆ', axes[0]),
            (results['venue_validation'], 'ç«¶é¦¬å ´ãƒã‚¤ãƒ³ãƒˆ', axes[1]),
            (results['distance_validation'], 'è·é›¢ãƒã‚¤ãƒ³ãƒˆ', axes[2])
        ]
        
        for validation, element_name, ax in validations:
            if validation['sample_size'] == 0:
                ax.text(0.5, 0.5, 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{element_name}\nï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰')
                continue
                
            x = validation['x_values']
            y = validation['y_values']
            y_pred = validation['y_predicted']
            
            # æ•£å¸ƒå›³
            ax.scatter(x, y, alpha=0.6, s=30, color='steelblue', edgecolors='white', linewidth=0.5)
            
            # å›å¸°ç›´ç·š
            sort_indices = np.argsort(x)
            ax.plot(x[sort_indices], y_pred[sort_indices], 'r-', linewidth=2)
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            corr = validation['pearson_correlation']
            r2 = validation['r2_score']
            ax.set_title(f'{element_name}\nr={corr:.3f}, RÂ²={r2:.3f}')
            ax.set_xlabel(f'{element_name}')
            ax.set_ylabel('è¤‡å‹ç‡')
            ax.grid(True, alpha=0.3)
        
        plt.suptitle('å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ï¼ˆç›¸é–¢åˆ†æï¼‰', fontsize=16, y=1.02)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'individual_elements_validation_comprehensive.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
    def save_results(self, results: Dict[str, Any]) -> None:
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        logger.info("ğŸ’¾ æ¤œè¨¼çµæœã‚’ä¿å­˜ä¸­...")
        
        # ä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆnumpyé…åˆ—ã‚„ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’é™¤å»ï¼‰
        save_data = {}
        
        for key, validation in results.items():
            if key == 'calculated_weights':
                save_data[key] = validation
                continue
                
            if isinstance(validation, dict) and 'regression_model' in validation:
                save_data[key] = {
                    'element_name': validation['element_name'],
                    'sample_size': int(validation['sample_size']),
                    'pearson_correlation': float(validation['pearson_correlation']),
                    'pearson_p_value': float(validation['pearson_p_value']),
                    'spearman_correlation': float(validation['spearman_correlation']),
                    'spearman_p_value': float(validation['spearman_p_value']),
                    'r2_score': float(validation['r2_score']),
                    'effect_size': str(validation['effect_size']),
                    'is_significant': bool(validation['is_significant'])
                }
        
        # ä¿å­˜
        output_file = self.output_dir / 'individual_validation_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ æ¤œè¨¼çµæœã‚’ä¿å­˜: {output_file}")
        
    def generate_report_section(self, results: Dict[str, Any]) -> str:
        """
        ãƒ¬ãƒãƒ¼ãƒˆã®ç¬¬5éƒ¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        """
        logger.info("ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆä¸­...")
        
        report = """## 5.1 å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ï¼ˆç›¸é–¢åˆ†æï¼‰

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€`RacePoint`ã®åŸºç¤ç‚¹ã‚’æ§‹æˆã™ã‚‹å„è¦ç´ ã¨ã€é¦¬ã”ã¨ã®`è¤‡å‹ç‡`ã¨ã®ç›¸é–¢ä¿‚æ•°ï¼ˆSpearmanï¼‰ã‚’å€‹åˆ¥ã«ç®—å‡ºã—ã€å„è¦ç´ ãŒå˜ç‹¬ã§ã©ã®ç¨‹åº¦ã®äºˆæ¸¬åŠ›ã‚’æŒã¤ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

### 5.1.1 æ¤œè¨¼é …ç›®1: ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢æ¤œè¨¼

"""
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ¤œè¨¼çµæœ
        grade_result = results.get('grade_validation', {})
        if grade_result.get('sample_size', 0) > 0:
            report += f"""**çµ±è¨ˆçµæœ:**
- ã‚µãƒ³ãƒ—ãƒ«æ•°: {grade_result['sample_size']:,}é ­
- Pearsonç›¸é–¢ä¿‚æ•°: r = {grade_result['pearson_correlation']:.3f} (p = {grade_result['pearson_p_value']:.3f})
- Spearmané †ä½ç›¸é–¢: Ï = {grade_result['spearman_correlation']:.3f} (p = {grade_result['spearman_p_value']:.3f})
- æ±ºå®šä¿‚æ•°: RÂ² = {grade_result['r2_score']:.3f}
- åŠ¹æœã‚µã‚¤ã‚º: {grade_result['effect_size']}
- çµ±è¨ˆçš„æœ‰æ„æ€§: {'æœ‰æ„' if grade_result['is_significant'] else 'éæœ‰æ„'} (Î± = 0.05)

**è§£é‡ˆ:** ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–“ã«ã¯{'æ­£ã®' if grade_result['pearson_correlation'] > 0 else 'è² ã®'}ç›¸é–¢ãŒè¦³æ¸¬ã•ã‚ŒãŸã€‚åŠ¹æœã‚µã‚¤ã‚ºã¯{grade_result['effect_size']}ã§ã‚ã‚Šã€çµ±è¨ˆçš„ã«{'æœ‰æ„ãªé–¢ä¿‚' if grade_result['is_significant'] else 'æœ‰æ„ã§ã¯ãªã„é–¢ä¿‚'}ãŒç¢ºèªã•ã‚ŒãŸã€‚

"""
        
        report += """### 5.1.2 æ¤œè¨¼é …ç›®2: ç«¶é¦¬å ´ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢æ¤œè¨¼

"""
        
        # ç«¶é¦¬å ´æ¤œè¨¼çµæœ
        venue_result = results.get('venue_validation', {})
        if venue_result.get('sample_size', 0) > 0:
            report += f"""**çµ±è¨ˆçµæœ:**
- ã‚µãƒ³ãƒ—ãƒ«æ•°: {venue_result['sample_size']:,}é ­
- Pearsonç›¸é–¢ä¿‚æ•°: r = {venue_result['pearson_correlation']:.3f} (p = {venue_result['pearson_p_value']:.3f})
- Spearmané †ä½ç›¸é–¢: Ï = {venue_result['spearman_correlation']:.3f} (p = {venue_result['spearman_p_value']:.3f})
- æ±ºå®šä¿‚æ•°: RÂ² = {venue_result['r2_score']:.3f}
- åŠ¹æœã‚µã‚¤ã‚º: {venue_result['effect_size']}
- çµ±è¨ˆçš„æœ‰æ„æ€§: {'æœ‰æ„' if venue_result['is_significant'] else 'éæœ‰æ„'} (Î± = 0.05)

**è§£é‡ˆ:** ç«¶é¦¬å ´ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–“ã«ã¯{'æ­£ã®' if venue_result['pearson_correlation'] > 0 else 'è² ã®'}ç›¸é–¢ãŒè¦³æ¸¬ã•ã‚ŒãŸã€‚åŠ¹æœã‚µã‚¤ã‚ºã¯{venue_result['effect_size']}ã§ã‚ã‚Šã€çµ±è¨ˆçš„ã«{'æœ‰æ„ãªé–¢ä¿‚' if venue_result['is_significant'] else 'æœ‰æ„ã§ã¯ãªã„é–¢ä¿‚'}ãŒç¢ºèªã•ã‚ŒãŸã€‚

"""
        
        report += """### 5.1.3 æ¤œè¨¼é …ç›®3: è·é›¢ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢æ¤œè¨¼

"""
        
        # è·é›¢æ¤œè¨¼çµæœ
        distance_result = results.get('distance_validation', {})
        if distance_result.get('sample_size', 0) > 0:
            report += f"""**çµ±è¨ˆçµæœ:**
- ã‚µãƒ³ãƒ—ãƒ«æ•°: {distance_result['sample_size']:,}é ­
- Pearsonç›¸é–¢ä¿‚æ•°: r = {distance_result['pearson_correlation']:.3f} (p = {distance_result['pearson_p_value']:.3f})
- Spearmané †ä½ç›¸é–¢: Ï = {distance_result['spearman_correlation']:.3f} (p = {distance_result['spearman_p_value']:.3f})
- æ±ºå®šä¿‚æ•°: RÂ² = {distance_result['r2_score']:.3f}
- åŠ¹æœã‚µã‚¤ã‚º: {distance_result['effect_size']}
- çµ±è¨ˆçš„æœ‰æ„æ€§: {'æœ‰æ„' if distance_result['is_significant'] else 'éæœ‰æ„'} (Î± = 0.05)

**è§£é‡ˆ:** è·é›¢ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®é–“ã«ã¯{'æ­£ã®' if distance_result['pearson_correlation'] > 0 else 'è² ã®'}ç›¸é–¢ãŒè¦³æ¸¬ã•ã‚ŒãŸã€‚åŠ¹æœã‚µã‚¤ã‚ºã¯{distance_result['effect_size']}ã§ã‚ã‚Šã€çµ±è¨ˆçš„ã«{'æœ‰æ„ãªé–¢ä¿‚' if distance_result['is_significant'] else 'æœ‰æ„ã§ã¯ãªã„é–¢ä¿‚'}ãŒç¢ºèªã•ã‚ŒãŸã€‚

"""
        
        # é‡ã¿ä»˜ã‘è¨ˆç®—çµæœ
        weights = results.get('calculated_weights', {})
        if weights:
            report += f"""### 5.1.4 ç›¸é–¢å¼·åº¦ã«åŸºã¥ãé‡ã¿ä»˜ã‘è¨ˆç®—

å„è¦ç´ ã®ç›¸é–¢ä¿‚æ•° `r` ã‚’ç”¨ã„ã¦ã€é‡ã¿ã‚’ `w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)` ã®ã‚ˆã†ã«ã€æ±ºå®šä¿‚æ•°ï¼ˆrÂ²ï¼‰ã§æ­£è¦åŒ–ã—ã¦ç®—å‡ºã—ãŸçµæœ:

**è¨ˆç®—ã•ã‚ŒãŸé‡ã¿:**
- ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: wâ‚ = {weights.get('grade_weight', 0):.3f}
- ç«¶é¦¬å ´é‡ã¿: wâ‚‚ = {weights.get('venue_weight', 0):.3f}  
- è·é›¢é‡ã¿: wâ‚ƒ = {weights.get('distance_weight', 0):.3f}

**é‡è¦åº¦é †ä½:** {self._get_weight_ranking(weights)}

ã“ã®é‡ã¿ä»˜ã‘ã«ã‚ˆã‚Šã€äºˆæ¸¬ã¸ã®å¯„ä¸åº¦ãŒå¤§ãã„è¦ç´ ã»ã©ã€å¤§ããªé‡ã¿ã‚’æŒã¤ã“ã¨ãŒå®¢è¦³çš„ã«æ±ºå®šã•ã‚ŒãŸã€‚

### 5.1.5 æ¤œè¨¼çµæœã®ç·åˆè©•ä¾¡

å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®çŸ¥è¦‹ãŒå¾—ã‚‰ã‚ŒãŸ:

1. **æœ€ã‚‚æœ‰åŠ¹ãªè¦ç´ :** {self._identify_most_effective_element(results)}
2. **çµ±è¨ˆçš„æœ‰æ„æ€§:** {self._count_significant_elements(results)}ã¤ã®è¦ç´ ã§çµ±è¨ˆçš„æœ‰æ„ãªç›¸é–¢ã‚’ç¢ºèª
3. **å®Ÿç”¨æ€§è©•ä¾¡:** ç®—å‡ºã•ã‚ŒãŸé‡ã¿ä»˜ã‘ã¯å®Ÿç”¨çš„ãªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«é©ç”¨å¯èƒ½

ã“ã‚Œã‚‰ã®çµæœã¯ã€æ¬¡ç¯€ã®çµ±åˆåˆ†æã«ãŠã„ã¦åˆæˆç‰¹å¾´é‡`HorseRaceLevel`ã®ç®—å‡ºã«æ´»ç”¨ã•ã‚Œã‚‹ã€‚
"""
        
        return report
        
    def _get_weight_ranking(self, weights: Dict[str, float]) -> str:
        """é‡ã¿ã®é †ä½ã‚’æ–‡å­—åˆ—ã§è¿”ã™"""
        if not weights:
            return "è¨ˆç®—ä¸å¯"
            
        sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)
        weight_names = {
            'grade_weight': 'ã‚°ãƒ¬ãƒ¼ãƒ‰',
            'venue_weight': 'ç«¶é¦¬å ´',
            'distance_weight': 'è·é›¢'
        }
        return ' > '.join([weight_names.get(w[0], w[0]) for w in sorted_weights])
        
    def _identify_most_effective_element(self, results: Dict[str, Any]) -> str:
        """æœ€ã‚‚æœ‰åŠ¹ãªè¦ç´ ã‚’ç‰¹å®š"""
        correlations = {}
        
        for key, result in results.items():
            if isinstance(result, dict) and 'pearson_correlation' in result:
                correlations[result.get('element_name', key)] = abs(result['pearson_correlation'])
        
        if not correlations:
            return "ç‰¹å®šä¸å¯"
            
        most_effective = max(correlations.items(), key=lambda x: x[1])
        return f"{most_effective[0]} (r={most_effective[1]:.3f})"
        
    def _count_significant_elements(self, results: Dict[str, Any]) -> int:
        """çµ±è¨ˆçš„æœ‰æ„ãªè¦ç´ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        count = 0
        for key, result in results.items():
            if isinstance(result, dict) and result.get('is_significant', False):
                count += 1
        return count
        
    def run_complete_validation(self) -> Dict[str, Any]:
        """
        å®Œå…¨ãªå€‹åˆ¥è¦ç´ æ¤œè¨¼ã®å®Ÿè¡Œ
        """
        logger.info("ğŸš€ å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ã‚’é–‹å§‹...")
        
        # 1. ãƒã‚¤ãƒ³ãƒˆè¨ˆç®—
        df_with_points = self.calculate_point_levels()
        
        # 2. é¦¬ã”ã¨ã®çµ±è¨ˆè¨ˆç®—
        horse_stats = self.calculate_horse_statistics(df_with_points)
        
        # 3. å€‹åˆ¥æ¤œè¨¼å®Ÿè¡Œ
        validation_results = self.perform_individual_validation(horse_stats)
        
        # 3.5 åˆæˆãƒã‚¤ãƒ³ãƒˆã®ç®—å‡ºã¨æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šï¼‰
        if 'calculated_weights' in validation_results:
            df_with_composite = self.compute_composite_race_points(df_with_points, validation_results['calculated_weights'])
            horse_stats_composite = self.calculate_horse_statistics(df_with_composite)
            self.create_composite_scatter_plots(horse_stats_composite)
        else:
            logger.warning("âš ï¸ é‡ã¿ãŒç®—å‡ºã§ããªã‹ã£ãŸãŸã‚ã€åˆæˆãƒã‚¤ãƒ³ãƒˆæ•£å¸ƒå›³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        
        # 4. æ•£å¸ƒå›³ä½œæˆ
        self.create_scatter_plots(horse_stats, validation_results)
        
        # 5. çµæœä¿å­˜
        self.save_results(validation_results)
        
        # 6. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_text = self.generate_report_section(validation_results)
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = self.output_dir / 'individual_validation_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
        logger.info("ğŸ‰ å€‹åˆ¥è¦ç´ ã®æœ‰åŠ¹æ€§æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        return validation_results
