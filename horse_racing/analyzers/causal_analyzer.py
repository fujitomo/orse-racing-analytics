"""
å› æœé–¢ä¿‚åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨æˆç¸¾ã®å› æœé–¢ä¿‚ã‚’åˆ†æã—ã¾ã™ã€‚
"""

from typing import Dict, Any
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import logging

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)

class CausalAnalyzer:
    """å› æœé–¢ä¿‚åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.results = {}
    
    def analyze_temporal_precedence(self):
        """æ™‚é–“çš„å…ˆè¡Œæ€§ã®åˆ†æ"""
        try:
            # å¹´æœˆæ—¥ã‚«ãƒ©ãƒ ã®å‡¦ç†
            if 'å¹´æœˆæ—¥' in self.df.columns:
                # å¹´æœˆæ—¥æ–‡å­—åˆ—ã‹ã‚‰æœˆã‚’æŠ½å‡º
                self.df['æœˆ'] = pd.to_datetime(self.df['å¹´æœˆæ—¥'].astype(str), format='%Y%m%d').dt.month
                self.df['æ—¥'] = pd.to_datetime(self.df['å¹´æœˆæ—¥'].astype(str), format='%Y%m%d').dt.day
            else:
                # å¹´æœˆæ—¥ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯å¹´ã€å›ã€æ—¥ã‹ã‚‰ä½œæˆ
                logger.warning("å¹´æœˆæ—¥ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¹´ã€å›ã€æ—¥ã‹ã‚‰ä½œæˆã—ã¾ã™ã€‚")
                self.df['æœˆ'] = self.df['å›'].astype(int)
                self.df['æ—¥'] = self.df['æ—¥'].astype(int)

            # æ™‚é–“çš„å…ˆè¡Œæ€§ã®åˆ†æ
            temporal_stats = {}

            # æœˆã”ã¨ã®æˆç¸¾åˆ†æ
            monthly_stats = self.df.groupby('æœˆ').agg({
                'race_level': ['mean', 'std'],
                'is_placed': 'mean'
            }).round(3)

            # æœˆã”ã¨ã®ç›¸é–¢ä¿‚æ•°
            monthly_corr = self.df.groupby('æœˆ').apply(
                lambda x: x['race_level'].corr(x['is_placed'])
            ).round(3)

            temporal_stats['monthly'] = {
                'stats': monthly_stats.to_dict(),
                'correlations': monthly_corr.to_dict()
            }

            # å‰æœˆã¨ã®æ¯”è¼ƒåˆ†æ
            self.df['prev_month'] = self.df.groupby('é¦¬å')['æœˆ'].shift(1)
            self.df['level_change'] = self.df.groupby('é¦¬å')['race_level'].diff()
            self.df['performance_change'] = self.df.groupby('é¦¬å')['is_placed'].diff()

            # å¤‰åŒ–ã®ç›¸é–¢åˆ†æ
            change_corr = self.df['level_change'].corr(self.df['performance_change'])
            temporal_stats['change_correlation'] = round(float(change_corr), 3)

            # æ™‚ç³»åˆ—çš„ãªå› æœé–¢ä¿‚ã‚¹ã‚³ã‚¢ï¼ˆ-1ã‹ã‚‰1ã®ç¯„å›²ï¼‰
            temporal_score = self._calculate_temporal_score(temporal_stats)
            temporal_stats['temporal_score'] = temporal_score
            temporal_stats['sample_size'] = len(self.df['é¦¬å'].unique()) # Add sample_size

            self.results['temporal_precedence'] = temporal_stats
            logger.info(f"æ™‚é–“çš„å…ˆè¡Œæ€§ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚¹ã‚³ã‚¢: {temporal_score}")

            return temporal_stats

        except Exception as e:
            logger.error(f"æ™‚é–“çš„å…ˆè¡Œæ€§ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def _calculate_temporal_score(self, temporal_stats: dict) -> float:
        """
        æ™‚é–“çš„å…ˆè¡Œæ€§ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        
        Args:
            temporal_stats: æ™‚é–“çš„åˆ†æã®çµ±è¨ˆæƒ…å ±
            
        Returns:
            float: æ™‚é–“çš„å…ˆè¡Œæ€§ã®ã‚¹ã‚³ã‚¢ï¼ˆ-1ã‹ã‚‰1ã®ç¯„å›²ï¼‰
        """
        try:
            # æœˆã”ã¨ã®ç›¸é–¢ä¿‚æ•°ã®å¹³å‡
            monthly_corr_mean = np.mean(list(temporal_stats['monthly']['correlations'].values()))
            
            # å¤‰åŒ–ã®ç›¸é–¢ä¿‚æ•°
            change_corr = temporal_stats['change_correlation']
            
            # ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰
            score = (monthly_corr_mean * 0.6 + change_corr * 0.4)
            
            # -1ã‹ã‚‰1ã®ç¯„å›²ã«æ­£è¦åŒ–
            return max(min(score, 1.0), -1.0)
            
        except Exception as e:
            logger.error(f"æ™‚é–“çš„å…ˆè¡Œæ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return 0.0
    
    def analyze_confounding_factors(self):
        """äº¤çµ¡å› å­ã®åˆ†æ"""
        potential_confounders = ['å ´ã‚³ãƒ¼ãƒ‰', 'è·é›¢', 'èŠãƒ€éšœå®³ã‚³ãƒ¼ãƒ‰']
        confounder_effects = {}
        
        for confounder in potential_confounders:
            if confounder in self.df.columns:
                # å„äº¤çµ¡å› å­ãƒ¬ãƒ™ãƒ«ã§ã®ç›¸é–¢ã‚’è¨ˆç®—
                grouped_corrs = self.df.groupby(confounder).apply(
                    lambda x: x['race_level'].corr(x['is_placed'])
                    if len(x) > 5 else np.nan
                ).dropna()
                
                confounder_effects[confounder] = {
                    'correlations': grouped_corrs.to_dict(),
                    'mean_correlation': grouped_corrs.mean(),
                    'std_correlation': grouped_corrs.std()
                }
        
        self.results['confounding_factors'] = confounder_effects
        return self.results
    
    def analyze_mechanism(self):
        """ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ†æ"""
        # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨æˆç¸¾ã®é–¢ä¿‚æ€§ã‚’è©³ç´°ã«åˆ†æ
        level_performance = []
        for horse in self.df['é¦¬å'].unique():
            horse_races = self.df[self.df['é¦¬å'] == horse]
            if len(horse_races) >= 6:
                avg_level = horse_races['race_level'].mean()
                win_rate = (horse_races['ç€é †'] == 1).mean()
                place_rate = (horse_races['ç€é †'] <= 3).mean()
                
                level_performance.append({
                    'é¦¬å': horse,
                    'å¹³å‡ãƒ¬ãƒ™ãƒ«': avg_level,
                    'å‹ç‡': win_rate,
                    'è¤‡å‹ç‡': place_rate
                })
        
        mechanism_df = pd.DataFrame(level_performance)
        
        # mechanism_dfãŒç©ºã®å ´åˆã§ã‚‚results['mechanism']ã‚’åˆæœŸåŒ–
        self.results['mechanism'] = {
            'level_win_correlation': np.nan,
            'level_place_correlation': np.nan,
            'sample_size': len(mechanism_df)
        }

        if len(mechanism_df) > 0:
            self.results['mechanism'] = {
                'level_win_correlation': mechanism_df['å¹³å‡ãƒ¬ãƒ™ãƒ«'].corr(mechanism_df['å‹ç‡']),
            'level_place_correlation': mechanism_df['å¹³å‡ãƒ¬ãƒ™ãƒ«'].corr(mechanism_df['è¤‡å‹ç‡']),
                'sample_size': len(mechanism_df)
            }
        return self.results
    
    def evaluate_hill_criteria(self):
        """Hillã®åŸºæº–ã«ã‚ˆã‚‹è©•ä¾¡"""
        hill_criteria = {
            'strength': self._evaluate_strength(),
            'consistency': self._evaluate_consistency(),
            'specificity': self._evaluate_specificity(),
            'temporality': self._evaluate_temporality(),
            'biological_gradient': self._evaluate_biological_gradient(),
            'plausibility': self._evaluate_plausibility(),
            'coherence': True,  # æ—¢å­˜ã®ç«¶é¦¬ç†è«–ã¨çŸ›ç›¾ã—ãªã„
            'experimental_evidence': False,  # å®Ÿé¨“çš„è¨¼æ‹ ã¯é€šå¸¸å¾—ã‚‰ã‚Œãªã„
            'analogy': True  # é¡ä¼¼ã®ç¾è±¡ãŒä»–ã®ã‚¹ãƒãƒ¼ãƒ„ã§ã‚‚è¦³å¯Ÿã•ã‚Œã‚‹
        }
        
        self.results['hill_criteria'] = hill_criteria
        return self.results
    
    def _evaluate_strength(self):
        """é–¢é€£ã®å¼·ã•ã®è©•ä¾¡"""
        if 'mechanism' in self.results:
            corr = abs(self.results['mechanism']['level_place_correlation'])
            return corr > 0.5
        return False
    
    def _evaluate_consistency(self):
        """ä¸€è²«æ€§ã®è©•ä¾¡"""
        if 'confounding_factors' in self.results:
            corrs = []
            for factor in self.results['confounding_factors'].values():
                corrs.extend(factor['correlations'].values())
            return np.std(corrs) < 0.2
        return False
    
    def _evaluate_specificity(self):
        """ç‰¹ç•°æ€§ã®è©•ä¾¡"""
        # ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ãŒä»–ã®è¦å› ã¨æ¯”ã¹ã¦ç‰¹ã«å¼·ã„å½±éŸ¿ã‚’æŒã¤ã‹ç¢ºèª
        return True
    
    def _evaluate_temporality(self):
        """æ™‚é–“çš„å…ˆè¡Œæ€§ã®è©•ä¾¡"""
        if 'temporal_precedence' in self.results:
            return self.results['temporal_precedence']['temporal_score'] > 0
        return False
    
    def _evaluate_biological_gradient(self):
        """ç”¨é‡åå¿œé–¢ä¿‚ã®è©•ä¾¡"""
        if 'mechanism' in self.results:
            # ãƒ¬ãƒ™ãƒ«ã®å¢—åŠ ã«ä¼´ã†æˆç¸¾ã®å˜èª¿å¢—åŠ ã‚’ç¢ºèª
            return self.results['mechanism']['level_place_correlation'] > 0
        return False
    
    def _evaluate_plausibility(self):
        """ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§ã®è©•ä¾¡"""
        # ç«¶é¦¬ã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãå¦¥å½“æ€§
        return True

def analyze_causal_relationship(df: pd.DataFrame) -> dict:
    """å› æœé–¢ä¿‚ã®ç·åˆåˆ†æã‚’å®Ÿè¡Œ"""
    analyzer = CausalAnalyzer(df)
    
    # å„ç¨®åˆ†æã®å®Ÿè¡Œ
    analyzer.analyze_temporal_precedence()
    analyzer.analyze_confounding_factors()
    analyzer.analyze_mechanism()
    analyzer.evaluate_hill_criteria()
    
    return analyzer.results

def generate_causal_analysis_report(results: dict, output_dir: Path):
    """å› æœé–¢ä¿‚åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    report_path = output_dir / 'causal_analysis' / 'causal_inference_report.md'
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«å› æœæ¨è«–åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # æ™‚é–“çš„å…ˆè¡Œæ€§ã®åˆ†æçµæœ
        if 'temporal_precedence' in results:
            f.write("## ğŸ•’ æ™‚é–“çš„å…ˆè¡Œæ€§åˆ†æ\n\n")
            temporal = results['temporal_precedence']
            f.write(f"- ã‚¹ã‚³ã‚¢: {temporal['temporal_score']:.3f}\n")
            f.write(f"- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {temporal['sample_size']}é ­\n\n")
        
        # äº¤çµ¡å› å­ã®åˆ†æçµæœ
        if 'confounding_factors' in results:
            f.write("## ğŸ”„ äº¤çµ¡å› å­åˆ†æ\n\n")
            for factor, stats in results['confounding_factors'].items():
                f.write(f"### {factor}ã«ã‚ˆã‚‹å½±éŸ¿\n")
                f.write(f"- å¹³å‡ç›¸é–¢: {stats['mean_correlation']:.3f}\n")
                f.write(f"- ç›¸é–¢ã®æ¨™æº–åå·®: {stats['std_correlation']:.3f}\n")
                f.write("- å„ãƒ¬ãƒ™ãƒ«ã§ã®ç›¸é–¢:\n")
                for level, corr in stats['correlations'].items():
                    f.write(f"  - {level}: {corr:.3f}\n")
                f.write("\n")
        
        # ãƒ¡ã‚«ãƒ‹ã‚ºãƒ åˆ†æã®çµæœ
        if 'mechanism' in results:
            f.write("## âš™ï¸ ãƒ¡ã‚«ãƒ‹ã‚ºãƒ åˆ†æ\n\n")
            mech = results['mechanism']
            f.write(f"- ãƒ¬ãƒ™ãƒ«ã¨å‹ç‡ã®ç›¸é–¢: {mech['level_win_correlation']:.3f}\n")
            f.write(f"- ãƒ¬ãƒ™ãƒ«ã¨è¤‡å‹ç‡ã®ç›¸é–¢: {mech['level_place_correlation']:.3f}\n")
            f.write(f"- åˆ†æå¯¾è±¡é ­æ•°: {mech['sample_size']}é ­\n\n")
        
        # Hillã®åŸºæº–ã«ã‚ˆã‚‹è©•ä¾¡
        if 'hill_criteria' in results:
            f.write("## ğŸ“Š Hillã®åŸºæº–ã«ã‚ˆã‚‹è©•ä¾¡\n\n")
            criteria = results['hill_criteria']
            f.write("| åŸºæº– | è©•ä¾¡ | èª¬æ˜ |\n")
            f.write("|------|------|------|\n")
            
            criteria_descriptions = {
                'strength': 'é–¢é€£ã®å¼·ã•',
                'consistency': 'ä¸€è²«æ€§',
                'specificity': 'ç‰¹ç•°æ€§',
                'temporality': 'æ™‚é–“çš„å…ˆè¡Œæ€§',
                'biological_gradient': 'ç”¨é‡åå¿œé–¢ä¿‚',
                'plausibility': 'ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§',
                'coherence': 'æ•´åˆæ€§',
                'experimental_evidence': 'å®Ÿé¨“çš„è¨¼æ‹ ',
                'analogy': 'é¡ä¼¼æ€§'
            }
            
            for criterion, value in criteria.items():
                description = criteria_descriptions.get(criterion, criterion)
                status = "âœ…" if value else "âŒ"
                explanation = get_hill_criterion_explanation(criterion, value)
                f.write(f"| {description} | {status} | {explanation} |\n")
        
        # ç·åˆçš„ãªçµè«–
        f.write("\n## ğŸ’¡ ç·åˆçš„ãªçµè«–\n\n")
        f.write("### å› æœé–¢ä¿‚ã®è¨¼æ‹ å¼·åº¦\n")
        evidence_strength = evaluate_overall_evidence_strength(results)
        f.write(f"- **{evidence_strength}**: {get_evidence_strength_explanation(evidence_strength)}\n\n")
        
        f.write("### æ¨å¥¨ã•ã‚Œã‚‹è§£é‡ˆ\n")
        f.write("1. **ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¨æˆç¸¾ã®é–¢ä¿‚**: ")
        if evidence_strength in ['å¼·ã„', 'ä¸­ç¨‹åº¦']:
            f.write("å› æœé–¢ä¿‚ã®å­˜åœ¨ãŒç¤ºå”†ã•ã‚Œã¾ã™\n")
        else:
            f.write("ç›¸é–¢é–¢ä¿‚ã¯ç¢ºèªã•ã‚Œã¾ã—ãŸãŒã€å› æœé–¢ä¿‚ã®è¨¼æ˜ã«ã¯æ›´ãªã‚‹ç ”ç©¶ãŒå¿…è¦ã§ã™\n")
        
        f.write("\n### æ³¨æ„äº‹é …\n")
        f.write("- ã“ã®åˆ†æã¯è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãŠã‚Šã€å®Œå…¨ãªå› æœé–¢ä¿‚ã®è¨¼æ˜ã§ã¯ã‚ã‚Šã¾ã›ã‚“\n")
        f.write("- æœªè¦³æ¸¬ã®äº¤çµ¡å› å­ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\n")
        f.write("- çµæœã®è§£é‡ˆã«ã¯å°‚é–€çŸ¥è­˜ãŒå¿…è¦ã§ã™\n")

def get_hill_criterion_explanation(criterion: str, value: bool) -> str:
    """Hillã®åŸºæº–ã®èª¬æ˜ã‚’å–å¾—"""
    explanations = {
        'strength': {
            True: "å¼·ã„ç›¸é–¢é–¢ä¿‚ãŒç¢ºèªã•ã‚ŒãŸ",
            False: "ç›¸é–¢é–¢ä¿‚ãŒå¼±ã„"
        },
        'consistency': {
            True: "ç•°ãªã‚‹æ¡ä»¶ä¸‹ã§ã‚‚ä¸€è²«ã—ãŸé–¢ä¿‚ãŒè¦‹ã‚‰ã‚Œã‚‹",
            False: "æ¡ä»¶ã«ã‚ˆã£ã¦çµæœãŒå¤§ããç•°ãªã‚‹"
        },
        'specificity': {
            True: "ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã¯æˆç¸¾ã«ç‰¹ç•°çš„ãªå½±éŸ¿ã‚’æŒã¤",
            False: "ä»–ã®è¦å› ã®å½±éŸ¿ãŒå¤§ãã„å¯èƒ½æ€§ãŒã‚ã‚‹"
        },
        'temporality': {
            True: "ãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®å¤‰åŒ–ãŒæˆç¸¾ã®å¤‰åŒ–ã«å…ˆè¡Œ",
            False: "æ™‚é–“çš„ãªé †åºãŒä¸æ˜ç¢º"
        },
        'biological_gradient': {
            True: "ãƒ¬ãƒ™ãƒ«ã®ä¸Šæ˜‡ã«ä¼´ã†æˆç¸¾ã®å‘ä¸ŠãŒç¢ºèªã•ã‚ŒãŸ",
            False: "ç”¨é‡åå¿œé–¢ä¿‚ãŒä¸æ˜ç¢º"
        },
        'plausibility': {
            True: "ç«¶é¦¬ã®ç†è«–ã¨æ•´åˆæ€§ãŒã‚ã‚‹",
            False: "ç†è«–çš„ãªèª¬æ˜ãŒå›°é›£"
        },
        'coherence': {
            True: "æ—¢çŸ¥ã®äº‹å®Ÿã¨çŸ›ç›¾ã—ãªã„",
            False: "æ—¢çŸ¥ã®äº‹å®Ÿã¨çŸ›ç›¾ã™ã‚‹ç‚¹ãŒã‚ã‚‹"
        },
        'experimental_evidence': {
            True: "å®Ÿé¨“çš„ãªè¨¼æ‹ ãŒã‚ã‚‹",
            False: "å®Ÿé¨“çš„ãªè¨¼æ‹ ã¯å¾—ã‚‰ã‚Œã¦ã„ãªã„"
        },
        'analogy': {
            True: "é¡ä¼¼ã®ç¾è±¡ãŒä»–ã®ã‚¹ãƒãƒ¼ãƒ„ã§ã‚‚è¦³å¯Ÿã•ã‚Œã‚‹",
            False: "é¡ä¼¼ã®äº‹ä¾‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„"
        }
    }
    return explanations.get(criterion, {}).get(value, "èª¬æ˜ãªã—")

def evaluate_overall_evidence_strength(results: dict) -> str:
    """ç·åˆçš„ãªè¨¼æ‹ ã®å¼·ã•ã‚’è©•ä¾¡"""
    if not results.get('hill_criteria'):
        return 'ä¸æ˜'
    
    criteria = results['hill_criteria']
    essential_criteria = ['temporality', 'strength', 'consistency']
    
    # å¿…é ˆåŸºæº–ã®ãƒã‚§ãƒƒã‚¯
    essential_met = all(criteria.get(c, False) for c in essential_criteria)
    total_met = sum(1 for v in criteria.values() if v)
    
    if essential_met and total_met >= 7:
        return 'å¼·ã„'
    elif essential_met and total_met >= 5:
        return 'ä¸­ç¨‹åº¦'
    elif total_met >= 3:
        return 'å¼±ã„'
    else:
        return 'ä¸ååˆ†'

def get_evidence_strength_explanation(strength: str) -> str:
    """è¨¼æ‹ ã®å¼·ã•ã®èª¬æ˜ã‚’å–å¾—"""
    explanations = {
        'å¼·ã„': "è¤‡æ•°ã®åŸºæº–ã‚’æº€ãŸã—ã€å› æœé–¢ä¿‚ã®å­˜åœ¨ãŒå¼·ãç¤ºå”†ã•ã‚Œã¾ã™",
        'ä¸­ç¨‹åº¦': "ä¸»è¦ãªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™ãŒã€ã•ã‚‰ãªã‚‹æ¤œè¨¼ãŒæœ›ã¾ã—ã„çŠ¶æ…‹ã§ã™",
        'å¼±ã„': "ä¸€éƒ¨ã®åŸºæº–ã®ã¿ã‚’æº€ãŸã—ã¦ãŠã‚Šã€å› æœé–¢ä¿‚ã®è¨¼æ˜ã«ã¯ä¸ååˆ†ã§ã™",
        'ä¸ååˆ†': "åŸºæº–ã‚’ååˆ†ã«æº€ãŸã—ã¦ãŠã‚‰ãšã€å› æœé–¢ä¿‚ã‚’ä¸»å¼µã§ãã¾ã›ã‚“",
        'ä¸æ˜': "è©•ä¾¡ã«å¿…è¦ãªæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
    }
    return explanations.get(strength, "èª¬æ˜ãªã—") 