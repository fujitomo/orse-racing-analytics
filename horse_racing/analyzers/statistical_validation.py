"""
çµ±è¨ˆçš„å¦¥å½“æ€§æ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã«ãŠã‘ã‚‹çµ±è¨ˆçš„å•é¡Œã‚’æ¤œå‡ºãƒ»é˜²æ­¢ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from scipy import stats
from sklearn.metrics import r2_score
import logging
from pathlib import Path
import warnings

logger = logging.getLogger(__name__)

class StatisticalValidationFramework:
    """çµ±è¨ˆçš„å¦¥å½“æ€§æ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, strict_mode: bool = True):
        """
        åˆæœŸåŒ–
        
        Args:
            strict_mode: å³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆTrue: å³ã—ã„åŸºæº–, False: ç·©ã„åŸºæº–ï¼‰
        """
        self.strict_mode = strict_mode
        self.validation_results = {}
        self.warnings = []
        self.errors = []
        
    def validate_temporal_split(self, train_data: pd.DataFrame, test_data: pd.DataFrame, 
                              date_column: str = 'å¹´æœˆæ—¥') -> Dict[str, Any]:
        """
        æ™‚ç³»åˆ—åˆ†å‰²ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
        
        Args:
            train_data: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            test_data: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            date_column: æ—¥ä»˜åˆ—å
            
        Returns:
            æ¤œè¨¼çµæœ
        """
        logger.info("ğŸ” æ™‚ç³»åˆ—åˆ†å‰²ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ä¸­...")
        
        results = {
            'is_valid': True,
            'issues': [],
            'recommendations': []
        }
        
        # 1. æ—¥ä»˜åˆ—ã®å­˜åœ¨ç¢ºèª
        if date_column not in train_data.columns or date_column not in test_data.columns:
            results['is_valid'] = False
            results['issues'].append(f"æ—¥ä»˜åˆ— '{date_column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            results['recommendations'].append("çœŸã®æ™‚ç³»åˆ—åˆ†å‰²ã«ã¯æ—¥ä»˜æƒ…å ±ãŒå¿…è¦ã§ã™")
            return results
        
        # 2. æ™‚ç³»åˆ—é †åºã®ç¢ºèª
        try:
            train_max_date = pd.to_datetime(train_data[date_column]).max()
            test_min_date = pd.to_datetime(test_data[date_column]).min()
            
            if train_max_date >= test_min_date:
                results['is_valid'] = False
                results['issues'].append(
                    f"æ™‚ç³»åˆ—é †åºãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: è¨“ç·´æœ€å¤§æ—¥({train_max_date}) >= ãƒ†ã‚¹ãƒˆæœ€å°æ—¥({test_min_date})"
                )
                results['recommendations'].append("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å…¨æœŸé–“ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šå‰ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        except Exception as e:
            results['is_valid'] = False
            results['issues'].append(f"æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # 3. ãƒ‡ãƒ¼ã‚¿é‡è¤‡ã®ç¢ºèª
        if len(train_data) > 0 and len(test_data) > 0:
            # é¦¬åã¨æ—¥ä»˜ã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if 'é¦¬å' in train_data.columns and 'é¦¬å' in test_data.columns:
                train_keys = set(zip(train_data['é¦¬å'], train_data[date_column]))
                test_keys = set(zip(test_data['é¦¬å'], test_data[date_column]))
                overlap = train_keys.intersection(test_keys)
                
                if overlap:
                    results['is_valid'] = False
                    results['issues'].append(f"ãƒ‡ãƒ¼ã‚¿é‡è¤‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {len(overlap)}ä»¶")
                    results['recommendations'].append("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«é‡è¤‡ãŒã‚ã£ã¦ã¯ã„ã‘ã¾ã›ã‚“")
        
        # 4. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
        min_train_size = 100 if self.strict_mode else 50
        min_test_size = 50 if self.strict_mode else 20
        
        if len(train_data) < min_train_size:
            results['issues'].append(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™: {len(train_data)} < {min_train_size}")
            results['recommendations'].append("çµ±è¨ˆçš„ä¿¡é ¼æ€§ã®ãŸã‚ã€ã‚ˆã‚Šå¤šãã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
        
        if len(test_data) < min_test_size:
            results['issues'].append(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™: {len(test_data)} < {min_test_size}")
            results['recommendations'].append("çµ±è¨ˆçš„ä¿¡é ¼æ€§ã®ãŸã‚ã€ã‚ˆã‚Šå¤šãã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
        
        logger.info(f"âœ… æ™‚ç³»åˆ—åˆ†å‰²æ¤œè¨¼å®Œäº†: {'å¦¥å½“' if results['is_valid'] else 'å•é¡Œã‚ã‚Š'}")
        return results
    
    def detect_circular_logic(self, features: pd.DataFrame, target: pd.Series, 
                            feature_names: List[str]) -> Dict[str, Any]:
        """
        å¾ªç’°è«–ç†ã®æ¤œå‡º
        
        Args:
            features: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            target: ç›®çš„å¤‰æ•°
            feature_names: ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆ
            
        Returns:
            æ¤œå‡ºçµæœ
        """
        logger.info("ğŸ” å¾ªç’°è«–ç†ã®æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...")
        
        results = {
            'circular_logic_detected': False,
            'suspicious_correlations': [],
            'recommendations': []
        }
        
        # ç•°å¸¸ã«é«˜ã„ç›¸é–¢ã®æ¤œå‡ºï¼ˆå¾ªç’°è«–ç†ã®å…†å€™ï¼‰
        high_correlation_threshold = 0.95 if self.strict_mode else 0.98
        
        for i, feature_name in enumerate(feature_names):
            if i < len(features.columns):
                feature_values = features.iloc[:, i]
                
                # æ¬ æå€¤ã‚’é™¤å¤–ã—ã¦ç›¸é–¢è¨ˆç®—
                valid_mask = ~(pd.isna(feature_values) | pd.isna(target))
                if valid_mask.sum() < 10:
                    continue
                
                correlation = np.corrcoef(feature_values[valid_mask], target[valid_mask])[0, 1]
                
                if abs(correlation) > high_correlation_threshold:
                    results['circular_logic_detected'] = True
                    results['suspicious_correlations'].append({
                        'feature': feature_name,
                        'correlation': correlation,
                        'severity': 'high' if abs(correlation) > 0.98 else 'medium'
                    })
        
        if results['circular_logic_detected']:
            results['recommendations'].extend([
                "ç•°å¸¸ã«é«˜ã„ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å¾ªç’°è«–ç†ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
                "ç‰¹å¾´é‡ã®è¨ˆç®—ã«ç›®çš„å¤‰æ•°ã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                "æ™‚é–“çš„åˆ†é›¢ãŒé©åˆ‡ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚"
            ])
        
        logger.info(f"âœ… å¾ªç’°è«–ç†æ¤œå‡ºå®Œäº†: {'æ¤œå‡º' if results['circular_logic_detected'] else 'æ­£å¸¸'}")
        return results
    
    def validate_statistical_tests(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        çµ±è¨ˆçš„æ¤œå®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
        
        Args:
            model_results: ãƒ¢ãƒ‡ãƒ«çµæœ
            
        Returns:
            æ¤œè¨¼çµæœ
        """
        logger.info("ğŸ” çµ±è¨ˆçš„æ¤œå®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ä¸­...")
        
        results = {
            'tests_valid': True,
            'missing_tests': [],
            'recommendations': []
        }
        
        # å¿…è¦ãªçµ±è¨ˆçš„æ¤œå®šã®ç¢ºèª
        required_tests = [
            'p_value',
            'confidence_interval',
            'effect_size'
        ]
        
        if 'h2_verification' in model_results:
            h2_results = model_results['h2_verification']
            
            for test in required_tests:
                if test not in h2_results:
                    results['tests_valid'] = False
                    results['missing_tests'].append(test)
            
            # på€¤ã®å¦¥å½“æ€§ç¢ºèª
            if 'p_value' in h2_results:
                p_value = h2_results['p_value']
                if not (0 <= p_value <= 1):
                    results['tests_valid'] = False
                    results['missing_tests'].append('invalid_p_value')
                    results['recommendations'].append(f"på€¤ãŒç„¡åŠ¹ã§ã™: {p_value}")
        
        if results['missing_tests']:
            results['recommendations'].append(
                f"ä»¥ä¸‹ã®çµ±è¨ˆçš„æ¤œå®šãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(results['missing_tests'])}"
            )
            results['recommendations'].append(
                "Fæ¤œå®šã€åŠ¹æœã‚µã‚¤ã‚ºè¨ˆç®—ã€ä¿¡é ¼åŒºé–“ã®ç®—å‡ºã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚"
            )
        
        logger.info(f"âœ… çµ±è¨ˆçš„æ¤œå®šæ¤œè¨¼å®Œäº†: {'å¦¥å½“' if results['tests_valid'] else 'ä¸è¶³ã‚ã‚Š'}")
        return results
    
    def check_data_leakage_indicators(self, train_performance: float, 
                                    test_performance: float) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã®å…†å€™ã‚’æ¤œå‡º
        
        Args:
            train_performance: è¨“ç·´æ€§èƒ½ï¼ˆRÂ²ç­‰ï¼‰
            test_performance: ãƒ†ã‚¹ãƒˆæ€§èƒ½ï¼ˆRÂ²ç­‰ï¼‰
            
        Returns:
            æ¤œå‡ºçµæœ
        """
        logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã®å…†å€™ã‚’æ¤œå‡ºä¸­...")
        
        results = {
            'leakage_suspected': False,
            'indicators': [],
            'recommendations': []
        }
        
        # 1. ç•°å¸¸ã«é«˜ã„ãƒ†ã‚¹ãƒˆæ€§èƒ½ï¼ˆé–¾å€¤ã‚’ç·©å’Œï¼‰
        if test_performance > 0.95:  # 0.9 â†’ 0.95ã«ç·©å’Œ
            results['leakage_suspected'] = True
            results['indicators'].append(f"ãƒ†ã‚¹ãƒˆæ€§èƒ½ãŒç•°å¸¸ã«é«˜ã„: {test_performance:.3f}")
        
        # 2. è¨“ç·´æ€§èƒ½ã¨ãƒ†ã‚¹ãƒˆæ€§èƒ½ã®å·®ãŒå°ã•ã™ãã‚‹ï¼ˆé–¾å€¤ã‚’ç·©å’Œï¼‰
        performance_gap = train_performance - test_performance
        if performance_gap < 0.005 and test_performance > 0.7:  # 0.01 â†’ 0.005, 0.5 â†’ 0.7ã«ç·©å’Œ
            results['leakage_suspected'] = True
            results['indicators'].append(f"æ€§èƒ½å·®ãŒå°ã•ã™ãã‚‹: {performance_gap:.4f}")
        
        # 3. ãƒ†ã‚¹ãƒˆæ€§èƒ½ãŒè¨“ç·´æ€§èƒ½ã‚’ä¸Šå›ã‚‹ï¼ˆé‡å¤§ãªå…†å€™ã€é–¾å€¤ã‚’ç·©å’Œï¼‰
        if test_performance > train_performance + 0.1:  # 0.05 â†’ 0.1ã«ç·©å’Œ
            results['leakage_suspected'] = True
            results['indicators'].append("ãƒ†ã‚¹ãƒˆæ€§èƒ½ãŒè¨“ç·´æ€§èƒ½ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹")
        
        if results['leakage_suspected']:
            results['recommendations'].extend([
                "ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
                "æ™‚ç³»åˆ—åˆ†å‰²ãŒé©åˆ‡ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                "ç‰¹å¾´é‡è¨ˆç®—ã«æœªæ¥ã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚",
                "ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†æ®µéšã§ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ãŒç™ºç”Ÿã—ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            ])
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸æ¤œå‡ºå®Œäº†: {'ç–‘ã„ã‚ã‚Š' if results['leakage_suspected'] else 'æ­£å¸¸'}")
        return results
    
    def generate_comprehensive_validation_report(self, validation_results: Dict[str, Any], 
                                               output_path: Path) -> str:
        """
        åŒ…æ‹¬çš„ãªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            validation_results: æ¤œè¨¼çµæœ
            output_path: å‡ºåŠ›ãƒ‘ã‚¹
            
        Returns:
            ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        report_path = output_path / "statistical_validation_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# çµ±è¨ˆçš„å¦¥å½“æ€§æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write("## æ¦‚è¦\n\n")
            f.write("ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã«ãŠã‘ã‚‹çµ±è¨ˆçš„å¦¥å½“æ€§ã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼ã—ãŸçµæœã‚’å ±å‘Šã—ã¾ã™ã€‚\n\n")
            
            # ç·åˆè©•ä¾¡
            overall_valid = all([
                validation_results.get('temporal_split', {}).get('is_valid', False),
                not validation_results.get('circular_logic', {}).get('circular_logic_detected', True),
                validation_results.get('statistical_tests', {}).get('tests_valid', False),
                not validation_results.get('data_leakage', {}).get('leakage_suspected', True)
            ])
            
            f.write(f"### ç·åˆè©•ä¾¡: {'âœ… çµ±è¨ˆçš„ã«å¦¥å½“' if overall_valid else 'âŒ å•é¡Œã‚ã‚Š'}\n\n")
            
            # å„æ¤œè¨¼é …ç›®ã®è©³ç´°
            for category, results in validation_results.items():
                f.write(f"## {category.replace('_', ' ').title()}\n\n")
                
                if isinstance(results, dict):
                    for key, value in results.items():
                        if key == 'recommendations' and value:
                            f.write("### æ¨å¥¨äº‹é …\n\n")
                            for rec in value:
                                f.write(f"- {rec}\n")
                            f.write("\n")
                        elif key == 'issues' and value:
                            f.write("### æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ\n\n")
                            for issue in value:
                                f.write(f"- âš ï¸ {issue}\n")
                            f.write("\n")
            
            f.write("---\n\n")
            f.write("*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯çµ±è¨ˆçš„å¦¥å½“æ€§æ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*\n")
        
        logger.info(f"ğŸ“„ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
        return str(report_path)

class OddsAnalysisValidator:
    """ã‚ªãƒƒã‚ºåˆ†æå°‚ç”¨ã®æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.framework = StatisticalValidationFramework(strict_mode=True)
    
    def validate_odds_comparison_analysis(self, analyzer, horse_df: pd.DataFrame, 
                                        results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã®åŒ…æ‹¬çš„æ¤œè¨¼
        
        Args:
            analyzer: ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æå™¨
            horse_df: é¦¬ãƒ‡ãƒ¼ã‚¿
            results: åˆ†æçµæœ
            
        Returns:
            æ¤œè¨¼çµæœ
        """
        logger.info("ğŸ”¬ ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã®åŒ…æ‹¬çš„æ¤œè¨¼ã‚’é–‹å§‹...")
        
        validation_results = {}
        
        # 1. å¾ªç’°è«–ç†ã®æ¤œå‡º
        if 'place_rate' in horse_df.columns:
            # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªï¼ˆå®Ÿéš›ã®ã‚«ãƒ©ãƒ åã‚’ä½¿ç”¨ï¼‰
            required_cols = ['avg_race_level', 'max_race_level', 'avg_win_prob_from_odds']
            available_cols = [col for col in required_cols if col in horse_df.columns]
            
            # ä»£æ›¿ã‚«ãƒ©ãƒ åã‚‚ãƒã‚§ãƒƒã‚¯
            alternative_mappings = {
                'avg_race_level': ['reqi', 'race_level'],
                'max_race_level': ['max_reqi'],
                'avg_win_prob_from_odds': ['win_prob', 'avg_win_prob']
            }
            
            for required_col in required_cols:
                if required_col not in available_cols:
                    alternatives = alternative_mappings.get(required_col, [])
                    for alt_col in alternatives:
                        if alt_col in horse_df.columns:
                            available_cols.append(alt_col)
                            logger.info(f"ğŸ“Š {required_col} ã®ä»£æ›¿ã¨ã—ã¦ {alt_col} ã‚’ä½¿ç”¨")
                            break
            
            if len(available_cols) < 2:
                logger.warning(f"âš ï¸ å¾ªç’°è«–ç†æ¤œè¨¼ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {required_cols}")
                logger.warning(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {available_cols}")
                logger.warning(f"ğŸ“Š å…¨ã‚«ãƒ©ãƒ ä¸€è¦§: {list(horse_df.columns)}")
                validation_results['circular_logic'] = {
                    'circular_logic_detected': False,
                    'reason': 'insufficient_columns',
                    'available_columns': available_cols
                }
            else:
                features = horse_df[available_cols]
                target = horse_df['place_rate']
                validation_results['circular_logic'] = self.framework.detect_circular_logic(
                    features, target, available_cols
                )
        
        # 2. çµ±è¨ˆçš„æ¤œå®šã®å¦¥å½“æ€§
        validation_results['statistical_tests'] = self.framework.validate_statistical_tests(results)
        
        # 3. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã®å…†å€™
        if 'regression' in results:
            regression_results = results['regression']
            if 'odds_baseline' in regression_results and 'combined_model' in regression_results:
                train_r2 = regression_results['combined_model'].get('r2_train', 0)
                test_r2 = regression_results['combined_model'].get('r2_test', 0)
                validation_results['data_leakage'] = self.framework.check_data_leakage_indicators(
                    train_r2, test_r2
                )
        
        logger.info("âœ… ã‚ªãƒƒã‚ºæ¯”è¼ƒåˆ†æã®åŒ…æ‹¬çš„æ¤œè¨¼å®Œäº†")
        return validation_results

