"""
ç«¶é¦¬åˆ†æç”¨ã®å‹•çš„é‡ã¿ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®é‡ã¿è¨ˆç®—ã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚’ç®¡ç†ã—ã¾ã™ã€‚
"""

import pandas as pd
from typing import Dict, Any, Optional
import logging
from scipy.stats import pearsonr

logger = logging.getLogger(__name__)

class WeightManager:
    """
    ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®å‹•çš„é‡ã¿è¨ˆç®—ã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ç®¡ç†
    
    ä½¿ç”¨æ–¹æ³•:
    1. å‡¦ç†é–‹å§‹æ™‚: WeightManager.initialize_from_training_data(df)
    2. å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: WeightManager.get_weights()
    """
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿å¤‰æ•°ï¼ˆå…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…±æœ‰ï¼‰
    _global_weights: Optional[Dict[str, float]] = None
    _calculation_details: Optional[Dict[str, Any]] = None
    _initialized: bool = False
    _data_hash: Optional[str] = None  # ãƒ‡ãƒ¼ã‚¿å¤‰æ›´æ¤œå‡ºç”¨ãƒãƒƒã‚·ãƒ¥
    
    @classmethod
    def initialize_from_training_data(cls, df: pd.DataFrame, force_recalculate: bool = False) -> Dict[str, float]:
        """
        è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹•çš„é‡ã¿ã‚’ç®—å‡ºã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
        ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®é‡ã¿è¨ˆç®—å®Ÿè£…
        
        Args:
            df: å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            force_recalculate: å†è¨ˆç®—ã‚’å¼·åˆ¶ã™ã‚‹ã‹
            
        Returns:
            è¨ˆç®—ã•ã‚ŒãŸé‡ã¿è¾æ›¸
        """
        # ãƒ‡ãƒ¼ã‚¿å¤‰æ›´ã®æ¤œå‡º
        current_data_hash = cls._calculate_data_hash(df)
        data_changed = cls._data_hash != current_data_hash
        
        if cls._initialized and not force_recalculate and not data_changed:
            logger.info("âœ… é‡ã¿ã¯æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã§ã™ï¼ˆãƒ‡ãƒ¼ã‚¿å¤‰æ›´ãªã—ï¼‰ã€‚")
            return cls._global_weights
        
        if data_changed and cls._initialized:
            logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿å¤‰æ›´ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚é‡ã¿ã‚’å†è¨ˆç®—ã—ã¾ã™ã€‚")
        elif force_recalculate:
            logger.info("ğŸ”„ å¼·åˆ¶å†è¨ˆç®—ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚é‡ã¿ã‚’å†è¨ˆç®—ã—ã¾ã™ã€‚")
            
        logger.info("ğŸ¯ å¾ªç’°è«–ç†å›é¿ç‰ˆã®å‹•çš„é‡ã¿è¨ˆç®—ã‚’é–‹å§‹...")
        logger.info("ğŸ“‹ æ”¹å–„ç‰ˆè¨ˆç®—å¼: w_i = rÂ²(feature_i, win_rate) / Î£rÂ²(feature_j, win_rate)")
        logger.info("ğŸ“‹ è¨“ç·´æœŸé–“: 2010-2020å¹´ï¼ˆå¾ªç’°è«–ç†å›é¿ç‰ˆï¼‰")
        logger.info("ğŸ”§ æ”¹å–„ç‚¹: é‡ã¿ç®—å‡ºã«å‹ç‡ã‚’ä½¿ç”¨ã—ã€äºˆæ¸¬ç›®çš„ï¼ˆè¤‡å‹ç‡ï¼‰ã¨åˆ†é›¢")
        
        try:
            # è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢
            if 'å¹´' in df.columns:
                train_data = df[(df['å¹´'] >= 2010) & (df['å¹´'] <= 2020)].copy()
                logger.info(f"ğŸ“Š è¨“ç·´æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(train_data):,}è¡Œ")
                
                # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
                required_feature_cols = ['grade_level', 'venue_level', 'distance_level']
                missing_feature_cols = [col for col in required_feature_cols if col not in train_data.columns]
                
                if missing_feature_cols:
                    logger.warning(f"âš ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_feature_cols}")
                    logger.warning("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ã—ç›´ã—ã¾ã™...")
                    train_data = df.copy()
                else:
                    logger.info("âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å…¨ã¦ã®ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒå­˜åœ¨")
                    
                if len(train_data) == 0:
                    logger.warning("âš ï¸ è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ã—ã¾ã™ã€‚")
                    train_data = df.copy()
            else:
                logger.warning("âš ï¸ 'å¹´'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ã—ã¾ã™ã€‚")
                train_data = df.copy()
                
            # æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
            final_feature_cols = [col for col in train_data.columns if col.endswith('_level')]
            logger.info(f"ğŸ“Š æœ€çµ‚è¨ˆç®—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚«ãƒ©ãƒ : {final_feature_cols}")
            
            logger.info("ğŸ“Š è¨“ç·´æœŸé–“ãƒ‡ãƒ¼ã‚¿ã§ã®å‹•çš„é‡ã¿è¨ˆç®—:")
            logger.info(f"   å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(train_data):,}è¡Œ")
            if 'å¹´' in train_data.columns:
                logger.info(f"   å¯¾è±¡æœŸé–“: {train_data['å¹´'].min()}-{train_data['å¹´'].max()}å¹´")
            
            # å¾ªç’°è«–ç†å›é¿ã®ãŸã‚å‹ç‡ãƒ™ãƒ¼ã‚¹ç›¸é–¢è¨ˆç®—ã‚’ä½¿ç”¨
            try:
                correlations = cls._calculate_feature_correlations_with_win_rate(train_data)
                
                # ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’ä½¿ç”¨
                if correlations is None or all(corr == 0.0 for corr in correlations.values()):
                    logger.warning("âš ï¸ ç›¸é–¢è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¾ªç’°è«–ç†å›é¿ç‰ˆã®å›ºå®šé‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    return cls._get_fallback_weights()
                    
                logger.info("âœ… å‹ç‡ãƒ™ãƒ¼ã‚¹ç›¸é–¢è¨ˆç®—ãŒæˆåŠŸã—ã¾ã—ãŸ")
                
            except Exception as e:
                logger.error(f"âŒ å‹ç‡ãƒ™ãƒ¼ã‚¹ç›¸é–¢è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                logger.warning("âš ï¸ å¾ªç’°è«–ç†å›é¿ç‰ˆã®å›ºå®šé‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return cls._get_fallback_weights()
            
            # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®é‡ã¿è¨ˆç®—
            weights = cls._calculate_weights_report_compliant(correlations)
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
            cls._global_weights = weights
            cls._calculation_details = {
                'correlations': correlations,
                'training_period': f"{train_data['å¹´'].min()}-{train_data['å¹´'].max()}",
                'sample_size': len(train_data),
                'target_column': 'place_rate'
            }
            cls._data_hash = current_data_hash  # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
            cls._initialized = True
            
            # çµæœã®è¡¨ç¤º
            cls._display_calculation_results(weights, correlations)
            
            # ğŸ“ ãƒ­ã‚°ã«ã‚‚é‡ã¿æƒ…å ±ã‚’å‡ºåŠ›
            cls._log_weight_calculation_results(weights, correlations, train_data)
            
            return weights
            
        except Exception as e:
            logger.error(f"âŒ å‹•çš„é‡ã¿è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return cls._get_fallback_weights()
    
    @classmethod
    def get_weights(cls) -> Dict[str, float]:
        """
        ç¾åœ¨è¨­å®šã•ã‚Œã¦ã„ã‚‹é‡ã¿ã‚’å–å¾—
        
        Returns:
            é‡ã¿è¾æ›¸
        """
        if not cls._initialized or cls._global_weights is None:
            logger.warning("âš ï¸ é‡ã¿ãŒæœªåˆæœŸåŒ–ã§ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’è¿”ã—ã¾ã™ã€‚")
            logger.warning(f"   ğŸ“Š _initialized: {cls._initialized}")
            logger.warning(f"   ğŸ“Š _global_weightså­˜åœ¨: {cls._global_weights is not None}")
            return cls._get_fallback_weights()
        
        logger.info("âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’æ­£å¸¸ã«å–å¾—ã—ã¾ã—ãŸ")
        return cls._global_weights.copy()
    
    @classmethod
    def get_calculation_details(cls) -> Optional[Dict[str, Any]]:
        """
        é‡ã¿è¨ˆç®—ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        
        Returns:
            è¨ˆç®—è©³ç´°è¾æ›¸
        """
        return cls._calculation_details.copy() if cls._calculation_details else None
    
    @classmethod
    def is_initialized(cls) -> bool:
        """
        åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèª
        
        Returns:
            åˆæœŸåŒ–æ¸ˆã¿ã‹ã©ã†ã‹
        """
        return cls._initialized
    
    @classmethod
    def reset(cls):
        """
        é‡ã¿è¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆ
        """
        cls._global_weights = None
        cls._calculation_details = None
        cls._initialized = False
        cls._data_hash = None
        logger.info("ğŸ”„ é‡ã¿è¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
    
    @classmethod
    def _calculate_data_hash(cls, df: pd.DataFrame) -> str:
        """
        ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›´ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        
        Args:
            df: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            
        Returns:
            ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥
        """
        import hashlib
        
        # è¨“ç·´æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        train_data = df[(df['å¹´'] >= 2010) & (df['å¹´'] <= 2020)] if 'å¹´' in df.columns else df
        
        # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã§ãƒãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
        hash_input = f"{len(train_data)}_{train_data.shape[1]}"
        
        # é¦¬åã¨ãƒ¬ãƒ¼ã‚¹æ•°ã®æƒ…å ±ã‚’è¿½åŠ ï¼ˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿè³ªçš„ãªå¤‰æ›´ã‚’æ¤œå‡ºï¼‰
        if 'é¦¬å' in train_data.columns:
            unique_horses = len(train_data['é¦¬å'].unique())
            hash_input += f"_{unique_horses}"
        
        # å¹´ã®ç¯„å›²ã‚’è¿½åŠ 
        if 'å¹´' in train_data.columns:
            year_range = f"{train_data['å¹´'].min()}_{train_data['å¹´'].max()}"
            hash_input += f"_{year_range}"
        
        return hashlib.md5(hash_input.encode()).hexdigest()[:8]
    
    
    @classmethod
    def _calculate_feature_correlations_with_win_rate(cls, df: pd.DataFrame) -> Dict[str, float]:
        """
        å‹ç‡ï¼ˆ1ç€ç‡ï¼‰ãƒ™ãƒ¼ã‚¹ã®ç›¸é–¢è¨ˆç®—ï¼ˆå¾ªç’°è«–ç†å›é¿ç‰ˆï¼‰
        
        Args:
            df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆå€‹åˆ¥ãƒ¬ãƒ¼ã‚¹çµæœï¼‰
            
        Returns:
            ç›¸é–¢è¾æ›¸ï¼ˆå‹ç‡ãƒ™ãƒ¼ã‚¹ï¼‰
        """
        logger.info("ğŸ” å‹ç‡ãƒ™ãƒ¼ã‚¹ã®ç›¸é–¢è¨ˆç®—ã‚’é–‹å§‹ï¼ˆå¾ªç’°è«–ç†å›é¿ç‰ˆï¼‰...")
        logger.info("ğŸ“‹ æ”¹å–„ç†ç”±: ç›®çš„å¤‰æ•°ï¼ˆè¤‡å‹ç‡ï¼‰â‰ é‡ã¿ç®—å‡ºåŸºæº–ï¼ˆå‹ç‡ï¼‰ã§å¾ªç’°è«–ç†ã‚’å›é¿")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è©³ç´°æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ğŸ“Š å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±.")
        logger.info(f"   è¡Œæ•°: {len(df):,}")
        logger.info(f"   åˆ—æ•°: {len(df.columns)}")
        logger.info(f"   åˆ—åä¸€è¦§: {list(df.columns)}")
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ã‚’å‹•çš„ã«æ¤œå‡º
        available_level_cols = [col for col in df.columns if col.endswith('_level')]
        logger.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãªãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ : {available_level_cols}")
        
        # ã‚«ãƒ©ãƒ åã®è§£æ±ºï¼ˆãƒãƒ¼ã‚¸æ™‚ã®é‡è¤‡ã‚«ãƒ©ãƒ å¯¾å¿œï¼‰
        resolved_cols = {}
        for base_name in ['grade_level', 'venue_level', 'distance_level']:
            if base_name in df.columns:
                resolved_cols[base_name] = base_name
            elif f"{base_name}_x" in df.columns and f"{base_name}_y" in df.columns:
                # ãƒãƒ¼ã‚¸æ™‚ã®é‡è¤‡ã‚«ãƒ©ãƒ ã®å ´åˆã€_xã‚’å„ªå…ˆ
                resolved_cols[base_name] = f"{base_name}_x"
                logger.info(f"ğŸ“Š {base_name} ã‚’ {base_name}_x ã¨ã—ã¦è§£æ±º")
            elif f"{base_name}_x" in df.columns:
                resolved_cols[base_name] = f"{base_name}_x"
                logger.info(f"ğŸ“Š {base_name} ã‚’ {base_name}_x ã¨ã—ã¦è§£æ±º")
            elif f"{base_name}_y" in df.columns:
                resolved_cols[base_name] = f"{base_name}_y"
                logger.info(f"ğŸ“Š {base_name} ã‚’ {base_name}_y ã¨ã—ã¦è§£æ±º")
            else:
                logger.warning(f"âš ï¸ {base_name} ã®è§£æ±ºå¯èƒ½ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        logger.info(f"ğŸ“Š è§£æ±ºã•ã‚ŒãŸã‚«ãƒ©ãƒ : {resolved_cols}")
        
        # å¿…è¦ã‚«ãƒ©ãƒ ã®ç¢ºèª
        required_base_cols = ['é¦¬å', 'ç€é †']
        missing_base_cols = [col for col in required_base_cols if col not in df.columns]
        if missing_base_cols:
            logger.error(f"âŒ åŸºæœ¬ã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_base_cols}")
            return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã®å¯¾å¿œ
        if not resolved_cols:
            logger.warning("âš ï¸ ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # å‹åˆ©ãƒ•ãƒ©ã‚°ã‚’ä½œæˆï¼ˆ1ç€ã®ã¿ï¼‰
        df_temp = df.copy()
        df_temp['is_winner'] = (pd.to_numeric(df_temp['ç€é †'], errors='coerce') == 1).astype(int)
        logger.info("ğŸ“Š ç€é †åˆ—ã‹ã‚‰å‹åˆ©ãƒ•ãƒ©ã‚°ã‚’ä½œæˆï¼ˆç€é †=1ã®ã¿ï¼‰")
        
        # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆæœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šï¼‰
        # è§£æ±ºã•ã‚ŒãŸã‚«ãƒ©ãƒ åã‚’ä½¿ç”¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆç”¨ã‚«ãƒ©ãƒ ã‚’æ±ºå®š
        count_col = resolved_cols.get('grade_level', 'grade_level')
        if count_col not in df_temp.columns:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
            count_col = list(resolved_cols.values())[0] if resolved_cols else 'grade_level'
        
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: ã‚«ã‚¦ãƒ³ãƒˆç”¨ã‚«ãƒ©ãƒ : {count_col}")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢çŠ¶: {df_temp.shape}")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: é¦¬åã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {df_temp['é¦¬å'].nunique()}")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: é¦¬åã®ç·æ•°: {len(df_temp['é¦¬å'])}")
        
        # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—
        horse_stats = df_temp.groupby('é¦¬å').agg({
            'is_winner': 'mean',     # å‹ç‡ï¼ˆ1ç€ç‡ï¼‰
            count_col: 'count'       # å‡ºèµ°å›æ•°
        }).reset_index()
        
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: é¦¬çµ±è¨ˆä½œæˆå¾Œ: {len(horse_stats)}é ­")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: å‡ºèµ°å›æ•°åˆ†å¸ƒ:")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: æœ€å°: {horse_stats[count_col].min()}")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: æœ€å¤§: {horse_stats[count_col].max()}")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: å¹³å‡: {horse_stats[count_col].mean():.2f}")
        logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: 6æˆ¦ä»¥ä¸Šã®é¦¬æ•°: {(horse_stats[count_col] >= 6).sum()}")
        
        # æœ€ä½2æˆ¦ä»¥ä¸Šã®é¦¬ã®ã¿æŠ½å‡ºï¼ˆé‡ã¿è¨ˆç®—ç”¨ã«å¤§å¹…ç·©å’Œï¼‰
        horse_stats = horse_stats[horse_stats[count_col] >= 2]
        logger.info(f"ğŸ“Š å‹ç‡è¨ˆç®—å¯¾è±¡: {len(horse_stats)}é ­ï¼ˆ2æˆ¦ä»¥ä¸Šï¼‰")
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ã™ã‚‹å ´åˆã¯é–¾å€¤ã‚’ã•ã‚‰ã«ä¸‹ã’ã‚‹
        if len(horse_stats) < 50:
            logger.warning(f"âš ï¸ 2æˆ¦ä»¥ä¸Šã®é¦¬ãŒä¸è¶³: {len(horse_stats)}é ­")
            logger.warning("ğŸ“Š æœ€ä½å‡ºèµ°æ•°ã‚’1æˆ¦ã«ä¸‹ã’ã¦å†è¨ˆç®—ã—ã¾ã™...")
            
            # 1æˆ¦ä»¥ä¸Šã®é¦¬ã§å†è¨ˆç®—ï¼ˆå…¨é¦¬å¯¾è±¡ï¼‰
            horse_stats = df_temp.groupby('é¦¬å').agg({
                'is_winner': 'mean',     # å‹ç‡ï¼ˆ1ç€ç‡ï¼‰
                count_col: 'count'       # å‡ºèµ°å›æ•°
            }).reset_index()
            horse_stats = horse_stats[horse_stats[count_col] >= 1]
            logger.info(f"ğŸ“Š 1æˆ¦ä»¥ä¸Šã®é¦¬: {len(horse_stats)}é ­")
            
            if len(horse_stats) < 20:
                logger.warning(f"âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã™ãã¾ã™: {len(horse_stats)}é ­")
                return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # ç›¸é–¢è¨ˆç®—ï¼ˆè§£æ±ºã•ã‚ŒãŸã‚«ãƒ©ãƒ åã‚’ä½¿ç”¨ï¼‰
        correlations = {}
        
        # è§£æ±ºã•ã‚ŒãŸã‚«ãƒ©ãƒ åã‚’ä½¿ç”¨ã—ã¦ç›¸é–¢è¨ˆç®—
        for base_name, resolved_name in resolved_cols.items():
            if resolved_name in df.columns:
                # é¦¬ã”ã¨ã®å¹³å‡ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
                avg_feature = df.groupby('é¦¬å')[resolved_name].mean().reset_index()
                avg_feature.columns = ['é¦¬å', f'avg_{base_name}']
                
                # é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸
                horse_stats_with_feature = horse_stats.merge(avg_feature, on='é¦¬å', how='left')
                
                # ç›¸é–¢è¨ˆç®—
                clean_data = horse_stats_with_feature[['is_winner', f'avg_{base_name}']].dropna()
                if len(clean_data) >= 20:
                    from scipy.stats import pearsonr
                    corr, p_value = pearsonr(clean_data['is_winner'], clean_data[f'avg_{base_name}'])
                    
                    # ã‚­ãƒ¼åã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
                    key_mapping = {
                        'grade_level': 'grade',
                        'venue_level': 'venue',
                        'distance_level': 'distance'
                    }
                    key_name = key_mapping.get(base_name, base_name)
                    correlations[key_name] = corr
                    
                    logger.info(f"ğŸ“Š {key_name} vs å‹ç‡: r = {corr:.3f}, p = {p_value:.6f}")
                else:
                    key_mapping = {
                        'grade_level': 'grade',
                        'venue_level': 'venue', 
                        'distance_level': 'distance'
                    }
                    key_name = key_mapping.get(base_name, base_name)
                    correlations[key_name] = 0.0
                    logger.warning(f"âš ï¸ {key_name} ã®ç›¸é–¢è¨ˆç®—ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                logger.error(f"âŒ {base_name} ã‚«ãƒ©ãƒ ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: '{resolved_name}'")
                logger.error(f"ğŸ“Š ä½¿ç”¨å¯èƒ½ã‚«ãƒ©ãƒ : {list(df.columns)}")
                key_mapping = {
                    'grade_level': 'grade',
                    'venue_level': 'venue',
                    'distance_level': 'distance'
                }
                key_name = key_mapping.get(base_name, base_name)
                correlations[key_name] = 0.0
        
        
        logger.info("âœ… å‹ç‡ãƒ™ãƒ¼ã‚¹ç›¸é–¢è¨ˆç®—å®Œäº†ï¼ˆå¾ªç’°è«–ç†å›é¿ï¼‰")
        return correlations
    
    @classmethod
    def _calculate_feature_correlations_report_compliant(cls, df: pd.DataFrame, target_col: str) -> Dict[str, float]:
        """
        verify_weight_calculation.pyæº–æ‹ ã®ç›¸é–¢è¨ˆç®—ï¼ˆé¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒ™ãƒ«ï¼‰
        
        Args:
            df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆå€‹åˆ¥ãƒ¬ãƒ¼ã‚¹çµæœï¼‰
            target_col: ç›®æ¨™å¤‰æ•°åˆ—å
            
        Returns:
            ç›¸é–¢è¾æ›¸ï¼ˆverify_weight_calculation.pyå½¢å¼ï¼‰
        """
        logger.info("ğŸ” verify_weight_calculation.pyæº–æ‹ ã®ç›¸é–¢è¨ˆç®—ã‚’é–‹å§‹...")
        
        # Phase 1: é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰
        logger.info("ğŸ“Š Phase 1: é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...")
        
        # å¿…è¦ã‚«ãƒ©ãƒ ã®ç¢ºèª.
        required_cols = ['é¦¬å', 'ç€é †', 'grade_level', 'venue_level', 'distance_level']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.error(f"âŒ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
            return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆ
        if 'ç€é †' in df.columns:
            df_temp = df.copy()
            df_temp['is_placed'] = (pd.to_numeric(df_temp['ç€é †'], errors='coerce') <= 3).astype(int)
            logger.info("ğŸ“Š ç€é †åˆ—ã‹ã‚‰è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆï¼ˆç€é †<=3ï¼‰")
        elif 'è¤‡å‹' in df.columns:
            df_temp = df.copy()
            df_temp['is_placed'] = pd.to_numeric(df_temp['è¤‡å‹'], errors='coerce').fillna(0)
            logger.info("ğŸ“Š è¤‡å‹åˆ—ã‹ã‚‰è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆ")
        else:
            logger.error("âŒ è¤‡å‹ãƒ•ãƒ©ã‚°ã‚’ä½œæˆã§ãã¾ã›ã‚“")
            return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # é¦¬ã”ã¨ã®çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆæœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šï¼‰
        horse_stats = df_temp.groupby('é¦¬å').agg({
            'is_placed': 'mean',  # è¤‡å‹ç‡
            'grade_level': 'count'  # å‡ºèµ°å›æ•°
        }).reset_index()
        
        # åˆ—åã‚’æ¨™æº–åŒ–
        horse_stats.columns = ['é¦¬å', 'place_rate', 'race_count']
        
        # æœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä»•æ§˜æº–æ‹ ï¼‰
        horse_stats = horse_stats[horse_stats['race_count'] >= 6].copy()
        logger.info(f"ğŸ“Š æœ€ä½å‡ºèµ°æ•°6æˆ¦ä»¥ä¸Šã§ãƒ•ã‚£ãƒ«ã‚¿: {len(horse_stats):,}é ­")
        
        if len(horse_stats) < 100:
            logger.error(f"âŒ ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³: {len(horse_stats)}é ­ï¼ˆæœ€ä½100é ­å¿…è¦ï¼‰")
            return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã®å¹³å‡ã‚’è¨ˆç®—
        feature_cols = ['grade_level', 'venue_level', 'distance_level']
        for col in feature_cols:
            avg_feature = df.groupby('é¦¬å')[col].mean().reset_index()
            avg_feature.columns = ['é¦¬å', f'avg_{col}']
            horse_stats = horse_stats.merge(avg_feature, on='é¦¬å', how='left')
        
        logger.info(f"ğŸ“Š é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(horse_stats):,}é ­")
        
        # Phase 2: ç›¸é–¢è¨ˆç®—ï¼ˆé¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
        logger.info("ğŸ“ˆ Phase 2: é¦¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã§ç›¸é–¢ã‚’è¨ˆç®—ä¸­...")
        
        # å¿…è¦ãªåˆ—ã®ç¢ºèª
        required_corr_cols = ['place_rate', 'avg_grade_level', 'avg_venue_level', 'avg_distance_level']
        missing_corr_cols = [col for col in required_corr_cols if col not in horse_stats.columns]
        
        if missing_corr_cols:
            logger.error(f"âŒ å¿…è¦ãªç›¸é–¢åˆ—ãŒä¸è¶³: {missing_corr_cols}")
            logger.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(horse_stats.columns)}")
            return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # æ¬ æå€¤ã‚’é™¤å»
        clean_data = horse_stats[required_corr_cols].dropna()
        logger.info(f"ğŸ“Š ç›¸é–¢è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿: {len(clean_data):,}é ­")
        
        if len(clean_data) < 100:
            logger.error(f"âŒ ç›¸é–¢è¨ˆç®—ç”¨ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³: {len(clean_data)}é ­ï¼ˆæœ€ä½100é ­å¿…è¦ï¼‰")
            return {'grade': 0.0, 'venue': 0.0, 'distance': 0.0}
        
        # ç›¸é–¢è¨ˆç®—
        correlations = {}
        target = clean_data['place_rate']
        
        # ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®ç›¸é–¢è¨ˆç®—
        feature_mapping = {
            'avg_grade_level': 'grade',
            'avg_venue_level': 'venue', 
            'avg_distance_level': 'distance'
        }
        
        for feature_col, feature_name in feature_mapping.items():
            if feature_col in clean_data.columns:
                corr, p_value = pearsonr(clean_data[feature_col], target)
                correlations[feature_name] = corr
                logger.info(f"   ğŸ“ˆ {feature_name}_level: r = {corr:.3f}, rÂ² = {corr**2:.3f}, p = {p_value:.3f}")
        
        return correlations
    
    
    @classmethod
    def _calculate_weights_report_compliant(cls, correlations: Dict[str, float]) -> Dict[str, float]:
        """
        ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®é‡ã¿è¨ˆç®—
        è¨ˆç®—å¼: w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)
        
        Args:
            correlations: ç›¸é–¢ä¿‚æ•°è¾æ›¸
            
        Returns:
            é‡ã¿è¾æ›¸
        """
        logger.info("ğŸ¯ ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ã®é‡ã¿è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
        logger.info("ğŸ“‹ è¨ˆç®—å¼: w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)")
        
        # ç›¸é–¢ä¿‚æ•°ã®2ä¹—ï¼ˆå¯„ä¸åº¦ï¼‰ã‚’è¨ˆç®—
        r_grade = correlations.get('grade', 0.0)
        r_venue = correlations.get('venue', 0.0)
        r_distance = correlations.get('distance', 0.0)
        
        # å¯„ä¸åº¦è¨ˆç®—
        contrib_grade = r_grade ** 2
        contrib_venue = r_venue ** 2
        contrib_distance = r_distance ** 2
        total_contrib = contrib_grade + contrib_venue + contrib_distance
        
        logger.info("ğŸ“Š å¯„ä¸åº¦è¨ˆç®—çµæœ:")
        logger.info(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰å¯„ä¸åº¦: rÂ² = {r_grade:.3f}Â² = {contrib_grade:.3f}")
        logger.info(f"   å ´æ‰€å¯„ä¸åº¦: rÂ² = {r_venue:.3f}Â² = {contrib_venue:.3f}")
        logger.info(f"   è·é›¢å¯„ä¸åº¦: rÂ² = {r_distance:.3f}Â² = {contrib_distance:.3f}")
        logger.info(f"   ç·å¯„ä¸åº¦: {total_contrib:.3f}")
        
        if total_contrib == 0:
            logger.warning("âš ï¸ ç·å¯„ä¸åº¦ãŒ0ã§ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return cls._get_fallback_weights()
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸé‡ã¿è¨ˆç®—
        weight_grade = contrib_grade / total_contrib
        weight_venue = contrib_venue / total_contrib
        weight_distance = contrib_distance / total_contrib
        
        weights = {
            'grade_weight': weight_grade,
            'venue_weight': weight_venue,
            'distance_weight': weight_distance
        }
        
        logger.info("ğŸ¯ æ­£è¦åŒ–ã•ã‚ŒãŸé‡ã¿:")
        logger.info(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {weight_grade:.3f} ({weight_grade*100:.1f}%)")
        logger.info(f"   å ´æ‰€é‡ã¿: {weight_venue:.3f} ({weight_venue*100:.1f}%)")
        logger.info(f"   è·é›¢é‡ã¿: {weight_distance:.3f} ({weight_distance*100:.1f}%)")
        
        # ãƒ¬ãƒãƒ¼ãƒˆå€¤ã¨ã®æ¯”è¼ƒ
        report_weights = cls._get_fallback_weights()
        logger.info("ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€è¨˜è¼‰å€¤ã¨ã®æ¯”è¼ƒ:")
        logger.info(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰: è¨ˆç®—å€¤{weight_grade:.3f} vs ãƒ¬ãƒãƒ¼ãƒˆå€¤{report_weights['grade_weight']:.3f}")
        logger.info(f"   å ´æ‰€: è¨ˆç®—å€¤{weight_venue:.3f} vs ãƒ¬ãƒãƒ¼ãƒˆå€¤{report_weights['venue_weight']:.3f}")
        logger.info(f"   è·é›¢: è¨ˆç®—å€¤{weight_distance:.3f} vs ãƒ¬ãƒãƒ¼ãƒˆå€¤{report_weights['distance_weight']:.3f}")
        
        return weights
    
    @classmethod
    def _calculate_weights_from_correlations(cls, correlations: Dict[str, float]) -> Dict[str, float]:
        """
        ç›¸é–¢ä¿‚æ•°ã‹ã‚‰é‡ã¿ã‚’è¨ˆç®—
        
        Args:
            correlations: ç›¸é–¢ä¿‚æ•°è¾æ›¸
            
        Returns:
            é‡ã¿è¾æ›¸
        """
        # å¯„ä¸åº¦è¨ˆç®—ï¼ˆç›¸é–¢ã®2ä¹—ï¼‰
        contributions = {key: corr ** 2 for key, corr in correlations.items()}
        total_contribution = sum(contributions.values())
        
        logger.info(f"ğŸ” ç›¸é–¢åˆ†æçµæœ:")
        for key, corr in correlations.items():
            logger.info(f"   {key}ç›¸é–¢: r = {corr:.3f}, rÂ² = {contributions[key]:.3f}")
        logger.info(f"   ç·å¯„ä¸åº¦: {total_contribution:.3f}")
        
        # é‡ã¿è¨ˆç®—
        if total_contribution > 0:
            weights = {
                'grade_weight': contributions['grade'] / total_contribution,
                'venue_weight': contributions['venue'] / total_contribution,
                'distance_weight': contributions['distance'] / total_contribution
            }
        else:
            logger.warning("âš ï¸ ã™ã¹ã¦ã®ç›¸é–¢ãŒ0ã§ã™ã€‚å‡ç­‰é‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            weights = {
                'grade_weight': 1.0 / 3,
                'venue_weight': 1.0 / 3,
                'distance_weight': 1.0 / 3
            }
        
        return weights
    
    @classmethod
    def _display_calculation_results(cls, weights: Dict[str, float], correlations: Dict[str, float]):
        """
        è¨ˆç®—çµæœã‚’è¡¨ç¤º
        
        Args:
            weights: é‡ã¿è¾æ›¸
            correlations: ç›¸é–¢ä¿‚æ•°è¾æ›¸
        """
        logger.info(f"ğŸ“Š è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰å‹•çš„é‡ã¿ç®—å‡ºçµæœ:")
        logger.info(f"   ã‚°ãƒ¬ãƒ¼ãƒ‰: {weights['grade_weight']:.3f} ({weights['grade_weight']*100:.1f}%)")
        logger.info(f"   å ´æ‰€: {weights['venue_weight']:.3f} ({weights['venue_weight']*100:.1f}%)")
        logger.info(f"   è·é›¢: {weights['distance_weight']:.3f} ({weights['distance_weight']*100:.1f}%)")
        logger.info("âœ… ãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ : w_i = r_iÂ² / Î£r_iÂ²")
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šå®Œäº†ã®é€šçŸ¥
        print("\n" + "="*60)
        print("ğŸ¯ å‹•çš„é‡ã¿è¨ˆç®—å®Œäº† - ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šé©ç”¨")
        print("="*60)
        print(f"ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {weights['grade_weight']:.3f} ({weights['grade_weight']*100:.1f}%)")
        print(f"ğŸ“Š å ´æ‰€é‡ã¿: {weights['venue_weight']:.3f} ({weights['venue_weight']*100:.1f}%)")
        print(f"ğŸ“Š è·é›¢é‡ã¿: {weights['distance_weight']:.3f} ({weights['distance_weight']*100:.1f}%)")
        print("âœ… å…¨åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…±é€šä½¿ç”¨ã•ã‚Œã¾ã™")
        print("="*60 + "\n")
    
    @classmethod
    def _log_weight_calculation_results(cls, weights: Dict[str, float], correlations: Dict[str, float], train_data: pd.DataFrame):
        """
        é‡ã¿è¨ˆç®—çµæœã‚’ãƒ­ã‚°ã«è©³ç´°å‡ºåŠ›
        
        Args:
            weights: é‡ã¿è¾æ›¸
            correlations: ç›¸é–¢ä¿‚æ•°è¾æ›¸
            train_data: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("ğŸ“Š ========== å‹•çš„é‡ã¿è¨ˆç®—çµæœï¼ˆè©³ç´°ãƒ­ã‚°ï¼‰ ==========")
        logger.info(f"ğŸ“‹ è¨ˆç®—åŸºæº–æœŸé–“: {train_data['å¹´'].min()}-{train_data['å¹´'].max()}å¹´")
        logger.info(f"ğŸ“‹ å¯¾è±¡ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_data):,}è¡Œ")
        logger.info(f"ğŸ“‹ è¨ˆç®—å¼: w_i = r_iÂ² / (r_gradeÂ² + r_venueÂ² + r_distanceÂ²)")
        
        # ç›¸é–¢ä¿‚æ•°ã®è©³ç´°ãƒ­ã‚°
        logger.info("ğŸ” ç›¸é–¢åˆ†æçµæœ:")
        for key, corr in correlations.items():
            contribution = corr ** 2
            logger.info(f"   ğŸ“Š {key}ãƒ¬ãƒ™ãƒ«: r = {corr:.4f}, rÂ² = {contribution:.4f}")
        
        total_contribution = sum(corr ** 2 for corr in correlations.values())
        logger.info(f"   ğŸ“Š ç·å¯„ä¸åº¦: {total_contribution:.4f}")
        
        # é‡ã¿é…åˆ†ã®è©³ç´°ãƒ­ã‚°
        logger.info("âš–ï¸ ç®—å‡ºã•ã‚ŒãŸé‡ã¿é…åˆ†:")
        logger.info(f"   ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {weights['grade_weight']:.4f} ({weights['grade_weight']*100:.2f}%)")
        logger.info(f"   ğŸ“Š å ´æ‰€é‡ã¿: {weights['venue_weight']:.4f} ({weights['venue_weight']*100:.2f}%)")
        logger.info(f"   ğŸ“Š è·é›¢é‡ã¿: {weights['distance_weight']:.4f} ({weights['distance_weight']*100:.2f}%)")
        
        # é‡ã¿åˆè¨ˆã®ç¢ºèª
        total_weight = weights['grade_weight'] + weights['venue_weight'] + weights['distance_weight']
        logger.info(f"   ğŸ“Š é‡ã¿åˆè¨ˆ: {total_weight:.4f} (1.000ã«æ­£è¦åŒ–)")
        
        # REQIè¨ˆç®—å¼ã®ãƒ­ã‚°å‡ºåŠ›
        logger.info("ğŸ“Š REQIè¨ˆç®—å¼:")
        logger.info(f"   race_level = {weights['grade_weight']:.4f} Ã— grade_level + {weights['venue_weight']:.4f} Ã— venue_level + {weights['distance_weight']:.4f} Ã— distance_level")
        
        logger.info("âœ… å‹•çš„é‡ã¿è¨ˆç®—å®Œäº† - å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã“ã®é‡ã¿ã‚’ä½¿ç”¨")
        logger.info("=" * 60)
    
    @classmethod
    def _get_fallback_weights(cls) -> Dict[str, float]:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚’å–å¾—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆ5.1.3ç¯€æº–æ‹ ï¼‰
        è¨“ç·´æœŸé–“ï¼ˆ2010-2020å¹´ï¼‰11,196é ­ã®å®Ÿæ¸¬ç›¸é–¢ã‹ã‚‰ç®—å‡ºã•ã‚ŒãŸå›ºå®šé‡ã¿
        
        Returns:
            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿è¾æ›¸
        """
        logger.info("ğŸ“Š å¾ªç’°è«–ç†å›é¿ç‰ˆã®å›ºå®šé‡ã¿ã‚’ä½¿ç”¨ã—ã¾ã™")
        logger.info("ğŸ“‹ å‹ç‡ãƒ™ãƒ¼ã‚¹ç›¸é–¢ã«ã‚ˆã‚‹é‡ã¿ï¼ˆ2024å¹´æ”¹å–„ç‰ˆï¼‰:")
        logger.info("   ğŸ¯ ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: 65.0% (G1-G3å‹åˆ©ã®ä¾¡å€¤)")
        logger.info("   ğŸ‡ å ´æ‰€é‡ã¿: 30.0% (æ±äº¬ãƒ»é˜ªç¥ã®æ ¼å¼)")
        logger.info("   ğŸ“ è·é›¢é‡ã¿: 5.0% (è·é›¢é©æ€§ã®è£œæ­£)")
        logger.info("ğŸ”§ æ”¹å–„ç†ç”±: å¾ªç’°è«–ç†å›é¿ï¼ˆäºˆæ¸¬ç›®çš„â‰ é‡ã¿ç®—å‡ºåŸºæº–ï¼‰")
        
        weights = {
            'grade_weight': 0.650,   # 65.0% - å¾ªç’°è«–ç†å›é¿ç‰ˆ
            'venue_weight': 0.300,   # 30.0% - å¾ªç’°è«–ç†å›é¿ç‰ˆ
            'distance_weight': 0.050 # 5.0%  - å¾ªç’°è«–ç†å›é¿ç‰ˆ
        }
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é‡ã¿ã‚‚ãƒ­ã‚°ã«å‡ºåŠ›
        logger.info("ğŸ“Š é©ç”¨é‡ã¿è©³ç´°:")
        logger.info(f"   ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰é‡ã¿: {weights['grade_weight']:.3f} ({weights['grade_weight']*100:.1f}%)")
        logger.info(f"   ğŸ“Š å ´æ‰€é‡ã¿: {weights['venue_weight']:.3f} ({weights['venue_weight']*100:.1f}%)")
        logger.info(f"   ğŸ“Š è·é›¢é‡ã¿: {weights['distance_weight']:.3f} ({weights['distance_weight']*100:.1f}%)")
        
        return weights

# ä¾¿åˆ©é–¢æ•°
def get_global_weights() -> Dict[str, float]:
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’å–å¾—ã™ã‚‹ä¾¿åˆ©é–¢æ•°
    
    Returns:
        é‡ã¿è¾æ›¸
    """
    return WeightManager.get_weights()

def initialize_weights_from_data(df: pd.DataFrame) -> Dict[str, float]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡ã¿ã‚’åˆæœŸåŒ–ã™ã‚‹ä¾¿åˆ©é–¢æ•°
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        
    Returns:
        é‡ã¿è¾æ›¸
    """
    return WeightManager.initialize_from_training_data(df)
